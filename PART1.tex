\documentclass[11pt, A4paper, openany, uplatex]{book}
\input{settings}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Methods of Econometrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Consistency and Asymptotic Normality}\label{chap:LLN_CLT}
\section{Convergence of real sequences}
Real numbers are numbers that have points on the number line $\mathbb{R}$.
Let $a$ be a real number and $(a_n)_{n=1}^\infty$ be a sequence of real numbers:
\[
	(a_n)_{n=1}^\infty = a_1, \; a_2, \; a_3, \ldots
\]
For example, a sequence
\[
	a_1 = 1, \; a_2 = 1.4, \; a_3 = 1.41, \; \cdots, \; a_n = 1.4142\ldots, \cdots
\]
gets closer and closer to $\alpha = \sqrt{2} \; (= 1.414213\ldots)$ as $n$ increases.
The number $\alpha$ is called the \textbf{limit\index{limit}} of the sequence $(a_n)_{n=1}^\infty$. 
We say that the sequence $(a_n)_{n=1}^\infty$ \textbf{converges\index{convergence}} to $\alpha$, and write
\begin{align*}
	& \lim_{n\to\infty}a_n = \alpha \\
	\text{or equivalently} \;\; & a_n \to \alpha \;\; (n\to\infty).
\end{align*}

For other examples, the limit of the sequence $a_1 = 1, \; a_2 = 1/2, \; a_3 = 1/3, \; \cdots, \; a_n = 1/n, \cdots$ is clearly $\lim_{n\to\infty}a_n = 0$.
It is well-known that the sequence
\[
	a_1 = (1 + (1/1))^1, \; a_2 = (1 + (1/2))^2, \; \cdots, \; a_n = (1 + (1/n))^n, \cdots
\]
converges to the Napier's constant: $\lim_{n\to\infty}a_n = e \; (=2.718 \cdots)$.
On the other hand, the limit of the sequence $a_1 = 1, \; a_2 = 2, \; a_3 = 3, \;\cdots, \; a_n = n, \cdots$ is infinite: $\lim_{n\to\infty}a_n = \infty$.
When the limit of a sequence is infinite, we say that the sequence is a \textbf{divergent\index{divergent}}.
There are sequences that are neither convergent nor divergent.
For example, the sequence
\[
	a_1 = -1, \; a_2 = 1, \; a_3 = -1, \; \cdots, \; a_n = (-1)^n, \cdots
\]
alternates between $1$ and $-1$.

The statement ``the sequence $(a_n)_{n=1}^\infty$ gets closer to $\alpha$ as $n$ increases'' is rather informal and mathematically unclear.
A formal definition of convergence is as follows.
\begin{definition}\label{def:conv}
We say that a sequence of real numbers $(a_n)_{n=1}^\infty$ converges to a limit $\alpha$ if for any given positive number $\kappa > 0$, there exists a natural number $n(\kappa)$ such that
\begin{equation}
	|a_n - \alpha| < \kappa \;\; \text{for all $n$ satisfying } n(\kappa) \leq n. \label{eq:conv}
\end{equation}
\end{definition}
The interpretation of \eqref{eq:conv} is as follows.
Choose an arbitrary small $\kappa$, say $\kappa < 10^{-100}$ or even smaller.
Even for such a small value of $\kappa$, if we set $n(\kappa)$ sufficiently large, the distance between the terms of the sequence after $a_{n(\kappa)}$ and $\alpha$ can be smaller than $\kappa$ (see Figure \ref{fig:limit}).
 
\begin{figure}[h!]
	\begin{center}
	\includegraphics[width=14cm]{Limit.png}\label{fig:limit}
	\caption{Convergence of a real sequence}
	\end{center}
\end{figure}

\hrulefill
\begin{exercise}\upshape
	What are the limits of the following sequences?
	\begin{enumerate}
		\item $a_n = n^2 - n$
		\item $a_n = (-1)^n/\sqrt{n}$
		\item $a_n = \sqrt{n + 1} - \sqrt{n}$
	\end{enumerate}
\end{exercise}
\begin{exercise}\upshape
	Prove that the infinite decimal $0.999 \ldots$ is equal to one using the notion of convergence and limit.
\end{exercise}
\begin{exercise}\upshape
	Let $S_n = \sum_{k=1}^n a_k$ and $\bar S_n = \sum_{k=1}^n |a_k|$.
	Show that if $\bar S_n$ converges to a finite number, so does $S_n$. (This is called \textbf{absolute convergence}.)
\end{exercise}

Note  that $a_k \to 0$ as $k \to \infty$ does not ensure the convergence of $S_n = \sum_{k = 1}^n a_k$ at all.
For example, as is well known, $\sum_{k = 1}^n k^{-1}$ diverges to infinity.\footnote{
	Proof: Observe that
	\begin{align*}
		\exp(1 + 2^{-1} + 3^{-1} + \cdots + n^{-1})
		& = \exp(1)\exp(2^{-1})\exp(3^{-1})\cdots\exp(n^{-1}) \ge (1 + 1)(1 + 2^{-1})(1 + 3^{-1})\cdots(1 + n^{-1}) \\
		& = 2 \cdot \frac{3}{2} \cdot \frac{4}{3} \cdots \frac{n + 1}{n} = n + 1,
	\end{align*}
	where we have used the fact $\exp(x) \ge 1 + x$.
	This implies that $1 + 2^{-1} + 3^{-1} + \cdots + n^{-1} \ge \log (n + 1)$, where the right-hand side diverges as $n$ tends to infinity.
}

\section{Convergence in probability}\label{subsec:inprob}

Now let $Z_n \in \mbb{R}$ be a  sequence of ``random variables''.
Recall that random variables are not numbers, but they are functions (rules) that assign some real numbers to random events.
Thus, whether the sequence of random variables $Z_n$ converges to a limit or not is itself a stochastic event, and hence the definition \ref{def:conv} cannot be applied directly.
However, since a probability is a real number, we can still consider the convergence of a sequence of probabilities in the usual sense.
Then, we introduce the notion of ``convergence in probability''.
\begin{definition}
For any given positive number $\kappa > 0$, if
\[
	\lim_{n \to \infty} \Pr(|Z_n - \mu| \le \kappa) = 1
\]
holds, we say that $Z_n$ \textbf{converges in probability\index{convergence in probability}} to $\mu$ as $n \to \infty$, and write 
\begin{align*}
	& \plim_{n \to \infty} Z_n = \mu \\
	\text{or equivalently} \;\; &  Z_n \overset{p}{\to} \mu \;\; (n\to\infty).
\end{align*}
\end{definition}

The above definition can be read as follows: $a_n \equiv \Pr(|Z_n - \mu| \le \kappa)$, which is a real number, converges to one for any $\kappa$ in the sense of Definition \ref{def:conv}.
It is important to note that $\mu$ can be either a random variable or a constant number in this definition.

When $Z_n$ is a sequence of vector-valued random variables, one can use an appropriate norm $||\cdot||$, such as Euclidean norm, in place of the absolute value $|\cdot|$.
Note that the choice of $\kappa$ is arbitrary, and thus it can be, for example, $\kappa < 10^{-100}$ or even smaller as long as it is larger than 0.
This definition says that the probability of observing the event $\{|Z_n - \mu| \le \kappa\}$ approaches to one as $n$ increases even for such very small $\kappa$. 

Note that a constant is a special case of a random variable.
Therefore, when a sequence of constants converges $a_n \to \alpha$, it trivially holds that $a_n \overset{p}{\to} \alpha$.
Further, for a sequence of random variables that converges in probability $Z_n \overset{p}{\to} \mu$, we also have $a_n Z_n  \overset{p}{\to} \alpha \mu $ because
\begin{align*}
	|a_n Z_n - \alpha \mu| 
	& \le |a_n Z_n - a_n \mu| + |a_n \mu - \alpha \mu|\\
	& = |a_n| \cdot |Z_n - \mu| + |\mu| \cdot |a_n - \alpha| \overset{p}{\to} 0,
\end{align*}
where the inequality follows from the triangle inequality.

\begin{lemma}[Markov's Inequality\index{Markov's inequality}]\label{markov}
	For any random variable $Z_n$ and positive constant $\kappa > 0$,
	\begin{equation}\label{eq:markov}
	\Pr\left( |Z_n| > \kappa \right) \leq \frac{\E|Z_n|}{\kappa}.
	\end{equation}
\end{lemma}

\begin{proof}
	First, observe the following equality:
	\begin{align*}
	\Pr( |Z_n| > \kappa ) 
	& = \E[\mbf{1}(|Z_n| > \kappa )] \\
	& = \E\left[\mbf{1}\left(\frac{|Z_n|}{\kappa} > 1 \right)\right]
	\end{align*}
	Since $\frac{|Z_n|}{\kappa}$ is non-negative, we have $\mbf{1}\left(\frac{|Z_n|}{\kappa} > 1 \right) \le \frac{|Z_n|}{\kappa}$.
	Hence,
	\begin{align*}
	\E\left[\mbf{1}\left(\frac{|Z_n|}{\kappa} > 1 \right)\right] \le \frac{\E|Z_n|}{\kappa}.
	\end{align*}
	This implies the desired result.
\end{proof}

As a corollary of Markov's inequality, we can obtain the following useful inequality:
\begin{align}\label{eq:chevyshev}
\Pr( |Z_n - \E Z_n | > \kappa ) = \Pr( (Z_n - \E Z_n)^2 > \kappa^2 ) \leq \frac{\Var(Z_n)}{\kappa^2},
\end{align}
which is known as \textbf{Chebyshev's inequality\index{Chevyshev's inequality}}.
Note that these inequalities do not make any assumptions about the probability distribution of $Z_n$; that is, they are distribution-free inequalities.
When $\E |Z_n|$ and $\Var(Z_n)$ do not exist, such that $\E |Z_n| = \infty$ and $\Var(Z_n) = \infty$, the inequalities  \eqref{eq:markov} and \eqref{eq:chevyshev} hold trivially.
\bigskip

Here, let $X \in \mbb{R}$ be a random variable, and $\bar X_n$ be the sample mean of $X$, where $n$ denotes the sample size.
Chebyshev's inequality implies that, if $\Var(\bar X_n) \to 0$ as $n \to \infty$,
\[
\Pr( |\bar X_n - \E \bar X_n | > \kappa ) \leq \frac{\Var(\bar X_n)}{\kappa^2} \to 0.
\]
In other words, since 
\[
	\Pr( |\bar X_n - \E\bar X_n | \le \kappa ) = 1 - \underbrace{\Pr( |\bar X_n - \E\bar X_n | > \kappa )}_{\to \: 0},
\]
the sample mean converges to its population mean in probability:
\[
\bar X_n \overset{p}{\to} \E\bar X_n.
\]
This result is known as the \textbf{weak law of large numbers\index{weak law of large numbers}} (WLLN).\footnote{	
	Since there is a ``weak'' LLN, there is also a ``strong'' version of LLN (SLLN), which states that $\Pr(\lim_{n \to \infty} \bar X_n = \E \bar X_n) = 1$.
	The proof of SLLN is much more complicated than that of WLLN.
	SLLN implies WLLN.}
As shown above, an easy-to-check sufficient condition for WLLN to hold is that $\Var(\bar X_n)$ converges to zero as $n$ increases.
In the case when the data $\{X_1, \ldots , X_n\}$ are independent and identically distributed (IID), the condition can be easily confirmed by the fact that $\Var(\bar X_n) = \Var(X)/n \to 0$ as $n \to \infty$ as long as $\Var(X)$ is finite.\footnote{
	Perhaps somewhat surprisingly, $\Var(X) < \infty$ is not a necessary condition for WLLN in general.
	}
\bigskip

An estimator which converges in probability to its population value is called a \textbf{consistent estimator\index{consistent estimator}}.
That is, letting $\what \theta_n$ be an estimator of $\theta_0$ obtained from a sample of size $n$, if $\what \theta_n \overset{p}{\to} \theta_0$, we say that $\what \theta_n$ is consistent for $\theta_0$.
WLLN states that the sample mean is a consistent estimator of the population mean under IID sampling.

\hrulefill
\begin{exercise}\upshape
	Suppose that $\{X_1, \ldots, X_n\}$ are independent, and
	\[
	X_n = \left\{\begin{array}{ll}
	0 & \text{ with probability } 1-1/n\\
	n & \text{ with probability } 1/n.
	\end{array}\right.
	\]
	Prove that $X_n$ converges to zero in probability.
\end{exercise}
\begin{exercise}\upshape
	Suppose that we have $n$ IID observations $\{X_1, \ldots, X_n\}$ drawn from $\text{Uniform}[0,1]$.
	Define 
	\[
	a_n \equiv \max\{X_1, \ldots, X_n\}.
	\]
	Prove that $a_n$ converges to one in probability.
\end{exercise}
\begin{exercise}\upshape
	(\textbf{Continuous mapping theorem}) 
	Prove the following claim: Suppose that $g(\cdot)$ is a continuous function.
	Then, if $Z_n \overset{p}{\to} \mu$, we have $g(Z_n) \overset{p}{\to} g(\mu)$.
\end{exercise}

\section{Laudau's notation: big $O$ and small $o$}

to be written soon: $O$, $o$, $O_P$, $o_P$

\section{Convergence in distribution}\label{subsec:indist}

Let $Z_n \in \mbb{R}$ be a  sequence of random variables, and $Z \in \mbb{R}$ be a random variable with distribution function  $F_Z(z) \equiv \Pr(Z \le z)$.
\begin{definition}
If
\[
	\Pr(Z_n \le z) \to F_Z(z) \;\; \text{for all $z \in \mbb{R}$}
\]
holds, we say that $Z_n$ \textbf{converges in distribution\index{convergence in distribution}} to $Z$ as $n \to \infty$, and write $Z_n \overset{d}{\to} Z$.
\end{definition}
When $Z_n \overset{d}{\to} Z$, $F_Z(\cdot)$ is referred to as $Z_n$'s \textbf{asymptotic distribution\index{asymptotic distribution}} or \textbf{limiting distribution\index{limiting distribution}}.
Throughout the rest of this section, $Z$ is assumed to be continuous for simplicity of discussion.

\begin{center}
		\includegraphics[width = 16cm]{probconv.png}
\end{center}


Note that convergence in probability is a stronger concept than convergence in distribution.
That is, $Z_n \overset{p}{\to} Z$ implies $Z_n \overset{d}{\to} Z$, but the converse is not true.
We formally prove the former result below.
The proof of the latter is easy by constructing a counterexample.
\begin{lemma}\label{lem:p_to_d}
	$Z_n \overset{p}{\to} Z \Longrightarrow Z_n \overset{d}{\to} Z$.
\end{lemma}

\begin{proof}
	For any positive constant $\kappa > 0$,
	\begin{align*}
	\Pr(Z_n \le z) 
	& = \Pr(Z_n \le z, \underbrace{|Z_n - Z| \le \kappa}_{Z_n - \kappa \; \le \; Z \; \le \; Z_n + \kappa}) + \Pr(Z_n \le z, |Z_n - Z| > \kappa) \\
	& \le \Pr(Z_n + \kappa \le z + \kappa, Z \le Z_n + \kappa) + \Pr(|Z_n - Z| > \kappa) \\
	& = \Pr(Z \le Z_n + \kappa \le z + \kappa) + \Pr(|Z_n - Z| > \kappa) \\
	& \le F_Z(z + \kappa) + \Pr(|Z_n - Z| > \kappa).
	\end{align*}
	Similarly,
	\begin{align*}
	F_Z(z - \kappa)
	& = \Pr(Z \le z - \kappa, \underbrace{|Z_n - Z | \le \kappa)}_{Z_n - \kappa \; \le \; Z \; \le \; Z_n + \kappa} + \Pr(Z \le z - \kappa, |Z_n - Z| > \kappa) \\
	& \le \Pr(Z \le z - \kappa, Z_n - \kappa \le Z) + \Pr(|Z_n - Z| > \kappa) \\
	& = \Pr(Z_n - \kappa \le Z \le z - \kappa) + \Pr(|Z_n - Z| > \kappa) \\
	& \le \Pr(Z_n \le z) + \Pr(|Z_n - Z| > \kappa).
	\end{align*}
	Combining these inequalities,
	\begin{align}\label{eq:ineq}
	F_Z(z - \kappa) - \Pr(|Z_n - Z| > \kappa) \le \Pr(Z_n \le z) \le F_Z(z + \kappa) + \Pr(|Z_n - Z| > \kappa).
	\end{align}
	Since $F_Z(\cdot)$ is assumed to be continuous, by choosing sufficiently small $\kappa > 0$, it holds that 
	\begin{align*}
	F_Z(z + \kappa) \le  F_Z(z) + \epsilon \\
	F_Z(z - \kappa) \ge F_Z(z) - \epsilon
	\end{align*}
	for a small $\epsilon > 0$.
	Then, taking the limit $n \to \infty$ of each term in \eqref{eq:ineq}, we have
	\[
	F_Z(z) - \epsilon \le \Pr(Z_n \le z) \le F_Z(z) + \epsilon
	\]
	with probability approaching one as $n \to \infty$. 
	Since $\epsilon$ can be made arbitrarily small, this completes the proof.
\end{proof}
\bigskip

Let $\{ X_1,\ldots,X_n \}$ be an IID sample of $X$ of size $n$ with $\E X = \mu$ and $\Var(X) = \sigma^2 < \infty$.
Then, as $n \to \infty$,
\[	
\sqrt{n}(\bar X_n - \mu) \overset{d}{\to} N(0,\sigma^2),
\]
where $N(0, \sigma^2)$ is a normal random variable with mean zero and variance $\sigma^2$; in other words,
\[
\lim_{n \to \infty}\Pr\left( \sqrt{n}(\bar X_n - \mu) \le z \right) = \frac{1}{\sqrt{2 \pi \sigma^2}}\int_{-\infty}^z \exp\left(-\frac{x^2}{2\sigma^2} \right) \mrm{d}x \;\;\; \text{for all $z\in\mathbb{R}$.}
\]
This result is known as the (Lindeberg-L\'{e}vy) \textbf{central limit theorem\index{central limit theorem}} (CLT).
The CLT states that if $n$ is sufficiently large, the distribution of $\sqrt{n}(\bar X_n - \mu)$ can be approximated by that of $N(0,\sigma^2)$.
The proof of CLT is rather complicated, and thus is omitted here.\footnote{
	In a special case where $X$ has a finite moment of any order, the proof can be greatly simplified by using the properties of the \textbf{moment generating function}.
}
\bigskip

Let $\what \theta_n$ be an estimator of $\theta_0$ obtained from a sample of size $n$.
If $\sqrt{n}(\what \theta_n - \theta_0) \overset{d}{\to} \mbf{N}(\mbf{0},\Omega)$, where $\mbf{N}(\mbf{0}, \Omega)$ is a multivariate normal distribution with mean vector $\mbf{0}$ and covariance matrix $\Omega$, we say that $\what \theta_n$ is \textbf{asymptotically normally distributed\index{asymptotic normality}} (at ``parametric'' rate).
$\Omega$ is called the asymptotic covariance matrix of $\sqrt{n}(\what \theta_n - \theta_0)$.

\hrulefill
\begin{exercise}\upshape
	Prove that $Z_n \overset{d}{\to} Z$ does not imply $Z_n \overset{p}{\to} Z$ by presenting a counterexample.
\end{exercise}

\begin{comment}
$X_0 \sim \text{Uniform}[-1,1]$, $X_n \equiv (-1)^n X_0$. 
Then $\Pr(X_n < x) = \Pr(X_0 < x)$, but $X_n$ never converges to $X_0$.
\end{comment}

\section{Asymptotic properties of OLS estimator}

Consider a linear regression model:
\begin{align}\label{eq:linmodel}
	Y = X^\top \beta_0 + \eps, 
\end{align}
where $Y$ is a dependent variable, $X$ is a vector of explanatory variables, and $\eps$ is an unobserved random variable (i.e., error term) satisfying $\E[\eps \mid X] = 0$.
$\beta_0$ is the true coefficient vector to be estimated.
When the data $\{(Y_i, X_i): 1 \le i \le n\}$ are available, the \textbf{ordinary least squares\index{OLS}} (OLS) estimator $\hat \beta_n^{ols}$ of $\beta_0$ is given by
\begin{align}\label{eq:ols}
\begin{split}
	\hat \beta_n^{ols} 
	& = \argmin_{\beta} \frac{1}{n}\sum_{i = 1}^n (Y_i - X_i^\top \beta)^2 \\
	& = \left(\frac{1}{n}\sum_{i = 1}^n X_i X_i^\top \right)^{-1}\frac{1}{n}\sum_{i = 1}^n X_i Y_i,
\end{split}
\end{align}
provided that $\frac{1}{n}\sum_{i = 1}^n X_i X_i^\top$ is a nonsingular matrix. 

Here, note that the conditional expectation function $\E[Y \mid X]$ is a minimizer of the \textbf{mean squared error\index{MSE}} (MSE) among all possible regression functions:
\begin{align}\label{eq:msemin}
	\E[Y \mid X] = \argmin_{g \in \mcl{G}} \E[(Y - g(X))^2],
\end{align}
where $\mcl{G}$ is the set of all measurable functions of $X$.
Since we have assumed that $\E[\eps \mid X] = 0 $ ($\iff \E [Y \mid X] = X^\top \beta_0$), we can restrict the above MSE minimization with respect to the class of linear functions of $X$:
\begin{align*}
	\beta_0 
	& = \argmin_{\beta \in \mbb{R}^{\text{dim}(X)}} \E[(Y - X^\top \beta)^2]\\
	& = \E[XX^\top]^{-1} \E[XY].
\end{align*} 
Thus, we can see that the OLS objective function in \eqref{eq:ols} is the sample analog of the population MSE, and that the estimated regression function, say $\hat g_n(X) \equiv X^\top \hat \beta_n^{ols}$, is an estimator of $\E[Y \mid X]$.\footnote{\label{foot:ols_moment}
	Another interpretation of the OLS estimator is that it is a sample solution to the moment equation $\E[Y \mid X] = X^\top\beta$.
	Premultiplying $X$ on both sides of this equation and taking the expectation yield
	\begin{align*}
		\E[X \E[Y \mid X]] = \E[XX^\top]\beta_0
		& \iff \E[XY] = \E[XX^\top]\beta_0 \\
		& \iff \beta_0 = \E[XX^\top]^{-1}\E[XY].
	\end{align*}
	Note that, in the above characterization of $\beta_0$, the multiplier does not need to be $X$.
	As easily confirmed, for any general $h(X)$, we can obtain $\beta_0 = \E[h(X)X^\top]^{-1}\E[h(X)Y]$ as long as $\E[h(X)X^\top]$ exists and is nonsingular.
	}
	By \eqref{eq:linmodel}, we can rewrite the right-hand side of \eqref{eq:ols} as
\begin{align*}
	\hat \beta_n^{ols} 
	& = \left(\frac{1}{n}\sum_{i = 1}^n X_i X_i^\top \right)^{-1}\frac{1}{n}\sum_{i = 1}^n X_i (X_i^\top\beta_0 + \eps_i) \\
	& = \beta_0 + \left(\frac{1}{n}\sum_{i = 1}^n X_i X_i^\top \right)^{-1}\frac{1}{n}\sum_{i = 1}^n X_i \eps_i.
\end{align*}

\paragraph{Proof of consistency}

To prove the consistency of $\hat \beta_n^{ols}$, it is sufficient to show that
\begin{align}\label{eq:convzero}
	\left(\frac{1}{n}\sum_{i = 1}^n X_i X_i^\top \right)^{-1}\frac{1}{n}\sum_{i = 1}^n X_i \eps_i \overset{p}{\to} \mbf{0}.
\end{align}
Assume that the data are IID and that $\E[XX^\top]$ exists and is nonsingular.
Then, by WLLN, $\frac{1}{n}\sum_{i = 1}^n X_i X_i^\top \overset{p}{\to} \E[XX^\top]$ element-wisely.
Further, by the continuity of matrix inversion, we can show that $\left(\frac{1}{n}\sum_{i = 1}^n X_i X_i^\top \right)^{-1} \overset{p}{\to} \E[XX^\top]^{-1}$.
In addition, it holds that $\frac{1}{n}\sum_{i = 1}^n X_i \eps_i \overset{p}{\to} \mbf{0}$ since
\[
	\E[X_i \eps_i ] = \E[X_i \E[\eps_i \mid X_i]] = \mbf{0}
\]
by the law of iterated expectations (LIE) and $\E[\eps_i \mid X_i] = 0$.
Finally, we can prove \eqref{eq:convzero} by Lemma \ref{lem:prodconv}.

\paragraph{Proof of asymptotic normality}
Suppose that $\E[\eps^2 \mid X ] = \sigma^2 < \infty$ (i.e., \textbf{homoskedasticity\index{homoskedasticity}}) for simplicity.
Write
\[
	\sqrt{n}(\hat \beta_n^{ols} - \beta_0) = \left(\frac{1}{n}\sum_{i = 1}^n X_i X_i^\top \right)^{-1}\frac{1}{\sqrt{n}}\sum_{i = 1}^n X_i \eps_i.
\]
Then, by Slutsky's theorem \ref{lem:slutsky}, the asymptotic distribution of  $\sqrt{n}(\hat \beta_n^{ols} - \beta_0)$ is equal to that of $\E[XX^\top]^{-1}\frac{1}{\sqrt{n}}\sum_{i = 1}^n X_i \eps_i$.
The mean of $\frac{1}{\sqrt{n}}\sum_{i = 1}^n X_i \eps_i$ is zero, and the variance of it is
\begin{align*}
	\Var\left(\frac{1}{\sqrt{n}}\sum_{i = 1}^n X_i \eps_i\right) 
	& = \E\left[ \frac{1}{n}\sum_{i = 1}^n\sum_{j = 1}^n X_i X_j^\top \eps_i \eps_j \right]\\
	& = \E\left[ \frac{1}{n}\sum_{i = 1}^n X_iX_i^\top \eps^2_i \right]\\
	& = \sigma^2 \frac{1}{n}\sum_{i = 1}^n  \E[  X_iX_i^\top ] = \sigma^2 \E[  XX^\top ],
\end{align*}
where the second equality follows by the independence assumption, and the third equality is by LIE and the homoskedasticity assumption.
Hence, by CLT, we have
\[
	\frac{1}{\sqrt{n}}\sum_{i = 1}^n X_i \eps_i \overset{d}{\to} \mbf{N}(\mbf{0}, \sigma^2 \E[XX^\top]),
\]
implying that 
\[
	\sqrt{n}(\hat \beta_n^{ols} - \beta_0) \overset{d}{\to} \mbf{N}(\mbf{0}, \sigma^2 \E[XX^\top]^{-1}).
\]

\hrulefill
\begin{exercise}\upshape
	Prove \eqref{eq:msemin}.
\end{exercise}
\begin{exercise}\upshape
	Suppose now that the error term is heteroskedastic in the following sense: $\E[\eps^2 \mid X] = \sigma^2(X)$.
	Derive the asymptotic distribution of $\sqrt{n}(\hat \beta_n^{ols} - \beta_0)$.
\end{exercise}
\begin{exercise}\upshape
	Consider a linear regression model:
	\[
	Y = X^\top \beta_0 + \eps,
	\]
	where $\E[\eps \mid X] = 0$.
	Define the \textbf{ridge regression} estimator:
	\[
	\hat \beta_n^{ridge} \equiv \left(\frac{1}{n}\sum_{i = 1}^n X_i X_i^\top + \lambda I_{\text{dim}(X)} \right)^{-1}\frac{1}{n}\sum_{i = 1}^n X_i Y_i,
	\]
	where $\lambda$ is a given positive number, and $I_{\text{dim}(X)}$ is an identity matrix of dimension $\text{dim}(X)$.
	Prove that (1) $\hat \beta_n^{ridge}$ is a biased estimator of $\beta_0$, and that (2) if $\lambda \to 0$, $\hat \beta_n^{ridge}$ is consistent for $\beta_0$.
\end{exercise}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Two-Stage Least Squares}\label{chap:2sls}
\section{Endogeneity and exogeneity}

Consider the following simple regression model:
\begin{align}\label{eq:simple}
Y = \beta_0 + X\beta_1 + \eps,
\end{align}
where $Y$ is a dependent variable, $X$ is a scalar explanatory variable, and $\eps$ is an unobserved error term satisfying $\E \eps = 0$.
$\beta_0$ and $\beta_1$ are unknown parameters to be estimated.
Taking the expectation of both sides of \eqref{eq:simple}, we obtain $\beta_0 = \E Y - \E [X] \beta_1$.
Then, the model \eqref{eq:simple} can be rewritten as
\begin{align}\label{eq:simplediff}
Y - \E Y = (X - \E X)\beta_1 + \eps.
\end{align}
Further, premultiplying both sides of \eqref{eq:simplediff} by $X$ and taking the expectation of them yield
\[
\underbrace{\E[X(Y - \E Y)]}_{\Cov(X,Y)} = \underbrace{\E[X (X - \E X)]}_{\Var(X)} \beta_1 + \underbrace{\E[X \eps]}_{\Cov(X, \eps)}.
\]
Assuming that $\Var(X) > 0$, divide the above equation by $\Var(X)$ to obtain
\begin{align}\label{eq:beta1}
\frac{\Cov(X,Y)}{\Var(X)} = \beta_1 + \frac{\Cov(X,\eps)}{\Var(X)}.
\end{align}
Notice that the sample analog of the left-hand side of \eqref{eq:beta1} is exactly the OLS estimator of the slope parameter $\beta_1$, and under standard conditions we have
\[
	\hat \beta_{n1}^{ols} \overset{p}{\to} \beta_1 + \frac{\Cov(X,\eps)}{\Var(X)}.
\]
This implies that the OLS estimator $\hat \beta_{n1}^{ols} $ cannot correctly (i.e., consistently) estimate $\beta_1$ if $\Cov(X, \eps) \neq 0$.
When $\Cov(X, \eps) = 0$ holds true, we say that $X$ is \textbf{exogenous\index{exogeneity}}.
If it is not satisfied, we say that $X$ is \textbf{endogenous\index{endogeneity}}.
\[
\begin{array}{ll}
\text{Exogeneity:} & \hspace{-5pt}\text{$X$ is uncorrelated with the error term $\eps$.} \\
\text{Endogeneity:} & \hspace{-5pt}\text{$X$ is correlated with the error term $\eps$.} \\
\end{array}
\]
Under exogeneity, the parameter $\beta_1$ can be characterized simply by 
\begin{equation*}
	\beta_1 = \frac{\Cov(X, Y)}{\Var(X) }.
\end{equation*}
On the other hand, if $X$ is endogenous, $\Cov(X, Y)/\Var(X)$ entails a bias, so-called \textbf{endogeneity bias\index{endogeneity bias}}, that amounts to $\Cov ( X, \eps )/\Var(X)$.
In general, the magnitude and even the sign of the endogeneity bias are uncertain because $\eps$ is unobservable.
Hence, in the presence of endogeneity, the OLS estimator is inconsistent, and thus the OLS estimate is uninformative about the true impact of $X$ on $Y$.
\bigskip

There are many potential sources of endogeneity.
The two most important sources are from \textbf{omitted variables\index{omitted variables}} and from \textbf{simultaneity\index{simultaneity}}. 

\paragraph{Omitted variables}
Consider the following multiple regression model with two explanatory variables $X_1$ and $X_2$:
\begin{align}\label{eq:multiple}
Y = \beta_0 + X_1 \beta_1 + X_2 \beta_2 + \eps.
\end{align}
Here, we assume that both $X_1$ and $X_2$ are exogenous: $\E[ X_1 \eps ] = \E [ X_2 \eps ] = 0$.
Then, when both $X_1$ and $X_2$ are observable, the slope parameters  $\beta_1$ and $\beta_2$ can be consistently estimated simply by regressing $Y$ on $(X_1, X_2)$.

Now consider a case in which $X_2$ is unobservable for some reason.
Excluding unobservable $X_2$ from the model \eqref{eq:multiple}, we regress $Y$ only on $X_1$ based on the following simple regression model:
\begin{align}\label{eq:simple2}
Y = \alpha +X_1 \beta_1 + \eta, 
\end{align}
where $\alpha = \beta_0 + \E[ X_2 ] \beta_2$, and $\eta$ is a new error term defined by
\[
\eta = ( X_2 - \E X_2  ) \beta_2 + \eps
\]
such that $\E\eta = 0$.
In this simple regression model, the explanatory variable $X_2$, which needs to be included in the model if $\beta_{2}\neq 0$, is omitted from the analysis. 
In order to correctly estimate $\beta_1$ based on the model \eqref{eq:simple2}, $X_1$ needs to be an exogenous variable in the sense that	$\E[ X_1 \eta ]  = 0$.
However, by the definition of $\eta$,
\begin{align*}
	\E[  X_1 \eta ]   
	& = \E\left[  X_1 \left(  X_2 - \E X_2   \right) \beta_2 + X_1 \eps \right] \\
	& = \Cov( X_1, X_2)  \beta_2 + \Cov( X_1, \eps)\\
	& = \Cov( X_1, X_2)  \beta_2.
\end{align*}
Hence, unless either $\Cov( X_1, X_2 ) = 0$ or $\beta_2 = 0$ (or both), $X_1$ is an endogenous variable in the model \eqref{eq:simple2}: $\E[X_1 \eta] \neq 0$ï¼Ž

\paragraph{Simultaneity}
As an example, consider estimating the effect of police on crime.
Let $X_i$ be the number of police officers in a district $i$ and $Y_i$ be the number of crime incidents in this district. 
It is expected that there exists a ``simultaneity" between police and crime; that is, if crime rate increases, a larger police force will be needed, at the same time, increasing police force reduces crime.
This relationship can be expressed as: 
\begin{align*}
Y_i  &  =\beta_0 + X_i \beta_1 + \eps_i \;\; \text{ ($\beta_1$: \# of  police officers $\rightarrow$ \# of crimes)} \\
X_i  &  =\gamma_0 + Y_i\gamma_1 + u_i \;\;\; \text{ ($\gamma_1$: \# of crimes $\rightarrow$ \# of police officers)},
\end{align*}
where $(\gamma_0, \gamma_1)$ is another set of unknown parameters, and $u$ is an unobserved error term.
For simplicity of discussion, we do not consider other factors that may influence on $Y$ and $X$.
If $X_i$ is an endogenous variable such that $\E[  X_i \eps_i ]  \neq 0$, simply regressing $Y_i$ on $X_i$ does not give us a consistent estimator of $\beta_1$.

According to the second model,
\begin{align*}
	\E[ X_i \eps_i ] 
	& = \E[(\gamma_0 + Y_i \gamma_1 + u_i) \eps_i ] \\
	& = \E[  Y_i \eps_i ]  \gamma_1 + \E[u_i \eps_i].
\end{align*}
For the second term on the right-hand side, if there are unobservable regional factors that can affect both $X$ and $Y$, we would have $\E[u_i \eps_i] \neq 0$.
Further, the first term is necessarily non-zero as long as $\gamma_1 \neq 0$ (i.e., simultaneity exists) because
\begin{align*}
	\E[  Y_i \eps_i ]
	& = \E[(\beta_0 + X_i \beta_1 + \eps_i) \eps_i ] \\
	& = \E[  X_i \eps_i ]  \beta_1 + \underbrace{\E[\eps^2_i]}_{ > \: 0}.
\end{align*}
Thus, endogeneity problem arises when simultaneity exists.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width = 10.6cm]{polcrime.png}
		\caption{Simultaneity between police and crime.}
	\end{center}
\end{figure}

\bigskip
\hrulefill
\begin{exercise}\upshape
	Give specific examples of endogeneity bias caused by (1) omitted variables and (2) simultaneity (other than those mentioned above).
\end{exercise}


\section{Instrumental variables}

A common approach to account for the endogeneity problem is to use \textbf{instrumental variables\index{instrumental variables}} (IV).
Again, consider the following simple regression model: 
\[
	Y = \beta_0 + X \beta_1 + \eps.
\]
Here, we assume that $X$ is endogenous: $\E[X\eps] \neq 0$.
An instrumental variable for $X$, which we denote by $Z$, is a random variable that satisfies the following conditions:
\[
\begin{array}{ll}
	\E[Z \eps] = \Cov(Z, \eps) = 0 & \textbf{Exclusion restriction\index{exclusion restriction}} \\
	\Cov(Z, X) \neq 0 & \textbf{Relevance condition\index{relevance condition}}
\end{array}
\]
The exclusion restriction says that the instrumental variable $Z$ is uncorrelated with the error term $\eps$.
This implies that $Z$ is not a direct determinant of $Y$; if $Z$ directly affects $Y$, the information about $Z$ must be contained in $\eps$, leading to a correlation of $Z$ and $\eps$ (this is why the condition is called ``exclusion" restriction).
The relevance condition requires that the instrumental variable $Z$ is a determinant of the endogenous variable $X$.

\begin{example}\upshape
	As a typical econometric application, suppose that $Y$ is an individual's wage and $X$ is his education in years.
	It would be expected that the higher the individual's ability, the higher his wage.
	However, since one's ability is hard to measure correctly, the effect of ability on $Y$ will be partly included in the error term $\eps$.
	Also, there must exist a positive correlation between one's ability and education.
	Consequently, a positive correlation between $X$ and $\eps$ arises naturally; that is, $X$ should be treated as an endogenous variable.
	Here, let $Z$ be the individual's parents' education level.
	It would be legitimate to suppose that the parents' education does not ``directly" affect his own wage $Y$ (but influences on $Y$ only through $X$), and it is surely correlated with his education level.
	Thus, the parents' education level can be a valid instrument for $X$.
\end{example}

\begin{example}\upshape
	Consider once again the simultaneous relationship of police and crime.
	Here, let $Y_{it}$ and $X_{it}$ be the number of crime incidents and the size of police force in city $i$ year $t$, respectively.
	As an instrumental variable for $X_{it}$, \cite{levitt1997using} employed an indicator variable $Z_{it}$ which takes one if $t$ is the mayoral election year in city $i$.
	In years when there is an election, the incumbent mayors would have an incentive to increase the police force to reduce crime (which was indeed confirmed as statistically significant in \cite{levitt1997using}).
	Also, the presence of elections should not have ``direct'' impacts on the number of crimes.
	Thus, $Z_{it}$ is considered to be a valid instrument for $X_{it}$. 
\end{example}

Now, suppose that we have a valid instrumental variable $Z$ satisfying both the exclusion restriction and relevance condition.
Then, premultiplying both sides of \eqref{eq:simplediff} by $Z$ and taking the expectation of them yield
\[
	\underbrace{\E[Z (Y - \E Y)]}_{\Cov(Z, Y)} = \underbrace{\E[Z (X - \E X)]}_{\Cov(Z, X) \neq 0} \beta_1 + \underbrace{\E[Z \eps]}_{\Cov(Z, \eps) = 0}.
\]
Thus, we obtain
\[
	\beta_1 = \frac{\Cov(Z, Y)}{\Cov(Z, X)}.
\]
Here, suppose that the sample $\{(Y_i, X_i, Z_i): 1 \le i \le n\}$ is available, and let $\what{\Cov}_n(Z, Y)$ and $\what{\Cov}_n(Z, X)$ be the sample covariances of $(Z, Y)$ and $(Z, X)$, respectively.
Then, the above result implies that $\beta_1$ can be estimated by
\[
	\what{\beta}_{n1} \equiv \frac{\what{\Cov}_n(Z, Y)}{\what{\Cov}_n(Z, X)}.
\] 
Further, note that 
\begin{align*}
	\what{\Cov}_n(Z, Y) 
	& = \frac{1}{n}\sum_{i = 1}^n Z_i (Y_i - \bar Y_n) \\
	& = \frac{1}{n}\sum_{i = 1}^n Z_i ((\beta_0 + X_i\beta_1 + \eps_i) - (\beta_0 + \bar X_n \beta_1 + \bar \eps_n))\\ 
	& =\frac{1}{n}\sum_{i = 1}^n Z_i ((X_i - \bar X_n) \beta_1 + (\eps_i - \bar \eps_n))\\ 
	& = \what{\Cov}_n(Z, X)\beta_1 + \what{\Cov}_n(Z, \eps),
\end{align*}
where $\bar Y_n$, $\bar X_n$, and $\bar \eps_n$ are the sample averages of $Y$, $X$, and $\eps$, respectively.
Hence, 
\begin{align}\label{eq:weakIV}
	\what{\beta}_{n1} = \beta_1 + \frac{\what{\Cov}_n(Z, \eps)}{\what{\Cov}_n(Z, X)}.
\end{align}
The second term on the right-hand side of \eqref{eq:weakIV} is the finite sample estimation bias.\footnote{
	Thus, this IV-based estimator is not an unbiased estimator.
}
If $\Cov(Z,X)$ is close to zero, i.e., if $Z$ is almost irrelevant to $X$, $\what{\Cov}_n(Z, X)$ is also close to zero.
Thus, in this case, the estimation bias of $\what{\beta}_{n1}$ can be very large.
This problem is known as the \textbf{weak instruments problem\index{weak instruments problem}}. 
There are several formal tests for weak instruments (see, e.g., \cite{stock2005testing}); however, the details are omitted here.

\hrulefill
\begin{exercise}\upshape
	Suppose that we would like to estimate the causal impact of aircraft noise on land prices by a regression analysis.
	In this regression model, we should suspect that the aircraft noise variable is endogenous. Explain why, and give an example of a valid instrumental variable in this analysis.
\end{exercise}

\section{2SLS procedure}

In the following, we consider a more general case with multiple regressors and multiple instrumental variables:
\begin{align*}
	Y = D \cdot \alpha_0 + X^\tr \beta_0 + \eps,
\end{align*}
where $X = (X_1, \ldots , X_{d_x})^\tr$ is a vector of exogenous explanatory variables (including a constant term), and $D$ is an endogenous explanatory variable.
Suppose that we have a vector of instrumental variables $Z_1 = (Z_{11}, \ldots, Z_{1d_z})^\tr$ for $D$.

We modify the above estimation procedure for a simple regression model as follows.
First, let $Z = (Z_1^\tr, X^\tr)^\tr$ be a vector of all exogenous variables, and assume that the inverse matrix $\E[Z Z^\tr]^{-1}$ exists.
Further, let $\mcl{P}_Z(A)$ be the orthogonal projection of $A$ onto $Z$; that is, $\mcl{P}_Z(A) \equiv Z^\tr \E[ZZ^\tr]^{-1}\E[Z A]$.
When $A$ is a vector $A = (A_1, \ldots A_k)^\tr$, $\mcl{P}_Z(A)$ is defined as 
\[
	\mcl{P}_Z(A) = (Z^\tr \E[Z Z^\tr]^{-1} \E[ZA_1], \ldots , Z^\tr \E[Z Z^\tr]^{-1} \E[ZA_k])^\tr.
\]
Rewrite the model as follows:
\begin{align}\label{eq:projection}
\begin{split}
	Y 
	& = \mcl{P}_Z(D) \cdot \alpha_0 + X^\tr \beta_0 +u \\
	& = \mcl{P}_Z(H)^\tr \theta_0 +u ,
\end{split}
\end{align}
where $H = (D, X^\tr)^\tr$, $\theta_0 = (\alpha_0, \beta_0^\tr)^\tr$, and $u = (D - \mcl{P}_Z(D)) \cdot \alpha_0 + \eps$.
The second equality in \eqref{eq:projection} is due to the fact that $\mcl{P}_Z(X) = X$ (see the supplementary material in this chapter).
By the exclusion restriction and \eqref{eq:ZX} below, we can find that 
\begin{align*}
	\E[\mcl{P}_Z(H) u] 
	& = \E\left[
	\begin{array}{c}
	\mcl{P}_Z(D) u \\
	X u
	\end{array}
	\right] \\
		& = \E\left[
	\begin{array}{c}
	\mcl{P}_Z(D) (D - \mcl{P}_Z(D))  \\
	X (D - \mcl{P}_Z(D))
	\end{array}
	\right] \cdot \alpha_0  = \underset{(1 + d_x) \times 1}{\mbf{0}}.
\end{align*}
This result implies that $\theta_0$ can be estimated by the least squares regression of $Y$ on $(\mcl{P}_Z(D), X)$, or equivalently, on $\mcl{P}_Z(H)$.
Figure \ref{fig:geo2sls} provides a simple geometry of the above discussion.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width = 9cm]{projection.png}
		\caption{Orthogonal projection of $D$ onto $Z = (Z_1, X)$.}
		\label{fig:geo2sls}
	\end{center}
\end{figure}

Now, suppose that we have data of $n$ observations $\{(Y_i, D_i, X_i, Z_{1i}): 1 \le i \le n\}$.
For a sample analog to the above procedure, the parameter $\theta_0$ can be estimated in the following two steps:

\textbf{Step (1)}: perform a least squares regression of $D$ on $Z = (Z_1^\tr, X^\tr)^\tr$, and compute the predicted value of $D$ by $\what D = Z^\tr \what \gamma_n$, where
\[
\what \gamma_n \equiv \left( \frac{1}{n}\sum_{i=1}^n Z_i Z_i^\tr \right)^{-1}\frac{1}{n}\sum_{i=1}^n Z_i D_i. 
\]
(At this step, it is important to check the significance of $\hat \gamma_n$.
If all elements of $\what \gamma_n$ are close to zero, the weak instruments problem can occur.)
Let $\what H = (\what D, X^\tr)^\tr$.

\textbf{Step (2)}: perform a least squares regression of $Y$ on $\what H$ to estimate $\theta_0$:
\[
	\what \theta_n^{2sls} \equiv \left( \frac{1}{n}\sum_{i=1}^n \what H_i \what H_i^\tr \right)^{-1}\frac{1}{n}\sum_{i=1}^n \what H_i Y_i.
\]
The above estimation procedure is called the \textbf{two-stage least squares\index{two-stage least squares}} (2SLS) estimator.

Noting the second equality in \eqref{eq:projection}, the 2SLS estimator can be, in fact, implemented in one step.
To do so, we introduce the following matrix notations: $\mbf{Y}_n = (Y_1, \ldots , Y_n)^\tr$, $\mbf{D}_n = (D_1, \ldots, D_n)^\tr$, $\mbf{X}_n = (X_1, \ldots, X_n)^\tr$, $\mbf{Z}_n = (Z_1, \ldots, Z_n)^\tr$, and $\mbf{H}_n = (H_1, \dots , H_n)^\tr$.
Further, let $\mcl{P}_{\mbf{Z}_n}$ be an empirical projection matrix, which is defined by
\[
	\mcl{P}_{\mbf{Z}_n} \equiv \mbf{Z}_n(\mbf{Z}_n^\tr \mbf{Z}_n)^{-1} \mbf{Z}_n^\tr.
\]
Then, $\what{\mbf{D}}_n = (\what D_1, \ldots , \what D_n)^\tr$ is obtained by $\what{\mbf{D}}_n = \mcl{P}_{\mbf{Z}_n} \mbf{D}_n$.
In addition, by the same argument as the fact $\mcl{P}_Z(X) = X$, we can easily see that $\mbf{X}_n = \mcl{P}_{\mbf{Z}_n} \mbf{X}_n$.
Therefore, $\what{\mbf{H}}_n \equiv [\what{\mbf{D}}_n \; ; \; \mbf{X}_n] = \mcl{P}_{\mbf{Z}_n}\mbf{H}_n$.
Consequently, noting the fact that $\mcl{P}_{\mbf{Z}_n}\mcl{P}_{\mbf{Z}_n} = \mcl{P}_{\mbf{Z}_n}$,\footnote{This property is called \textit{idempotency}.} the 2SLS estimator $\what \theta_n^{2sls}$ can be numerically equivalently expressed as 
\begin{align}\label{eq:2slsMat}
	\begin{split}
	\what \theta_n^{2sls}
	& = \left(\what{\mbf{H}}_n^\tr \what{\mbf{H}}_n \right)^{-1} \what{\mbf{H}}_n^\tr \mbf{Y}_n\\
	& = \left(\mbf{H}_n^\tr \mcl{P}_{\mbf{Z}_n} \mcl{P}_{\mbf{Z}_n} \mbf{H}_n \right)^{-1} \mbf{H}_n^\tr \mcl{P}_{\mbf{Z}_n} \mbf{Y}_n \\
	& = \left(\mbf{H}_n^\tr  \mbf{Z}_n(\mbf{Z}_n^\tr \mbf{Z}_n)^{-1} \mbf{Z}_n^\tr \mbf{H}_n \right)^{-1} \mbf{H}_n^\tr  \mbf{Z}_n(\mbf{Z}_n^\tr \mbf{Z}_n)^{-1} \mbf{Z}_n^\tr \mbf{Y}_n.
	\end{split}
\end{align}

\section{Asymptotic properties}\label{sec:2slsAsymptotics}

The asymptotic properties of the 2SLS estimator \eqref{eq:2slsMat} can be relatively easily derived.
First, noting that $\mbf{Y}_n = \mbf{H}_n^\top\theta_0 + \mcl{E}_n$, where $\mcl{E}_n = (\eps_1, \ldots , \eps_n)^\tr$, we can obtain
\begin{align*}
	\what \theta_n^{2sls} 
	& = \theta_0 + \left[\mbf{H}_n^\top \mbf{Z}_n(\mbf{Z}_n^\top \mbf{Z}_n)^{-1}\mbf{Z}_n^\top \mbf{H}_n  \right]^{-1}\mbf{H}_n^\top \mbf{Z}_n(\mbf{Z}_n^\top \mbf{Z}_n)^{-1}\mbf{Z}_n^\top\mcl{E}_n.
\end{align*}
by WLLN, under standard conditions, we have $\mbf{H}_n^\top \mbf{Z}_n/n \overset{p}{\to} \E[H Z^\tr]$,  $\mbf{Z}_n^\tr \mbf{Z}_n/n \overset{p}{\to} \E[Z Z^\tr]$, and  $\mbf{Z}_n^\tr \mcl{E}_n/n \overset{p}{\to} \E[Z \eps] = \underset{(d_z + d_x)\times 1}{\mathbf{0}}$ as the sample size increases to infinity.
Thus,
\begin{align*}	
	\what \theta_n^{2sls} - \theta_0
	& = \left[(\mbf{H}_n^\top \mbf{Z}_n/n)(\mbf{Z}_n^\top \mbf{Z}_n/n)^{-1}(\mbf{Z}_n^\top \mbf{H}_n/n ) \right]^{-1}(\mbf{H}_n^\top \mbf{Z}_n/n)(\mbf{Z}_n^\top \mbf{Z}_n/n)^{-1}(\mbf{Z}_n^\top\mcl{E}_n/n) \\
	& \overset{p}{\to} \left[ \E[H Z^\tr] \E[Z Z^\tr]^{-1} \E[Z H^\tr] \right]^{-1} \E[H Z^\tr] \E[Z Z^\tr]^{-1} \underbrace{\E[Z \eps]}_{= \; \mathbf{0}} = \mathbf{0}
\end{align*}
by Lemma \ref{lem:prodconv} and the continuous mapping theorem, implying that the 2SLS estimator $\what \theta_n^{2sls}$ is consistent for $\theta_0$.

Next, suppose for simplicity that the error terms $\eps_i$'s are IID with variance $\E[\eps^2] = \sigma^2$.
Further, assume that $\eps$ is independent of $Z$ (note that this is a stronger condition than the uncorrelation between $\eps$ and $Z$).
Then, by CLT, it would hold that $\mbf{Z}_n^\top\mcl{E}_n/\sqrt{n} \overset{d}{\to} \mbf{N}(\mbf{0}, \E[Z Z^\tr]\sigma^2)$.
Hence, by Slutsky's theorem \ref{lem:slutsky}, we obtain the asymptotic normality of $\sqrt{n}(\what \theta_n^{2sls} - \theta_0)$ as follows:
\begin{align*}
	\sqrt{n}(\what \theta_n^{2sls} - \theta_0) 
	& = \left[(\mbf{H}_n^\top \mbf{Z}_n/n)(\mbf{Z}_n^\top \mbf{Z}_n/n)^{-1}(\mbf{Z}_n^\top \mbf{H}_n/n ) \right]^{-1}(\mbf{H}_n^\top \mbf{Z}_n/n)(\mbf{Z}_n^\top \mbf{Z}_n/n)^{-1}\mbf{Z}_n^\top\mcl{E}_n/\sqrt{n} \\
	& \overset{d}{\to} \mbf{N}\left(\mbf{0}, \sigma^2  \left[ \E[H Z^\tr] \E[Z Z^\tr]^{-1} \E[Z H^\tr] \right]^{-1}\right).
\end{align*}

\hrulefill
\begin{exercise}\upshape
	Explain what happens if we create $\hat D$ using only $Z_1$ without $X$ and perform a 2SLS estimation.
	Is such an estimator consistent? If yes, prove that, and if no, explain under what additional condition(s) the estimator becomes consistent.
\end{exercise}

\section{Durbin-Wu-Hausman test for endogeneity}

Note that the 2SLS estimator is consistent and asymptotically normal even when the variable $D$ is in fact exogenous. 
Of course, if $D$ is an exogenous variable, we can use the OLS estimator, and, in general, the OLS estimator is more efficient than the 2SLS estimator when both estimators are available.
Thus, statistically testing the appropriateness of using OLS or the necessity of using 2SLS is useful.

Here, we consider the following null hypothesis
\[
	\mbb{H}_0: \Cov(D, \eps) = 0,
\]
and the alternative hypothesis is its negation
\[
	\mbb{H}_1: \Cov(D, \eps) \neq 0.
\]
Under the null hypothesis $\mbb{H}_0$, both OLS and 2SLS estimators of $\alpha_0$, say $\hat \alpha_n^{ols}$ and $\hat \alpha_n^{2sls}$, respectively,  are consistent and asymptotically normal.
However under $\mbb{H}_1$, the OLS estimator $\hat \alpha_n^{ols}$ converges to $\alpha_0 + \text{endogeneity bias}$, while the 2SLS estimator is still consistent for $\alpha_0$.
Thus the difference between the OLS and 2SLS estimators is a valid test statistic for endogeneity. 

Suppose that $\mbb{H}_0$ is true.
Then, since both $\hat \alpha_n^{ols}$ and $\hat \alpha_n^{2sls}$ are asymptotically distributed as normal, we can show that the difference of them $\hat \alpha_n^{ols} - \hat \alpha_n^{2sls}$ is also asymptotically normal.
Thus, letting 
\[
	T_n \equiv \frac{\hat \alpha_n^{ols} - \hat \alpha_n^{2sls}}{\sqrt{\Var(\hat \alpha_n^{ols} - \hat \alpha_n^{2sls})}},
\]	
we have $T_n \overset{d}{\to} N (0,1)$ and also $T_n^2 \overset{d}{\to} \chi^2(1)$ under $\mbb{H}_0$, where $\chi^2(1)$ denotes the chi-square distribution with one degree of freedom.
Thus, if $T_n^2 > 3.841\ldots$ for instance, we can reject $\mbb{H}_0$ at the 5\% significance level.

In a more general setting where $q$ endogenous regressors $D = (D_1, \ldots, D_q)^\top$ exist, the statistic $T_n$ can be defined as $T_n \equiv \left[\Cov(\hat \alpha_n^{ols} - \hat \alpha_n^{2sls})\right]^{-1/2}(\hat \alpha_n^{ols} - \hat \alpha_n^{2sls})$, and it holds that $T_n^\top T_n \overset{d}{\to} \chi^2(q)$.
The above testing procedure for endogeneity is called the \textbf{Durbin-Wu-Hausman test\index{Durbin-Wu-Hausman test}}.

 
\section{A causal interpretation of the 2SLS estimate}

In this section, for simplicity, suppose that we have a dummy endogenous variable $D \in \{0,1\}$ and a dummy instrumental variable $Z \in \{0,1\}$ only, with no other covariates.
Here, we are interested in estimating the ``causal'' impact of $D$ on $Y$, rather than a mere correlation between them.

For each $d \in \{0,1\}$, define the \textbf{potential outcome} $Y(d)$ as the outcome that will be obtained when $D = d$.
Note that we never observe outcomes for the same individual under different treatment status at the same time.
That is, when $D = 1$ (resp. $D = 0$) is realized, we can observe only $Y(1)$ (resp. $Y(0)$), but $Y(0)$ (resp. $Y(1)$) is an unobservable ``counterfactual'' outcome.
Thus, the ``observed'' outcome $Y$ can be written as
\[
	Y = D \cdot Y(1) + (1 - D) \cdot Y(0).
\]
In this framework, the causal effect of $D$ on $Y$ is defined as
\[
	Y(1) - Y(0).
\]
This quantity is referred to as the \textbf{treatment effect\index{treatment effect}}, and $D$ is called the \textbf{treatment variable\index{treatment variable}} in this context.
In this way, we can define the causal effect of the treatment $D$ on the outcome $Y$ as the difference of the potential outcomes $Y(0)$ and $Y(1)$.
This approach of defining causal relationship between variables is called the \textbf{Rubin's Causal Model\index{Rubin's Causal Model}}.
Since we cannot observe both potential outcomes $Y_i(0)$ and $Y_i(1)$ at the same time (unless there is someone completely identical to $i$), it is generally impossible to estimate the individual-specific treatment effects.
Therefore, studies on treatment effects typically aim at estimating treatment effects averaged over some subpopulation; for example, 
\begin{align*}
	&\text{ATE (the average treatment effect)}: \E[Y(1) - Y(0)]\\
	&\text{CATE (the conditional average treatment effect)}: \E[Y(1) - Y(0) \mid X = x] \\
	&\text{ATET (the average treatment effect on the treated)}: \E[Y(1) - Y(0) \mid D = 1].
\end{align*}

In a seminal paper, \cite{imbens1994identification} studied conditions under which the 2SLS estimate can be interpreted as the \textbf{LATE} (local average treatment effect) -- the average treatment effect for those whose treatment status is altered by the instrument. 
Note that when both $D$ and $Z$ are dummy variables, we can classify individuals into the following four latent types:
\[
\begin{array}{ll}
	\text{\bf Complier:} & Z = 0 \Rightarrow D = 0 \;\; \& \;\; Z = 1 \Rightarrow D = 1 \\
	\text{\bf Defier:}   & Z = 0 \Rightarrow D = 1 \;\; \& \;\; Z = 1 \Rightarrow D = 0 \\
	\text{\bf Always-taker:} & Z = 0 \Rightarrow D = 1 \;\; \& \;\; Z = 1 \Rightarrow D = 1 \\
	\text{\bf Never-taker:}  & Z = 0 \Rightarrow D = 0 \;\; \& \;\; Z = 1 \Rightarrow D = 0 
\end{array}
\]
For example, consider estimating the causal effect of college degree ($D = 1$: college graduate or higher, $D = 0$: otherwise) on annual income ($Y$).
As an instrumental variable for $D$, we consider father's education level ($Z = 1$: the individual's father has a college degree or higher, $Z = 0$: otherwise).
Then, the compliers are those who will (not) go on to a college if their fathers (do not) have a college degree, the always-takers are those who will go on to a college irrespective of their fathers' education level, and so forth.
If we can assume that there are no defiers, we can distinguish the latent types from the observed data in the following sense: 
\begin{center}
	$
	\begin{array}
	[c]{cc|ccc}
	&  &  & Z & \\
	&  & 0 &  & 1\\\hline
	& 0 & \text{Complier or Never-Taker} &  & \text{Never-taker}\\
	D &  &  &  & \\
	& 1 & \text{Always-taker} &  & \text{Complier or Always-taker}
	\end{array}
	$
\end{center}
The assumption of no defiers is called \textbf{monotonicity assumption\index{monotonicity assumption}} because, when no defiers exist, increasing $Z$ from 0 to 1 never decreases $D$.
In addition, assume that $Z$ is independent of the potential outcomes (i.e., the exclusion restriction) and the individuals' types such that
\begin{align*}
	& \Pr(\text{Always-taker} \mid Z = 1) = \Pr(\text{Always-taker} \mid Z = 0) = \pi_A\\
	& \Pr(\text{Never-taker} \mid Z = 1) = \Pr(\text{Never-taker} \mid Z = 0) = \pi_N\\
	& \Pr(\text{Complier} \mid Z = 1) = \Pr(\text{Complier} \mid Z = 0) = \pi_C.
\end{align*}
Under these conditions, one can show that
\begin{align}\label{eq:LATE}
\begin{split}
	\underset{\text{\bf LATE}}{\E[Y(1) - Y(0) \mid \text{Complier}]} 
	& = \frac{\E[Y \mid Z = 1] - \E[Y \mid Z = 0]}{\E[D \mid Z = 1] - \E[D \mid Z = 0]}\\
	& = \frac{\Cov(Z, Y)}{\Cov(Z, D)}.
\end{split}
\end{align}
The term on the left-hand side of \eqref{eq:LATE} is called \textbf{LATE\index{LATE}} (the local average treatment effect).
The first equality of \eqref{eq:LATE} can be shown as follows.
Observe that
\small{\begin{align*}
	\E[Y \mid Z = 1 ] 
	& = \E[Y \mid Z = 1, \text{AT} ] \cdot \Pr(\text{AT} \mid Z = 1) + \E[Y \mid Z = 1, \text{NT} ] \cdot \Pr(\text{NT} \mid Z = 1) + \E[Y \mid Z = 1, \text{C} ] \cdot \Pr(\text{C} \,mid Z = 1)\\
	& = \E[Y(1) \mid Z = 1, \text{AT} ] \cdot \pi_A + \E[Y(0) \mid Z = 1, \text{NT} ] \cdot \pi_N + E[Y(1) \mid Z = 1, \text{C} ] \cdot \pi_C\\
	& = \E[Y(1) \mid \text{AT} ] \cdot \pi_A  + E[Y(0) \mid \text{NT} ] \cdot \pi_N  + E[Y(1) \mid \text{C} ] \cdot \pi_C,
\end{align*}}
\normalsize
where the last equality follows from the exclusion restriction.
Similarly, we can show that
\begin{align*}
	\E[Y \mid Z = 0]
	& = \E[Y(1) \mid \text{AT} ] \cdot \pi_A + \E[ Y(0) \mid \text{NT} ] \cdot \pi_N  + \E[ Y(0) \mid \text{C} ]  \cdot \pi_C
\end{align*}
Thus, 
\begin{align*}
	\E [ Y \mid Z = 1 ] - E[ Y \mid Z = 0 ] 
	&= \left\{ \E[ Y(1) \mid \text{Complier} ] - \E[ Y(0) \mid \text{Complier} ] \right\} \cdot \pi_C\\
	& = \mbf{LATE}\cdot\pi_C
\end{align*}
Here, since we have assumed that there are no defiers, we must have
\begin{align*}
	\E[ D \mid Z = 1 ]   &  = \pi_C + \pi_A \\
	\E[ D \mid Z = 0 ]   &  = \pi_A.
\end{align*}
Therefore, it holds that
\begin{equation*}
	\E [D \mid Z = 1 ] - \E[ D \mid Z = 0 ] = \pi_C.
\end{equation*}
Finally, combining these results gives the desired equality, provided that $\pi_C > 0$.

The second equality of \eqref{eq:LATE} is left for an exercise.
Recall that the slope coefficient $\alpha_0$ in the regression model $Y = \beta_0 + D \cdot \alpha_0 + \eps$ can be expressed as $\alpha_0 = \Cov(Z, Y)/\Cov(Z, D)$.
This means that the 2SLS estimator for $\alpha_0$ can be seen as an estimator of the LATE parameter.

\hrulefill
\begin{exercise}\upshape
	Prove the following equality: $\displaystyle \frac{\Cov(Z, Y)}{\Cov(Z, D)} = \frac{\E[Y \mid Z = 1] - \E[Y \mid Z = 0]}{\E[D \mid Z = 1] - \E[D \mid Z = 0]}$.
\end{exercise}

\section{Numerical simulations with \textbf{R}}

In this section, we see how the endogeneity problem is serious and can be solved by the method of instrumental variables.
We first generate data of sample size $500$ as follows.
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
N   <- 500      # sample size
X   <- rnorm(N) # exogenous regressor
err <- rnorm(N) # error term

Z1 <- runif(N, -1, 1)  # instrumental variable
D  <- 1 + Z1 + 0.5*err # endogenous regressor
Y  <- 1 + D + X + err  # outcome variable
\end{lstlisting}
We create the variable $D$ so that it is strongly correlated with the error term; i.e., $D$ is endogenous.
The variable $Z_1$ is an instrument for $D$.
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
>  cov(D, err)  # endogeneity
[1] 0.5744865
>  cov(Z1, err) # exclusion restriction
[1] 0.03393532
>  cov(Z1, D)   # relevance condition
[1] 0.3617284
\end{lstlisting}
First, we run an OLS regression without accounting for the endogeneity of $D$.
The result is as follows.
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
> # OLS estimation #
> 
>  lm(Y ~ D + X)

Call:
lm(formula = Y ~ D + X)

Coefficients:
(Intercept)            D            X  
0.1229       1.8853       0.9780  
\end{lstlisting}
Recalling that the true coefficient of $D$ is one, we can see that the OLS coefficient estimate for $D$ is severely biased.
Next, we implement the 2SLS estimation.
Following \eqref{eq:2slsMat}, we can run the 2SLS regression in the following manner. 
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
# 2SLS estimation #

H <- cbind(1,D, X)
Z <- cbind(1,Z1,X)

Proj  <- Z%*%solve(t(Z)%*%Z)%*%t(Z)
theta <- solve(t(H)%*%Proj%*%H)%*%t(H)%*%Proj%*%Y

>  theta
[,1]
0.9056225
D 1.0944848
X 0.9809257
\end{lstlisting}
Then, we can observe that the 2SLS estimator performs accurately.
However, as mentioned above, the performance of the 2SLS estimator is crucially dependent on the ``strength'' of the instruments.
To see this, we recreate $D$ so that the correlation between $D$ and $Z_1$ is much weaker than the current case.
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
# Weak instruments problem #

D  <- 1 + 0.2*Z1 + 0.5*err # endogenous regressor
Y  <- 1 + D + X + err      # outcome variable

>  cov(Z1, D) # relevance condition
[1] 0.08591981
\end{lstlisting}

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
# 2SLS estimation #

H <- cbind(1,D, X)
Z <- cbind(1,Z1,X)

Proj  <- Z%*%solve(t(Z)%*%Z)%*%t(Z)
theta <- solve(t(H)%*%Proj%*%H)%*%t(H)%*%Proj%*%Y

>  theta
[,1]
0.6027511
D 1.3973392
X 0.9839573
\end{lstlisting}
As can be seen from this result, the estimation performance of the 2SLS is clearly deteriorated when the instrument is weak.

\section*{Appendix: A property of the orthogonal projection}
	
Using the formula in \eqref{eq:inverse}, denoting $\E_{AB} \equiv \E[A B^\tr] $, we have
\small\begin{align}\label{eq:ZX}
\begin{split}
\E[ZZ^\tr]^{-1}\E[ZX^\tr] 
	& = \left(\begin{array}{cc}
		\E_{Z_1 Z_1} & \E_{Z_1 X} \\
		\E_{X Z_1} & \E_{X X} 
		\end{array}	\right)^{-1}
	\left( \begin{array}{c}
		\E_{Z_1 X} \\
		\E_{X X} 
	\end{array}	\right) \\
	& \hspace{-80pt}= \left(\begin{array}{cc}
		(\E_{Z_1 Z_1}  - \E_{Z_1 X} \E_{X X} ^{-1}\E_{X Z_1} )^{-1} & -(\E_{Z_1 Z_1}  - \E_{Z_1 X} \E_{X X}^{-1} \E_{X Z_1} )^{-1} \E_{Z_1 X} \E_{X X}^{-1} \\
		-(\E_{X X}  - \E_{X Z_1} \E_{Z_1 Z_1}^{-1} \E_{Z_1 X} )^{-1} \E_{X Z_1} \E_{Z_1 Z_1}^{-1}  & (\E_{X X} - \E_{X Z_1} \E_{Z_1 Z_1}^{-1} \E_{Z_1 X} )^{-1}
	\end{array} \right)
	\left( \begin{array}{c}
		\E_{Z_1 X} \\
		\E_{X X} 
	\end{array}	\right)\\
	& \hspace{-80pt}= 
	\left(\begin{array}{c}
	\underset{d_z \times d_x}{\mathbf{0}} \\
	-(\E_{X X}  - \E_{X Z_1} \E_{Z_1 Z_1}^{-1} \E_{Z_1 X} )^{-1} \E_{X Z_1} \E_{Z_1 Z_1}^{-1} \E_{Z_1 X} + (\E_{X X} - \E_{X Z_1} \E_{Z_1 Z_1}^{-1} \E_{Z_1 X} )^{-1} \E_{XX}
	\end{array} \right) \\
	& \hspace{-80pt}= 
	\left(\begin{array}{c}
	\underset{d_z \times d_x}{\mathbf{0}} \\
	(\E_{X X}  - \E_{X Z_1} \E_{Z_1 Z_1}^{-1} \E_{Z_1 X} )^{-1} (\E_{X X} - \E_{X Z_1} \E_{Z_1 Z_1}^{-1} \E_{Z_1 X} )
	\end{array} \right)
	= 
	\left(\begin{array}{c}
	\underset{d_z \times d_x}{\mathbf{0}} \\
	I_{d_x}
	\end{array} \right).
\end{split}
\end{align}\normalsize
Hence,
\begin{align*}
	\mcl{P}_Z(X)
	& = \left( Z^\tr \E[ZZ^\tr]^{-1} \E[ZX^\tr] \right)^\tr  = \left( \left[ Z^\tr_1 \; , \; X^\tr \right] \left(\begin{array}{c}
	\underset{d_z \times d_x}{\mathbf{0}} \\
	I_{d_x}
	\end{array} \right) \right)^\tr = X.
\end{align*}
That is, the orthogonal projection of $X$ onto $Z = (Z_1, X)$ is $X$ itself.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Maximum Likelihood Estimation}\label{chap:mle}
\section{Maximum likelihood principle}
The maximum likelihood method is a basic statistical technique for estimating parameters by finding the ``most likely'' data-generating process to explain the observed data.
To what extent it is ``likely" is measured by the joint probability/density. 
\begin{align*}
\text{Least Squares} &= \text{estimate parameters by minimizing the sum of squared errors}\\
\text{Maximum Likelihood} &= \text{estimate parameters by maximizing the probability of observing the data}
\end{align*}

For example, suppose that a random variable $X$ is distributed as normal; but we do not know its mean $\E X = 2$ and variance $\Var(X) = 9$.
We have a set of observations $\{X_1, \ldots ,X_n\}$ of sample size $n$ independently drawn from the same distribution as $X$.
The relative frequency histogram of this data is given below ($n = 200$):
\begin{center}
	\includegraphics[width=7cm]{hist.png}
\end{center}
As a candidate distribution, for example, consider $N(6, 2)$, which is shown by a blue curve in the next figure:
\begin{center}
\includegraphics[width=7cm]{hist2.png}
\end{center}
If $N(6, 2)$ is the true distribution of $X$, the values in the neighborhood of $6$ would be most likely observable.
In other words, it is less likely that our data are generated from $N(6, 2)$.
The probability distribution that generates our data with highest likelihood is the one that best fits the histogram:
\begin{center}
\includegraphics[width=7cm]{hist3.png}
\end{center}
The red curve in the above figure is the normal distribution with its mean and variance being equal to the sample average (1.85) and sample variance (8.55) of $X$, respectively.
Indeed, as stated below, the sample average and sample variance are the \textbf{maximum likelihood estimators\index{maximum likelihood estimator}} (MLE) of $\E X$ and $\Var(X)$, respectively.

\begin{example}[Normal random variable]\label{ex:normal}\upshape
Suppose that we have a set of observations $\{X_1, \ldots, X_n\}$ of size $n$, which are known to be IID normal $N(\mu_0, \sigma_0^2)$, but $\mu_0$ and $\sigma_0^2$ are unknown.
The joint density of the data can be written as
\[
	\text{Joint density of $\{X_1, \ldots, X_n\}$} = \prod_{i=1}^n f_X(X_i)
\] 
by the IID assumption, where $f_X(x) = \frac{1}{\sqrt{2 \pi \sigma_0^2}}\exp\left(-\frac{(x - \mu_0)^2}{2 \sigma_0^2}\right)$.
Since $\mu_0$ and $\sigma_0^2$ are unknown, we consider their candidate values $\mu$ and $\sigma^2$, respectively, and let
\[
	L_n(\theta) \equiv \prod_{i=1}^n f_X(X_i; \mu, \sigma^2),
\]
where $\theta = (\mu, \sigma^2)$, and $f_X(x; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}\exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)$.
The function $L_n(\theta)$ is the joint density of the data under $\theta$, which is called the \textbf{likelihood function\index{lieklihood function}}.
The likelihood function is a function of the parameter $\theta$ such that it coincides with the true joint density when evaluated at $\theta = \theta_0$.
The MLE for $\theta_0$ is defined as the maximizer of the ``log'' of the likelihood, the so-called \textbf{log-likelihood function\index{log-likelihood function}}; that is,
\begin{align}\label{eq:mle-normal}
\begin{split}
	\hat{\theta}^{mle}_n 
		& \equiv \argmax_{\theta} \log L_n(\theta) \\
		& = \argmax_{(\mu, \sigma^2)} \sum_{i=1}^n \log f_X(X_i; \mu, \sigma^2)\\
		& = \argmax_{(\mu, \sigma^2)} \sum_{i=1}^n \left[ - \frac{1}{2}\log \sigma^2 - \frac{(X_i - \mu)^2}{2 \sigma^2}\right] = \argmin_{(\mu, \sigma^2)}  \left[ \frac{n}{2}\log \sigma^2 + \frac{\sum_{i=1}^n (X_i - \mu)^2}{2 \sigma^2}\right].
\end{split}
\end{align}
A theoretical reason for maximizing the log-likelihood function, rather than maximizing the likelihood function itself, will be described later.\footnote{
	An intuitive reason for this is that, without taking the log, the product of the likelihood (probability) tends to zero as $n$ increases.
	}

From the last line of \eqref{eq:mle-normal}, we can see that the MLE $\hat{\mu}^{mle}_n$ of $\mu_0$ can be obtained independently by solving the problem $\min_\mu \sum_{i=1}^n (X_i - \mu)^2$, whose solution is clearly $ \hat{\mu}^{mle}_n = \bar X_n$, where $\bar X_n = \frac{1}{n}\sum_{i=1}^n X_i$.
Hence, the sample average of $X$ is the MLE of $\mu_0$.
For the MLE $\hat{\sigma}^{2, mle}_n$ of $\sigma^2_0$, by the first-order condition it satisfies that
\begin{align*}
	0 = \frac{n}{\hat{\sigma}^{2, mle}_n} - \frac{\sum_{i=1}^n (X_i - \bar X_n)^2}{ \left[\hat{\sigma}^{2, mle}_n \right]^2 },
\end{align*}
which further implies that
\begin{align*}
 \frac{\sum_{i=1}^n (X_i - \bar X_n)^2}{ \hat{\sigma}^{2, mle}_n  } = n \;\; \Rightarrow \;\; \hat{\sigma}^{2, mle}_n = \frac{1}{n} \sum_{i=1}^n (X_i - \bar X_n)^2.
\end{align*}
Thus, the MLE of $\sigma_0^2$ is the sample variance of $X$.
It should be noted that the MLE variance estimator is not an unbiased variance estimator.\footnote{
	As is well known, an unbiased variance estimator is obtained by $\what{\Var}_n(X) = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar X_n)^2$.
	}

\end{example}

\begin{example}[Linear regression]\upshape
	We consider the following linear regression model with normally distributed error term:
	\begin{align*}
	& Y_i = X_i^\top \beta_0 + \eps_i, \;\; i = 1, \ldots, n \\
	& \eps_i \sim N(0, \sigma_0^2)
	\end{align*}
	Note that the second line of the above is equivalent to $Y_i - X_i^\top \beta_0 \sim N(0, \sigma_0^2)$.
	Thus, the conditional density of $Y_i$ given $X_i$ can be characterized by
	\[
	Y_i | X_i \sim N(X_i^\top \beta_0, \sigma_0^2).
	\]
	
	Denote the marginal density of $X_i$ as $f_X(\cdot)$, and let $\phi(\cdot ; \mu, \sigma^2)$ be the normal density function with mean $\mu$ and variance $\sigma^2$.
	Then, the joint density of $(Y_i, X_i)$ can be written as
	\[
	f_{Y,X}(Y_i, X_i) = \phi(Y_i ; X_i^\top \beta_0, \sigma_0^2) \cdot f_X(X_i).
	\]
	If the data are IID, the joint density of our data is
	\[
	\text{Joint density of }\{(Y_i, X_i): 1 \le i \le n\} = \prod_{i = 1}^n \phi(Y_i ; X_i^\top \beta_0, \sigma_0^2) \cdot f_X(X_i).
	\]
	Thus, letting $L_n(\theta) \equiv \prod_{i = 1}^n \phi(Y_i ; X_i^\top \beta, \sigma^2) \cdot f_X(X_i)$ be the likelihood function with $\theta = (\beta, \sigma^2)$,  the log-likelihood function is given by
	\begin{align*}
	\log L_n(\theta) = \sum_{i = 1}^n \log \phi(Y_i ; X_i ^\top \beta, \sigma^2) + \sum_{i=1}^n \log f_X(X_i).
	\end{align*}
	Since the second term on the right-hand side is irrelevant to the unknown parameters, the MLE for $\theta_0 = (\beta_0, \sigma_0^2)$ can be obtained by maximizing only the first term:
	\begin{align*}
	\hat \theta_n^{mle} 
	& \equiv \argmax_\theta \log L_n(\theta) \\
	& = \argmax_{(\beta, \sigma^2)} \left[ \sum_{i = 1}^n \log \phi(Y_i ; X_i ^\top \beta, \sigma^2)  \right] =  \argmin_{(\beta, \sigma^2)} \left[ \frac{n}{2}\log \sigma^2 + \frac{\sum_{i=1}^n (Y_i - X_i ^\top \beta)^2}{2 \sigma^2}\right],
	\end{align*}
	where the last equality follows from the same argument as in \eqref{eq:mle-normal}.
	Further, by the similar arguments in Example \ref{ex:normal}, it is easy to see that the MLE $\hat \beta_n^{mle}$ coincides with the OLS estimator and that the MLE $\hat \sigma_n^{2, mle}$ is obtained as the sample variance of the OLS residuals.
\end{example}

\begin{example}[Flipping an uneven coin]\label{ex:coin}\upshape
Suppose that we have a possibly uneven coin such that the probability of heads is unknown and may not be equal to 0.5. 
Let $X$ be a dummy variable defined by
\begin{align*}
&X = 1 \text{ for ``head",}\\
&X = 0 \text{ for ``tail".}
\end{align*}
We use $p_0$ to denote the coin's true probability of heads: $\E X = p_0$. 
Suppose that we have a set of coin-flipping data $\{X_1, \ldots , X_n \}$ of $n$ independent trials.
A natural estimator of $p_0$ is the sample average, i.e., the sample proportion of heads: $\bar{X}_n = \frac{1}{n}\sum_{i = 1}^n X_i$.
In fact, the sample average $\bar{X}_n$ is the least-squares estimator for $p_0$.
To see this, let
\[
	\hat p_n^{ls} \equiv \argmin_{p} \frac{1}{n}\sum_{i=1}^n (X_i - p)^2.
\]
The first-order condition of the minimization problem gives
\begin{align*}
	- \frac{2}{n} \sum_{i = 1}^n (X_i - \hat p_n^{ls}) = 0 \;\; 
	& \Rightarrow \;\; \frac{1}{n}\sum_{i = 1}^n X_i  -  \hat p_n^{ls} = 0 \\
	& \Rightarrow \;\; \hat p_n^{ls} = \bar{X}_n.
\end{align*}

As shown below, $\bar{X}_n$ is also an MLE for $p_0$.
The probability distribution of $X$ is
\[
	\Pr(X = 1) = p_0, \;\; \Pr(X = 0) = 1- p_0
\]
This type of probability distribution is called the \textbf{Bernoulli distribution\index{Bernoulli distribution}}.
By the independence, the joint probability of $\{X_1, \ldots , X_n\}$ is equal to
\[
	\text{Joint probability of $\{X_1, \ldots, X_n\}$} = \prod_{i = 1}^n  p_0^{X_i} (1 - p_0)^{1-X_i}.
\]
Then, the likelihood function is given by $L_n(p) \equiv \prod_{i = 1}^n  p^{X_i} (1 - p)^{1-X_i}$, and the MLE is defined as
\begin{align*}
	\hat{p}^{mle}_n 
	& \equiv \argmax_{p \in (0,1)} \log L_n(p) \\
	& = \argmax_{p \in (0,1)} \sum_{i =1}^n\left\{ X_i\log p + (1 - X_i)\log( 1-p)\right\}.
\end{align*}
The first-order condition for the above maximization implies that 
\begin{align*}
	0
	& = \sum_{i=1}^n\left\{ \frac{X_i}{\hat{p}^{mle}_n} - \frac{1 - X_i}{1 - \hat{p}^{mle}_n} \right\}\\
	%& = \sum_{i=1}^n\left\{ \frac{X_i\cdot (1 - \hat{p}^{mle}_n) - (1 - X_i)\cdot  \hat{p}^{mle}_n}{\hat{p}^{mle}_n \cdot (1 - \hat{p}^{mle}_n)} \right\}  \\	
	& = \sum_{i=1}^n\left\{ \frac{X_i - \hat{p}^{mle}_n}{\hat{p}^{mle}_n \cdot (1 - \hat{p}^{mle}_n)} \right\}.
\end{align*}
By definition, $\hat{p}^{mle}_n$ is not equal to zero or one.
Therefore, $\hat{p}_n^{mle}$ must satisfy
\begin{align*}
\sum_{i=1}^n (X_i - \hat{p}_n^{mle}) = 0
& \Rightarrow \hat{p}_n^{mle} = \bar{X}_n.
\end{align*}
\end{example}
\bigskip

We generalize the above discussion.
Suppose we have a set of IID observations $\{X_1, \ldots, X_n\}$ of sample size $n$.
Here, $X$ may be a scalar or a vector.
For continuous (resp. discrete) $X$, let $p(x ; \theta_0)$ be the population density (resp. probability) function of $X$, where $\theta_0$ is the vector of unknown parameters to be estimated.
The functional form of $p(x ; \theta_0)$ is known up to $\theta_0$; that is, $\theta_0$ is the only unknown component.\footnote{
	If the model specification of $p$ is incorrect, then the resulting MLE is not consistent for $\theta_0$.
	Note that, for example in the case of linear regression models, we may use a least-squares method to consistently estimate the parameters even when $p$ is partly unknown.
	In this sense, in general, maximum likelihood methods require more restrictive assumptions than the other types of estimators.
	}
Then, the \textbf{likelihood function\index{likelihood function}} is defined as
\[
	L_n(\theta) \equiv \prod_{i = 1}^n p(X_i; \theta),
\]
and the \textbf{log-likelihood function\index{log-likelihood function}} is
\[
	\ell_n(\theta) \equiv \log L_n(\theta) = \sum_{i = 1}^n \log p(X_i; \theta).
\]
The \textbf{maximum likelihood estimator\index{maximum likelihood estimator}} (MLE) of $\theta_0$ is defined as the maximizer of the log-likelihood function $\ell_n(\theta)$: 
\[
	\hat \theta_n^{mle} \equiv \argmax_{\theta}\ell_n(\theta),
\]
or numerically equivalently,
\begin{align*}
	\hat \theta_n^{mle} 
	& \equiv \argmax_{\theta} \frac{1}{n} \ell_n(\theta) \\ 
	& = \argmax_{\theta} \frac{1}{n} \sum_{i = 1}^n \log p(X_i; \theta).
\end{align*}
Dividing the objective function by $n$ is for technical convenience in deriving the asymptotic properties of MLE.\footnote{
	As $n$ increases to infinity, $\ell_n(\theta)$ may also diverge to infinity since it is a summation of $n$ non-zero terms.
	However, by dividing it by $n$, under standard conditions, we can have $\frac{1}{n} \ell_n(\theta) \overset{p}{\to} \E[\log p(X_i; \theta)]$ for all $\theta$ by the ``uniform" law of large numbers.\label{foot:UWLLN}
	}

\hrulefill
\begin{exercise}\upshape
	Suppose that we have $n$ IID observations $\{X_1, \ldots, X_n\}$ drawn from a Poisson distribution with parameter $\lambda_0$; i.e.,
	\[
		\Pr(X = k) = \frac{\lambda_0^k \exp(-\lambda_0)}{k !}.
	\]
	Find the MLE of $\lambda_0$.
\end{exercise}
\begin{exercise}\label{ex:maxuni}\upshape
	Suppose that we have $n$ IID observations $\{X_1, \ldots, X_n\}$ drawn from $\text{Uniform}[0, \alpha_0]$.
	Show that the MLE of $\alpha_0$ is given by 
	\[
	\hat \alpha_n^{mle} = \max\{X_1, \ldots, X_n\}. 
	\] 
\end{exercise}

\subsection{Kullback-Leibler divergence}

Let $p(x; \theta_1)$ and $p(x; \theta_2)$ be two density (or probability) functions such that $p(x; \theta_1) = p(x; \theta_2)$ for all $x$ if and only if $\theta_1 = \theta_2$.
The following quantity is called the \textbf{Kullback-Leibler divergence\index{Kullback-Leibler divergence}}  (KL divergence) between $p(x; \theta_1)$ and $p(x; \theta_2)$:\footnote{
	Note that the KL divergence is not symmetric: $K\left( p(X; \theta_1) \| p(X; \theta_2) \right) \neq K\left( p(X; \theta_2) \| p(X; \theta_1) \right)$, in general.
	In addition, it does not satisfy the triangle inequality.
	Thus, KL divergence is not a distance function. 
	}
\renewcommand{\arraystretch}{1.5}
\begin{align*}
	K\left( p(X; \theta_1) \| p(X; \theta_2) \right)
	& \equiv \E_{\theta_1}\left[\log \frac{p(X; \theta_1)}{p(X; \theta_2)} \right]\\
	& = \left\{ \begin{array}{ll}
		\int \left[\log \frac{p(x; \theta_1)}{p(x; \theta_2)} \right] p(x; \theta_1) \mrm{d}x & \text{ if $X$ is continuous } \\
		\sum_{x \in \{x_1, \ldots , x_k \}} \left[\log \frac{p(x; \theta_1)}{p(x; \theta_2)} \right] p(x; \theta_1) & \text{ if $X$ is discrete with its support $\{x_1, \ldots, x_k\}$}
	\end{array} \right.
\end{align*}
\renewcommand{\arraystretch}{1}
where $\E_{\theta}[\; \cdot \;]$ denotes the expectation under $p(x; \theta)$ (note that $\E[\; \cdot \;] \equiv \E_{\theta_0}[\; \cdot \; ]$, where $\theta_0$ is the true value of $\theta$).
It is straightforward to see that 
\[
	K\left( p(X; \theta_1) \| p(X; \theta_2) \right) = 0
\]
if $\theta_1 = \theta_2$ since $\log 1 = 0$.
Moreover, it holds that for any $\theta_1 \neq \theta_2$, 
\[
	K\left( p(X; \theta_1) \| p(X; \theta_2) \right) > 0.
\]
Formally, we have the following result.
\begin{theorem}\label{thm:KL}
	For any density (or probability) functions $p(x; \theta_1)$ and $p(x; \theta_2)$ such that $p(x; \theta_1) = p(x; \theta_2)$ for all $x$ if and only if $\theta_1 = \theta_2$, it holds that $K(p(x; \theta_1) \| p(x; \theta_2)) \geq 0$.
	The equality holds if and only if $\theta_1 = \theta_2$.
\end{theorem}

\begin{proof}
	We only prove the case when $p(x; \theta)$ is a density function.
	First, note that
	\begin{align*}
		K(p(x; \theta_1) \| p(x; \theta_2))  
		& = \E_{\theta_1}\left[\log \frac{p(X; \theta_1)}{p(X; \theta_2)} \right] \\
		& = \E_{\theta_1}\left[- \log \frac{p(X; \theta_2)}{p(X; \theta_1)} \right].
	\end{align*}
	Then, since $-\log(\cdot)$ is a convex function, by Jensen's inequality \ref{lem:Jensen},
	\begin{align*}
	K(p(x; \theta_1) \| p(x; \theta_2))  
	& \geq -\log \E_{\theta_1}\left[ \frac{p(X; \theta_2)}{p(X; \theta_1)} \right]\\
	& = - \log \int \frac{p(x; \theta_2)}{p(x; \theta_1)}  p(x; \theta_1) \mrm{d}x\\
	& = - \log \int p(x; \theta_2) \mrm{d}x \\
	& = - \log 1 = 0.
	\end{align*}
	This proves the first argument.
	For the second argument, The ``if"-part is trivial.
	To prove the ``only if"-part, note that, if a function $\varphi(\cdot)$ is strictly convex, $\E[\varphi(X)] = \varphi(\E[X])$ occurs only when $X = \E[X]$ (i.e., when $X$ is a constant).
	 Since $- \log(\cdot)$ is a strictly convex function, $K(p(x; \theta_1) \| p(x; \theta_2)) = 0$ holds only when
	 \begin{align*}
	 	\frac{p(X; \theta_2)}{p(X; \theta_1)} 
	 	& =  \E_{\theta_1}\left[ \frac{p(X; \theta_2)}{p(X; \theta_1)} \right]\\
	 	& = \int \frac{p(x; \theta_2)}{p(x; \theta_1)}  p(x; \theta_1) \mrm{d}x\\
	 	& = \int p(x; \theta_2) \mrm{d}x = 1.
	 \end{align*}
	 This implies the desired result.
\end{proof}

\subsection{MLE minimizes the KL divergence}\label{subsec:MLEconsistency}

From Theorem \ref{thm:KL}, for the true parameter $\theta_0$, we can obtain the following trivial equality:
\begin{align*}
	\theta_0 = \argmin_\theta K( p(X; \theta_0) \| p(X; \theta) ).
\end{align*}
 Note that
\begin{align*}
	K( p(X; \theta_0) \| p(X; \theta) )
	&= \E\left[\log \frac{p(X; \theta_0)}{p(X; \theta)} \right]\\
	&= \E[\log p(X; \theta_0)] - \E[\log p(X; \theta) ].
\end{align*}
Since the first term on the right-hand side is irrelevant to $\theta$, it further holds that
\begin{align}\label{eq:pop-mle}
	\theta_0 = \argmax_\theta \E[\log p(X; \theta) ],
\end{align}
implying that the true parameter $\theta_0$ is the maximizer of the population log-likelihood function.
Thus, maximizing the population log-likelihood is equivalent to minimize the KL divergence.

Here, by the law of large numbers, we can expect that
\[
	\frac{1}{n}\sum_{i=1}^n \log p(X_i; \theta) \overset{p}{\to}  \E[\log p(X; \theta) ],
\]
for all $\theta$ (see also footnote \ref{foot:UWLLN}).
Since $\hat{\theta}_n^{mle} = \argmax_{\theta}\frac{1}{n}\sum_{i=1}^n \log p(X_i; \theta)$, in view of \eqref{eq:pop-mle}, it holds that  $\hat{\theta}_n^{mle} \overset{p}{\to} \theta_0$.
For more precise discussion, see, for example, \cite{newey1994large}.
\begin{center}
	\includegraphics[width=10cm]{mle.png}
\end{center}

\section{Cramer-Rao lower bound}\label{sec:CRLB}
Now, we introduce two important concepts.
The \textbf{score function\index{score function}} (or simply score) is the partial derivative of the log-likelihood $\log p(x; \theta)$ with respect to $\theta$, which we denote by $s(x; \theta)$.
The variance covariance matrix of the score under $p(x; \theta)$ is called the \textbf{Fisher information matrix\index{Fisher infromation matrix}}, which we denote by $I(\theta)$.
Observe that the expectation of the score function (under $p(x; \theta)$) is always zero:
\begin{align}\label{eq:scorezero}
\begin{split}
	\E_\theta[s(X; \theta)]
	& = \int \left\{\frac{\partial}{\partial \theta}\log p(x; \theta)\right\} p(x; \theta) \mrm{d}x \\
	& =  \int \frac{\frac{\partial}{\partial \theta} p(x; \theta)}{p(x; \theta)}p(x; \theta) \mrm{d}x \\
	& = \int \frac{\partial}{\partial \theta} p(x; \theta)  \mrm{d}x \\
	& = \frac{\partial}{\partial \theta} \underbrace{\int p(x; \theta) \mrm{d}x}_{=1} = \mathbf{0},
\end{split}
\end{align}
and, therefore
\begin{align*}
	I(\theta) 
	& \equiv \Cov_{\theta}\left[ s(X; \theta) \right] \\
	& = \E_\theta \left[ ( s(X; \theta) - \E_\theta s(X; \theta) ) ( s(X; \theta) - \E_\theta s(X; \theta) )^\top \right] \\
	& = \E_\theta\left[ s(X; \theta) s(X; \theta)^\top \right].
\end{align*}
\bigskip

When estimating $\theta_0$, we can consider many alternative estimators $\hat \theta_n$'s, including OLS, MLE, etc.
An important criterion for the choice of estimator is the \textbf{efficiency\index{efficiency}}; the more efficient estimator is the one that has the smaller variance.
Surprisingly or not, under certain regularity conditions --	see the appendix of this chapter, the inverse of the Fisher information matrix $[I(\theta_0)]^{-1}$ gives the ``lower bound'', known as \textbf{Cramer-Rao lower bound\index{Cramer-Rao lower bound}}, of the covariance matrix for all possible estimators of $\theta_0$; that is, for any ``regular" estimator $\hat \theta_n$ of $\theta_0$,
\begin{align}\label{eq:CRLB}
	\Cov\left(\sqrt{n}(\hat \theta_n - \theta_0)\right) \ge [I(\theta_0)]^{-1}.
\end{align}
Here, for any square matrices $A$ and $B$ of the same size, $A \ge B$ means that $A - B$ is positive-semidefinite.
As described below, the MLE $\hat \theta_n^{mle}$ generally attains this lower bound; that is, \eqref{eq:CRLB} holds with equality.
In other words, whenever the MLE is available, it is the most efficient estimator we can use in the class of regular estimators.
An example of ``irregular'' estimator will be provided in the appendix of this chapter.
\bigskip

We can relatively easily check \eqref{eq:CRLB} if we restrict our attention to the class of unbiased estimators.
Suppose that the data $\{X_1, \ldots , X_n\}$ are IID with density function $p(x; \theta_0)$, where $\theta_0$ is a scalar true parameter for simplicity, and that we have an \textbf{unbiased estimator\index{unbiased estimator}} $\hat \theta_n$ such that
\begin{align}\label{eq:unbiased}
	\E_\theta[\hat \theta_n] - \theta = 0
\end{align}
for any $\theta$.
Note that the above expectation is taken with respect to the data $\{X_1, \ldots , X_n\}$ since any estimator is a function of the data sample; thus, we may write $\hat \theta_n = \theta_n(X_1, \ldots , X_n)$.
\bigskip

\textbf{Proof of \eqref{eq:CRLB} for unbiased estimators.} \quad Since \eqref{eq:unbiased} holds for all $\theta$, differentiating both sides of \eqref{eq:unbiased} with respect to $\theta$, we obtain
\begin{align*}
	0 = \frac{\partial }{\partial \theta}\E_\theta[\hat \theta_n - \theta]
	& = \frac{\partial}{\partial \theta} \int [\theta_n(x_1, \ldots , x_n) - \theta] \prod_{i=1}^n p(x_i; \theta) \mrm{d}x_1 \cdots  \mrm{d}x_n\\
	& = \int [\theta_n(x_1, \ldots , x_n) - \theta] \frac{\partial}{\partial \theta} \prod_{i=1}^n p(x_i; \theta) \mrm{d}x_1 \cdots  \mrm{d}x_n - 1\\
	& = \int [\theta_n(x_1, \ldots , x_n) - \theta]  \left\{ \frac{\partial}{\partial \theta} \log\left( \prod_{i=1}^n p(x_i; \theta) \right) \right\} \prod_{i=1}^n p(x_i; \theta) \mrm{d}x_1 \cdots  \mrm{d}x_n - 1 \\
	& = \int [\theta_n(x_1, \ldots , x_n) - \theta]  \left\{\frac{\partial}{\partial \theta}\sum_{i=1}^n  \log p(x_i; \theta) \right\}\prod_{i=1}^n p(x_i; \theta) \mrm{d}x_1 \cdots  \mrm{d}x_n - 1 \\
	& = \int [\theta_n(x_1, \ldots , x_n) - \theta]  \sum_{i=1}^n s(x_i; \theta) \prod_{i=1}^n p(x_i; \theta) \mrm{d}x_1 \cdots  \mrm{d}x_n - 1\\
	& = \E_\theta\left[ (\hat \theta_n- \theta)  \sum_{i=1}^n s(X_i; \theta)  \right] - 1.
\end{align*}
Recall that $\E_\theta[\hat \theta_n- \theta] = 0$ and $\E_\theta\left[ s(X_i; \theta)  \right] = 0$ by \eqref{eq:scorezero}.
Then, by Cauchy-Schwarz inequality \ref{lem:CS} and the IID assumption, we have
\begin{align*}
	1 
	& \le \left| \E_\theta\left[ \sqrt{n}(\hat \theta_n- \theta) \cdot \frac{1}{\sqrt{n}}  \sum_{i=1}^n s(X_i; \theta)  \right] \right|\\
	& \le \Var_\theta\left( \sqrt{n}(\hat \theta_n- \theta) \right) \Var_\theta\left(\frac{1}{\sqrt{n}}\sum_{i=1}^n s(X_i; \theta) \right)\\
	& = \Var_\theta\left(\sqrt{n}(\hat \theta_n- \theta) \right) I(\theta),
\end{align*}
and, thus,
\[
	\Var_\theta\left(\sqrt{n}(\hat \theta_n- \theta) \right) \ge \frac{1}{I(\theta)}.
\]
$\blacksquare$

\section{Asymptotic properties}
Suppose that we have a consistent MLE $\hat \theta_n^{mle}$ of $\theta_0$.
By the first-order condition, the MLE $\hat \theta_n^{mle}$ satisfies that
\[
	\frac{1}{n}\sum_{i = 1}^n s(X_i; \hat \theta_n^{mle}) = \mbf{0}.
\]
Applying the mean-value expansion to $s(X_i; \hat \theta_n^{mle}) $ around $\theta_0$, we have
\[
	\mbf{0} = \frac{1}{n}\sum_{i = 1}^n s(X_i; \theta_0) + \frac{1}{n}\sum_{i=1}^n H(X_i; \bar \theta_n) (\hat \theta_n^{mle} - \theta_0),
\]
where $H(X_i; \theta) \equiv \partial s(X_i; \theta)/(\partial \theta^\top)$, or equivalently, $H(X_i; \theta) \equiv \partial^2 \log p(X_i; \theta)/(\partial \theta \partial \theta^\top)$, i.e., the Hessian matrix of $\log p(X_i; \theta)$, and $\bar \theta_n \in [\hat \theta_n^{mle}, \theta_0]$.
Then, if $\frac{1}{n}\sum_{i=1}^n H(X_i; \bar \theta_n) $ is nonsingular, we obtain
\begin{align}\label{eq:taylor}
	\sqrt{n}(\hat \theta_n^{mle} - \theta_0) = -\left[\frac{1}{n}\sum_{i=1}^n H(X_i; \bar \theta_n) \right]^{-1}\frac{1}{\sqrt{n}}\sum_{i = 1}^n s(X_i; \theta_0).
\end{align}
As shown in Section \ref{sec:CRLB}, we have $\E[s(X; \theta_0)] = 0$ and $\Cov[s(X; \theta_0)] = I(\theta_0)$.
Since $X_i$'s are IID, so are $s(X_i; \theta_0)$'s.
Then, we can apply CLT to $\frac{1}{\sqrt{n}}\sum_{i = 1}^n s(X_i; \theta_0)$ to obtain
\[
	\frac{1}{\sqrt{n}}\sum_{i = 1}^n s(X_i; \theta_0) \overset{d}{\to} \mbf{N}(\mbf{0},  I(\theta_0)).
\]
Further, note that $\hat \theta_n^{mle}$ is a consistent estimator of $\theta_0$ and so is $\bar{\theta}_n$ clearly by its definition.
Then, we have $\frac{1}{n}\sum_{i=1}^n H(X_i; \bar \theta_n) \overset{p}{\to} \E[H(X; \theta_0)]$ by WLLN, assuming that $H(x; \theta)$ is continuous at $\theta_0$.
Finally, combining these results and applying Slutsky's theorem \ref{lem:slutsky} give the following result:
\[
	\sqrt{n}(\hat \theta_n^{mle} - \theta_0) \overset{d}{\to} \mbf{N}\left( \mbf{0}, \E[H(X; \theta_0)]^{-1} I(\theta_0) \E[H(X; \theta_0)]^{-1} \right).
\]
From this result, one might think that the MLE is not necessarily the most efficient estimation method.
However, that is not the case; that is, the MLE is always the most efficient because of the following fact:
\begin{align}\label{eq:infoeq}
	I(\theta_0) = - \E[H(X; \theta_0)].
\end{align}
This equality is known as the \textbf{information matrix equality\index{information matrix equality}}.
\bigskip

\textbf{Proof of \eqref{eq:infoeq}.} \quad Since \eqref{eq:scorezero} holds for any $\theta$, differentiating both sides of \eqref{eq:scorezero} with respect to $\theta$, we have
\begin{align*}
	\mbf{0} = \frac{\partial }{\partial \theta^\top} \E_\theta[s(X; \theta)] 
	& = \frac{\partial}{\partial \theta^\top}\int s(x; \theta) p(x; \theta) \mrm{d}x \\
	& = \int H(x; \theta) p(x; \theta) \mrm{d}x + \int s(x; \theta) \frac{\frac{\partial}{\partial \theta^\top} p(x; \theta)}{p(x; \theta)} p(x; \theta) \mrm{d}x \\
	& = \int H(x; \theta) p(x; \theta) \mrm{d}x + \int s(x; \theta) s(x; \theta)^\top p(x; \theta) \mrm{d}x.
\end{align*} 
The last line implies that $-\E_\theta[H(X; \theta)] = \E_\theta[s(X; \theta) s(X; \theta)^\top]$.
Finally, the result follows from the definition of $I(\theta)$. $\; \blacksquare$

Therefore, thanks to the information matrix equality, the asymptotic distribution of the MLE can be simply written in two ways as
\begin{align*}
	& \sqrt{n}(\hat \theta_n^{mle} - \theta_0) \overset{d}{\to} \mbf{N}\left( \mbf{0}, -\E[H(X; \theta_0)]^{-1}  \right) \\
	& \sqrt{n}(\hat \theta_n^{mle} - \theta_0) \overset{d}{\to} \mbf{N}\left( \mbf{0}, [I(\theta_0)]^{-1}  \right).
\end{align*}

\section{An application: Binary response models}\label{sec:binary}
As an important application of the maximum likelihood method, we consider estimating binary response models.
Let $D$ be a binary outcome variable defined by
\[
	D = \mbf{1}\{X^\top\beta_0 \ge \eps\},
\]
where $X$ is a vector of explanatory variables, $\eps$ is an unobserved error term, and $\beta_0$ is the vector of parameters to be estimated.

First of all, note that for any constant $c > 0$, it holds that  $D = \mbf{1}\{X^\top(c\beta_0) \ge \eta\}$, where $\eta$ is a new error term defined as $\eta = c\eps$.
Thus, when the distribution of $\eps$ is fully unrestricted, $\beta_0$ cannot be ``identified" without some scale normalization (more formal discussion will be provided in Chapter \ref{chap:identification}). 
To overcome this issue, we typically impose a scale restriction on $\eps$ by assuming that $\eps$ is distributed as either the standard normal or standard logistic with location 0 and scale 1.
The resulting model based on the former assumption is called the \textbf{probit model\index{probit model}} and the one based on the latter is called the \textbf{logit model\index{logit model}}.

Then, under appropriate scale normalization, we can estimate $\beta_0$ by the maximum likelihood method in the following manner.
Observe that the conditional probability of $D = 1$ given $X$ is equal to
\[
	\Pr(D = 1 | X) = \Pr( \eps \le X^\top\beta_0 | X) = F(X^\top\beta_0),
\]
where $F(\cdot)$ is the distribution function of $\eps$, which is a known function.
Suppose that we have the data of $n$ IID observations $\{(D_i, X_i): 1 \le i \le n\}$.
Then, similarly as in Example \ref{ex:coin}, the (conditional) likelihood function for the data is given by
\[
	L_n(\beta) \equiv \prod_{i=1}^n F(X_i^\top\beta)^{D_i} (1 -  F(X_i^\top\beta))^{1 - D_i},
\] 
and thus the MLE of $\beta_0$ is defined as
\begin{align*}
	\hat \beta_n^{mle} 
	& \equiv \argmax_\beta \log L_n(\beta) \\
	& = \argmax_\beta \sum_{i =1}^n\left\{ D_i \cdot \log F(X_i^\top \beta) + (1 - D_i) \cdot \log(1 -  F(X_i^\top\beta)) \right\}.
\end{align*}
Next, we derive the Fisher information matrix.
To this end, we need to calculate the score function, which is the derivative of  $D \log F(X^\top \beta) + (1 - D) \log(1 -  F(X^\top\beta)$ with respect to $\beta$:
\begin{align*}
	s(X; \beta) 
	& = X \frac{D \cdot f(X^\top \beta)}{F(X^\top \beta)} - X \frac{(1 - D) \cdot f(X^\top \beta)}{1 - F(X^\top \beta)}\\
	& = X \frac{(D - F(X^\top \beta))f(X^\top \beta)}{F(X^\top \beta) \cdot (1 -  F(X^\top\beta)) },
\end{align*}
where $f(\cdot)$ is the density function of $\eps$.
Thus, noting that 
\[
	\E[(D - F(X^\top\beta_0))^2|X] = F(X^\top \beta_0) (1 - F(X^\top \beta_0)),
\]
the information matrix $I(\beta_0)$ is obtained by
\begin{align*}
	I(\beta_0)
	& = \E\left[ s(X; \beta_0) s(X; \beta_0)^\top \right] \\ 
	& = \E\left[ XX^\top  \frac{(D - F(X^\top \beta_0))^2 [f(X^\top \beta_0)]^2}{[F(X^\top \beta_0) \cdot (1 -  F(X^\top\beta_0))]^2 }\right] \\
	& = \E\left[ XX^\top  \frac{[f(X^\top \beta_0)]^2}{F(X^\top \beta_0) \cdot (1 -  F(X^\top\beta_0)) }\right] 
\end{align*}
by LIE.
Hence, we have
\[
	\sqrt{n}(\hat \beta_n^{mle} - \beta_0) \overset{d}{\to} \mbf{N}\left( \mbf{0}, \E\left[ XX^\top  \frac{[f(X^\top \beta_0)]^2}{F(X^\top \beta_0) \cdot (1 -  F(X^\top\beta_0)) }\right]^{-1}  \right).
\]

It is also straightforward to confirm the information matrix equality.
By the chain rule, the Hessian $H(X; \beta_0)$ can be written as
\begin{align*}
	H(X; \beta_0) 
	& = \partial s(X; \beta_0)/ \partial \beta^\top \\
	& = - XX^\top \frac{[f(X^\top \beta_0)]^2}{F(X^\top \beta_0) \cdot (1 -  F(X^\top\beta_0)) } + X (D - F(X^\top \beta_0)) \cdot \frac{\partial}{\partial \beta^\top} \left[ \frac{f(X^\top \beta_0)}{F(X^\top \beta_0) \cdot (1 -  F(X^\top\beta_0)) }\right].
\end{align*}
By LIE, we can see that the second term on the right-hand side will be zero after taking the expectation.
Thus, we have $-\E[H(X; \beta_0) ] = I(\beta_0)$, as desired.

\hrulefill
\begin{exercise}\upshape
	Consider an ordered choice model with three ordered outcomes:
	\begin{align*}
	Y^* 
	= X^\top\beta_0 + \eps, \qquad D = 1 \iff  \qquad\:  Y^* & \le c_1 \\
	D = 2  \iff c_1 < Y^* & \le c_2 \\
	D = 3  \iff c_2 < Y^* &,
	\end{align*}
	where $Y^*$ is an unobservable latent variable, and $\theta_0 = (\beta_0, c_1, c_2)$ is the set of unknown parameters to be estimated.
	Suppose that we have an IID sample $\{(D_i, X_i): 1 \le i \le n\}$.
	The error term $\eps$ is assumed to be independent of $X$ and has a known distribution function $F$.
	Derive the log-likelihood function for this model and define the MLE $\hat \theta_n^{mle}$ of $\theta_0$.
\end{exercise}

\section{Likelihood ratio test}\label{sec:LRT}
Partition the estimation parameters as $\theta_0 = (\theta_{01}, \theta_{02})$, where $\theta_{01}$ is a $q_1 \times 1$ vector, and $\theta_{02}$ is a $q_2 \times 1$ vector.
Suppose that we would like to test the following null hypothesis:
\[
	\mbb{H}_0: \theta_{02} = \theta_{02}^*.
\]
The alternative hypothesis is that at least one element of $\theta_{02}$ is not equal to the corresponding element of $\theta_{02}^*$, which is written as
\[
	\mbb{H}_1: \theta_{02} \neq \theta_{02}^*.
\]
By the maximum likelihood principle, it holds that 
\[
	\ell_n(\hat \theta_{n1}^{mle}, \hat \theta_{n2}^{mle}) \ge \ell_n(\tilde \theta_{n1}^{mle}, \theta_{02}^*),
\]
where $(\hat \theta_{n1}^{mle}, \hat \theta_{n2}^{mle})$ is the MLE of $(\theta_{01}, \theta_{02})$ under $\mbb{H}_1$, and $\tilde \theta_{n1}^{mle}$ is the ``restricted'' MLE of $\theta_{01}$ under $\mbb{H}_0$; namely, $\tilde \theta_{n1}^{mle} = \argmax_{\theta_1} \ell_n(\theta_1, \theta_{02}^*)$.
If $\mbb{H}_1$ is true, then we can expect that the former maximized log-likelihood will be significantly larger than the latter.
Thus, we can use the the difference of these log-likelihoods as a valid test statistic for $\mbb{H}_0$.
Let
\begin{align*}
	T_n 
	& \equiv 2\left(\ell_n(\hat \theta_{n1}, \hat \theta_{n2}) - \ell_n(\tilde \theta_{n1}, \theta_{02}^*) \right)\\
	& = 2 \log \frac{L_n( \hat \theta_{n1}, \hat \theta_{n2})}{L_n( \tilde \theta_{n1}, \theta_{02}^*)},
\end{align*}
where the superscript ``\textit{mle}'' is omitted for simplicity.
This test statistic is called the \textbf{likelihood ratio statistic\index{likelihood ratio statistic}}. 
Then, under certain regularity conditions, we can show that 
\begin{align}\label{eq:LRT}
T_n \overset{d}{\to} \chi^2(q_2)
\end{align}
under $\mbb{H}_0$.
Thus, if the computed $T_n$ is sufficiently large compared with the critical value of $\chi^2(q_2)$, then we can reject $\mbb{H}_0$.
This testing procedure is called the \textbf{likelihood ratio test\index{likelihood ratio test}}.
\bigskip

For a better understanding, it would be useful to provide a sketch of the proof of \eqref{eq:LRT} for a simpler case, where $\theta_0$ is a scaler and the null hypothesis is $\mbb{H}_0: \theta_0 = \theta_0^*$.
In this case, the test statistic is simplified as follows: $T_n \equiv 2\left(\ell_n(\hat \theta_n) - \ell_n(\theta_0^*) \right)$.
By the second order Taylor expansion, we have
\begin{align*}
	\ell_n(\theta_0^*) - \ell_n(\hat \theta_n)
	& =  (\theta_0^* - \hat \theta_n) \cdot \ell_n'(\hat \theta_n) + \frac{1}{2}(\theta_0^* - \hat \theta_n)^2 \cdot \ell_n''(\bar \theta_n)\\
	& = \frac{1}{2}(\theta_0^* - \hat \theta_n)^2 \cdot \ell_n''(\bar \theta_n)
\end{align*}
where $\ell_n'(\hat \theta_n)$ and $\ell_n''(\hat \theta_n)$ are the first and second derivatives of $\ell_n$ evaluated at $\hat \theta_n$, respectively, and $\bar \theta_n \in [\theta_0^*, \hat \theta_n]$.
The second equality holds from the first order condition of the maximum likelihood estimation.
Hence, we can write
\begin{align*}
	T_n 
	& = -(\theta_0^* - \hat \theta_n)^2 \cdot \ell_n''(\bar \theta_n)\\
	& =  (\sqrt{n}(\hat \theta_n - \theta_0^*))^2 \cdot \left( - \frac{1}{n}\ell_n''(\bar \theta_n) \right).
\end{align*}
Observe that under $\mbb{H}_0: \theta_0 = \theta_0^*$, we can have $\sqrt{n}(\hat \theta_n - \theta_0^*) = \sqrt{n}(\hat \theta_n - \theta_0)$, and
\[
	-\frac{1}{n} \ell_n''(\bar \theta_n) = -\frac{1}{n} \sum_{i=1}^n \frac{\partial^2 \log p(X_i, \bar \theta_n)}{\partial \theta \partial \theta} \overset{p}{\to} -\E[H(X; \theta_0^*)] = I(\theta_0).
\]
Since $\sqrt{n}(\hat \theta_n - \theta_0) \overset{d}{\to} N(0, [I(\theta_0)]^{-1})$, 
\begin{align*}
	T_n \approx \Bigg[ \: \underbrace{\frac{\sqrt{n}(\hat \theta_n - \theta_0)}{\sqrt{I(\theta_0)}}}_{\overset{d}{\to} \: N(0,1)} \: \Bigg]^2 \overset{d}{\to} \chi^2(1).
\end{align*}


\paragraph{A numerical simulation with R}
The following code gives the simulated distributions of $T_n$ for the coin-flipping data in Example \ref{ex:coin} with $n = 500$ and $\mbb{H}_0: p = p^*$ for $p^* \in \{0.5, 0.55, 0.6\}$.
Here, the true value of $p$ is set to 0.5.

We first define the log-likelihood function:
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
 LL <- function(p){ 
 
	LogP <- X*log(p) + (1 - X)*log(1 - p)
	ell  <- sum(LogP)
	return(ell)

	}
\end{lstlisting}
Next, we calculate the test statistic $T_n$ for each $\mbb{H}_0$ for 5000 replicated datasets:
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
 nrep <- 5000
 T0   <- numeric(nrep)
 T1   <- numeric(nrep)
 T2   <- numeric(nrep)

 for(i in 1:nrep){

	X   <- rbinom(500, 1, 0.5)  # flipping an even coin 500 times
	mle <- mean(X)              # MLE = sample average

	T0[i] <- 2*(LL(mle) - LL(0.5 ))
	T1[i] <- 2*(LL(mle) - LL(0.55))
	T2[i] <- 2*(LL(mle) - LL(0.6 ))

	}
\end{lstlisting}
Finally, plot the densities of the calculated $T_n$'s:
\begin{lstlisting}[basicstyle=\ttfamily\scriptsize, frame=single]
 xlm <- c(0,20)
 ylm <- c(0,1)
 xlb <- "T_n"
 ylb <- "Density"
 
 plot(density(T0), xlim = xlm, ylim = ylm, col = 2, main = "", xlab = xlb, ylab = ylb)
 par(new = T)
 plot(density(T1), xlim = xlm, ylim = ylm, col = 3, main = "", xlab = xlb, ylab = ylb)
 par(new = T)
 plot(density(T2), xlim = xlm, ylim = ylm, col = 4, main = "", xlab = xlb, ylab = ylb)
 par(new = T)
 curve(dchisq(x,1), xlim = xlm, ylim = ylm, lwd = 2, lty = 2, main = "", xlab = xlb, ylab = ylb)
 legend(
	"topright",
	c("p = 0.5", "p = 0.55", "p = 0.6", "ChiSq(1)"), 
	lwd = c(1,1,1,2),
	lty = c(1,1,1,2),
	col = c(2,3,4,1)
	) 
\end{lstlisting}
\begin{center}
	\includegraphics[width=10cm]{LRTsim.png}
\end{center}

\section{Akaike's Information Criterion}

When the number of regressors is large and only some of them are meaningful, including all of them in the model may cause an \textbf{overfitting\index{overfitting}} problem.
That is, using too many explanatory variables results in a model that adapts to the data too well, and shows poor performance in out-of-sample predictions.
In an extreme case, when the number of regressors is equal to the sample size $n$, it is possible to achieve a ``perfect'' fit to the data with zero residuals, but such a model has no predictive power.

In order to avoid the overfitting problem, we need to select the regressors that should be included in the model using some criterion.
Note that the log-likelihood is not a valid variable selection criterion since it monotonically increases in the number of included regressors; model selection based on the log-likelihood value always results in the largest model.
A ``good'' model should be defined as a model whose likelihood function well approximates the ``true'' data distribution, rather than the empirical distribution of the observed data.
\bigskip

Here, let $f_X(x)$ be the true density function of $X$.
The parameterized density $p(x; \theta_0)$ may or may not be equal to $f_X(x)$ (if they are not same, $\theta_0$ is interpreted as a ``pseudo'' true parameter).
Let $\hat \theta_n = \theta_n(X_1, \ldots, X_n)$ be the MLE from our model and $p(x; \hat \theta_n)$ be the estimated likelihood function.
Then, taking the sampling error involved in the estimate of $\hat \theta_n$ into account, we may conclude that our modelling is optimal if the following statistic is sufficiently small:
\begin{align*}
		\E_{data} K( f_X(X) \| p(X; \hat \theta_n) )
		& = \E_{data} \E_X\left[ \log \frac{f_X(X)}{p(X; \hat \theta_n)} \right] \quad \left(=\int\int \log \frac{f(x)}{p(x; \theta_n(x_1, \ldots, x_n))} f_X(x)\mrm{d}x \prod_{i=1}^n f(x_i) \mrm{d}x_i \right)\\
		& = \E_X[\log f_X(X)] - \E_{data} \E_X[\log p(X; \hat \theta_n)],
\end{align*}
which is the expected KL divergence between the true data distribution and the estimated distribution.
Since the first term is independent of the model selection, minimizing the KL divergence over the whole model space is equal to maximizing $\E_{data} \E_X[\log p(X; \hat \theta_n)]$.

By using the second order Taylor expansion around $\theta_0$, we have
\[
	\log p(X; \hat \theta_n) \approx \log p(X; \theta_0) + s(X; \theta_0)^\top (\hat \theta_n - \theta_0) + R_n(X; \hat \theta_n ),
\]
where $R_n(X; \hat \theta_n ) \equiv \frac{1}{2}(\hat \theta_n - \theta_0)^\top H(X; \theta_0) (\hat \theta_n - \theta_0)$.
Further, by \eqref{eq:scorezero},
\begin{align}\label{eq:aic1}
	\E_X[\log p(X; \hat \theta_n)] - \E_X[\log p(X; \theta_0)] \approx \E_X[R_n(X; \hat \theta_n )].
\end{align}
Also, letting $\bar s_n(\theta_0) \equiv \frac{1}{n}\sum_{i = 1}^n s(X_i; \theta_0)$, observe that by \eqref{eq:taylor} and the information matrix equality \eqref{eq:infoeq},
\begin{align}\label{eq:aic2}
\begin{split}
	 \frac{1}{n}\sum_{i = 1}^n \log p(X_i; \hat \theta_n) - \E_X[\log p(X; \theta_0)] 
	& \approx \frac{1}{n}\sum_{i = 1}^n \left(\log p(X_i; \hat \theta_n) - \log p(X_i; \theta_0) \right) \\
	& \approx \bar s_n(\theta_0)^\top (\hat \theta_n - \theta_0) + \frac{1}{n}\sum_{i=1}^nR_n(X_i; \hat \theta_n ) \\
	& \approx -\bar s_n(\theta_0)^\top \underbrace{\left[\frac{1}{n}\sum_{i=1}^n H(X_i; \bar \theta_n) \right]^{-1}}_{\overset{p}{\to}\: -[I(\theta_0)]^{-1}}\bar s_n(\theta_0) +  \E_X[R_n(X; \hat \theta_n )].
\end{split}
\end{align}
Combining \eqref{eq:aic1} and \eqref{eq:aic2} yields
\[
	\frac{1}{n}\sum_{i = 1}^n \log p(X_i; \hat \theta_n) - \E_X[\log p(X; \hat \theta_n)] \approx \bar s_n(\theta_0)^\top [I(\theta_0)]^{-1}\bar s_n(\theta_0).
\]
Then, under the IID assumption,
\begin{align*}
\begin{split}
	\E_{data} \left[\frac{1}{n}\sum_{i = 1}^n \log p(X_i; \hat \theta_n) - \E_X[\log p(X; \hat \theta_n)]\right]
	& \approx \E_{data}\left[\bar s_n(\theta_0)^\top [I(\theta_0)]^{-1}\bar s_n(\theta_0)\right]\\
	& =\text{trace}\left\{ [I(\theta_0)]^{-1}\E_{data}\left[ \bar s_n(\theta_0)\bar s_n(\theta_0)^\top \right] \right\}\\
	& =\text{trace}\Bigg\{ [I(\theta_0)]^{-1}\Bigg(\frac{1}{n}\sum_{i = 1}^n \underbrace{\E_{data}\left[ s(X_i; \theta_0) s(X_i; \theta_0)^\top \right]}_{= \: I(\theta_0)} \Bigg) \Bigg\}/n\\
	& = \frac{\text{trace}\left\{I_{\text{dim}(\theta_0)}\right\}}{n} = \frac{\text{dim}(\theta_0)}{n}.
\end{split}
\end{align*}
Finally, this implies that we can approximate $\E_{data} \E_X[\log p(X; \hat \theta_n)]$ by
\begin{align*}
	\E_{data} \E_X[\log p(X; \hat \theta_n)] 
	& \approx \frac{1}{n}\sum_{i = 1}^n \log p(X_i; \hat \theta_n) - \frac{\text{dim}(\theta_0)}{n}\\
	& = \frac{1}{n} ( \ell_n(\hat \theta_n) - \text{dim}(\theta_0) ) = - \frac{1}{2n} \text{AIC}, 
\end{align*}
where $\text{AIC}$ is defined as
\[
	\text{AIC} \equiv - 2\ell_n(\hat \theta_n) + 2 \text{dim}(\theta_0).
\]
Hence, minimizing the AIC is approximately equivalent to choosing a model which has minimum KL divergence to the true data distribution.
This way of model selection was first proposed in \cite{akaike1973information}, which is known as \textbf{Akaike's Information Criterion\index{Akaike's Information Criterion}}.
\begin{center}
	\includegraphics[width=12cm]{akaike.png}
\end{center}
It should be noted that the derivation of the AIC statistic involves a lot of large sample approximations, and it is known that the AIC tends to select a less parsimonious model under finite sample size.
Then, to improve the finite sample model selection performance, a number of modified versions of AIC has been proposed in the literature.

\section*{Appendix: Irregular estimators}

Let $\{X_1, \ldots, X_n\}$ be an IID sample drawn from $N(\mu_0, 1)$.
Then, the MLE for $\mu_0$ is the sample average $\bar X_n$, and we have the asymptotic normality result: $\sqrt{n}(\bar X_n - \mu_0) \overset{d}{\to} N(0, 1)$. 
Note that this argument holds for any given $\mu_0$.

Now, we consider the following estimator:
\begin{align*}
	\hat \mu_n = \begin{cases}
		\bar X_n & \text{if \;\; $|\bar X_n| \ge n^{-1/4}$}\\
		0        & \text{if \;\; $|\bar X_n| < n^{-1/4}$}
	\end{cases}
\end{align*}
This estimator is known as \textbf{Hodge's estimator\index{Hodge's estimator}}.
Namely, Hodge's estimator returns the MLE when $|\bar X_n| \ge n^{-1/4}$ and zero when $|\bar X_n|$ does not exceed $n^{-1/4}$.

To derive the limiting distribution of Hodge's estimator, we first consider the case where $\mu_0 \neq 0$.
Then, since $|\mu_0| - n^{-1/4} > 0$ for sufficiently large $n$, we have
\begin{align*}
	\Pr(\hat \mu_n \neq \bar X_n) 
	& = \Pr(|\mu_0 - (\mu_0 - \bar X_n) | < n^{-1/4}) \\
	& \le \Pr(|\mu_0| - |\mu_0 - \bar X_n| < n^{-1/4}) \\
	& = \Pr(|\mu_0| - n^{-1/4} < |\mu_0 - \bar X_n|) \to 0
\end{align*}
as $n \to \infty$.
This implies that $\hat \mu_n$ has the same limiting distribution as the MLE; that is, $\sqrt{n}(\hat \mu_n - \mu_0) \overset{d}{\to} N(0, 1)$.

Next, consider the case where $\mu_0 = 0$.
In this case, for any $\kappa > 0$,
\begin{align*}
	\Pr\left(\left| \sqrt{n} \hat \mu_n \right| > \kappa \right) 
	& = \Pr\left(\left| \sqrt{n}\hat \mu_n \right| > \kappa, \hat \mu_n = 0\right) + \Pr\left( \left| \sqrt{n} \hat \mu_n \right| > \kappa, \hat \mu_n \neq 0\right) \\
	& = \Pr(\left| \sqrt{n} \hat \mu_n \right| > \kappa, \hat \mu_n \neq 0)  \le \Pr( \hat \mu_n \neq 0). 
\end{align*}
Note that
\begin{align*}
	\Pr( \hat \mu_n \neq 0) 
	& = \Pr(|\bar X_n| \ge n^{-1/4}) \\
	& = \Pr(\sqrt{n}|\bar X_n - \mu_0| \ge n^{1/4}) \to 0
\end{align*}
as $n \to \infty$ since $\sqrt{n}|\bar X_n - \mu_0|$ is asymptotically equivalent to $|N(0,1)|$.
Hence, $\sqrt{n}\hat \mu_n \overset{p}{\to} 0$ and, in view of Lemma \ref{lem:p_to_d}, $\sqrt{n}\hat \mu_n \overset{d}{\to} 0$.
To summarize, the limiting distribution of Hodge's estimator is
\begin{align*}
	\sqrt{n}(\hat \mu_n - \mu_0) \overset{d}{\to} \begin{cases}
		N(0,1) & \text{if \;\; $\mu_0 \neq 0$}\\
		0      & \text{if \;\; $\mu_0 = 0$}.
	\end{cases}
\end{align*}
Although it is tempting to interpret this result as indicating that Hodge's estimator is better than the MLE, Hodge's estimator has a serious drawback.
When $\mu_0$ is away from zero, $\hat \mu_n$ behaves almost like the MLE.
However, as $\mu_0$ gets closer to zero, it differs from the MLE quite often.
To see this, we investigate the performance of $\hat \mu_n$ when $\mu_0 = h/n^{1/4}$ for some $0 < h < 1$.
The data are $\{X_1, \ldots, X_n\}$ that are drawn from $N(h/n^{1/4}, 1)$ for each $n$.
The asymptotic distribution of the MLE does not change under $\mu_0 = h/n^{1/4}$: $\sqrt{n}(\bar X_n - h/n^{1/4}) \overset{d}{\to} N(0,1)$.
On the other hand,
\begin{align*}
	\Pr(\hat \mu_n = 0)
	& = \Pr(|\bar X_n| < n^{-1/4}) \\
	& = \Pr(- n^{-1/4} < \bar X_n < n^{-1/4}) \\
	& = \Pr \left(- \sqrt{n}(n^{-1/4} + h n^{-1/4}) < \sqrt{n}(\bar X_n - h n^{-1/4}) < \sqrt{n}(n^{-1/4} - h n^{-1/4}) \right) \\
	& = \Pr\left(-n^{1/4}(1 + h) < \sqrt{n}(\bar X_n - h n^{-1/4}) < n^{1/4}(1 - h)\right) \to 1.
\end{align*}
Hence,
\begin{align*}
	\sqrt{n}(\hat \mu_n - h/n^{1/4}) \overset{p}{\to} hn^{1/4} \to \infty.
\end{align*}
Thus, the mean squared error (MSE) of Hodge's estimator $\E [\{\sqrt{n}(\hat \mu_n - \mu_0)\}^2]$ explodes to infinity as $n$ increases when $\mu_0$ is close to zero, whereas $\E[\{\sqrt{n}(\bar X_n - \mu_0)\}^2] = 1$ holds for any $\mu_0$.
Figure \ref{fig:hodge} shows the graphs of the MSE of Hodge's estimator for three different $n$'s.
The R code to create this figure is attached below.
Hodge's estimator ``buys'' its better asymptotic behavior exactly at $\mu_0 = 0$ at the cost of erratic behavior close to zero.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width = 9cm]{hodge.png}
		\caption{MSE of Hodge's estimator}
		\label{fig:hodge}
	\end{center}
\end{figure}

The above discussion shows that in order to have a meaningful discussion on the efficiency of estimators, we need to rule out pathological estimators such as Hodge's estimator and focus on ``regular'' estimators.
An estimator $\hat \theta_n$ is called \textbf{regular\index{regular estimators}} if a small change in the true parameter $\theta_0$ changes the distribution of the estimator only slightly.
Hodge's estimator is a typical example of ``irregular'' estimators.
Within the class of regular estimators, we can show the optimality of MLE.
For more precise discussion, see, for example, Section 8.5 of \cite{van2000asymptotic}.

\bigskip

\textbf{R code to create Figure \ref{fig:hodge}}

\bigskip

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
ns  <- c(10, 50, 500)
mus <- seq(-1.5, 1.5, 0.05)

mse <- function(n, mu){

	se <- function(seed){
	   set.seed(seed)
	   X <- rnorm(n, mean = mu, sd = 1)
	   Xbar <- mean(X)
	   H <- 0*(abs(Xbar) < n^{-1/4}) + Xbar*(abs(Xbar) >= n^{-1/4})
	   se <- (sqrt(n)*(H - mu))^2
	   return(se)
	   }

   R <- numeric(2000)
   for(r in 1:2000) R[r] <- se(r)
   return(mean(R))
	
   }
	
result <- matrix(0, length(ns), length(mus))
for(i in 1:length(ns)) for(j in 1:length(mus)) result[i,j] <- mse(ns[i],mus[j])

plot(mus, result[1,], ylim = c(0,13), xlab = "mu", ylab = "MSE", type = "l", col = 1)
par(new = T)
plot(mus, result[2,], ylim = c(0,13), xlab = "mu", ylab = "MSE", type = "l", col = 2)
par(new = T)
plot(mus, result[3,], ylim = c(0,13), xlab = "mu", ylab = "MSE", type = "l", col = 4)
legend("topleft", legend = c("n = 10", "n = 50", "n = 500"), lty = c(1, 1, 1), col = c(1, 2, 4))
\end{lstlisting}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Generalized Method of Moments}\label{chap:gmm}
\section{Moment conditions}\label{sec:mom}

Consider the following multiple regression model:
\begin{align*}
	Y = X_1^\top \theta_0 + \eps.
\end{align*}
We assume that the regressor $\underset{d_{x1} \times 1}{X_1}$ is exogenous: $\E [X_1 \eps] = \mbf{0}$, and that $\E[X_1 X_1^\top]$ is nonsingular.
Then, one can estimate $\theta_0$ by the OLS estimator based on the following moment equations:
\begin{align*}
	\underset{d_{x1} \times 1}{\mbf{0}} = \E[X_1 \eps] = \E[X_1 (Y - X_1^\top\theta_0)]  \iff \theta_0 = \E[X_1 X_1^\top]^{-1} \E[X_1 Y].
\end{align*}
Note here that the number of moment equations $\underset{d_{x1} \times 1}{\E[X_1 (Y - X_1^\top \theta_0)] = \mbf{0}}$ available is exactly the same as the number of elements in $\underset{d_{x1} \times 1}{\theta_0}$.
In this case we say that $\theta_0$ is \textbf{just-identified\index{just-identification}}.\footnote{
	It is well-known in linear algebra that when the number of independent linear equations equals the number of unknowns, the system can be uniquely solved.
}

Now suppose that we know that not only $X_1$ but also $\underset{d_{x2} \times 1}{X_2}$ satisfies the same moment equality $\E [X_2 \eps] = \mbf{0}$.\footnote{$X_2$ could be a function of $X_1$; see also footnote \ref{foot:ols_moment} in Chapter \ref{chap:LLN_CLT}.}
For simplicity suppose that $d_{x1} = d_{x2}$.
These additional $d_{x2}$ moment conditions $\underset{d_{x2} \times 1}{\E[X_2 (Y - X_1^\top \theta_0)] = \mbf{0}}$ can be utilized in the estimation of $\theta_0$.
That is, if $\E [X_2X_1^\top]^{-1}$ exists, we can characterize $\theta_0$ alternatively by
\[
	\theta_0 = \E[X_2 X_1^\top]^{-1} \E[X_2 Y].
\]
This situation is called \textbf{over-identification\index{over-identification}}; that is, the number of available moment equations is larger than the number of unknowns.
In other words, we can write down $\theta_0$ as a function of the moments in several different ways. 
\bigskip

The above example is a special case of a more general class of moment restriction models.
Let $g(W; \theta)$ be an $\mbb{R}^J$-valued function, where $\theta$ is a $K \times 1$ vector of parameters, and $W$ is a vector of observable random variables.
We assume that $J \geq K$.
Suppose that the following moment conditions hold:
\begin{align}\label{eq:momcond}
	\E[g(W; \theta_0)] = \underset{J \times 1}{\mbf{0}},
\end{align}
where $\theta_0$ is the true value of $\theta$.
In the above multiple regression model, $g(W; \theta_0)$ corresponds to $(X_1^\top, X_2^\top)^\top (Y - X_1^\top\theta_0)$, and $W$ corresponds to $(Y, X_1, X_2)$; thus, in this case, $J = d_{x1} + d_{x2}$ and $K = d_{x1}$ (note that any two linearly dependent moment conditions are counted as the same moment condition).
If $J = K$, we say that $\theta_0$ is just-identified, and if $J > K$, it is over-identified.

\begin{example}[Linear regression]\label{ex:ols}\upshape
	Consider the following linear regression model with exogenous regressors:
	\[
	Y = X^\top\theta_0 + \eps, \;\; \E[X \eps] = \mbf{0}.
	\]
	Then, setting $g(W; \theta) = X( Y - X^\top\theta)$ gives \eqref{eq:momcond} with $W = (Y,X)$.
\end{example}

\begin{example}[Instrumental variables regression]\label{ex:2sls}\upshape
	Consider the following linear regression model with both endogenous and exogenous regressors:
	\[
	Y = D^\top\alpha_0 + X^\top\beta_0 + \eps, \;\; \E[D \eps] \neq \mbf{0}, \;\; \E[X \eps] = \mbf{0}.
	\]
	Suppose that we have a set of instrumental variables $Z_1$ for $D$, and let $Z = (Z_1^\top, X^\top)^\top$.
	Then, we have $\E [Z \eps] = \mbf{0}$, which gives \eqref{eq:momcond} by setting $g(W; \theta) = Z( Y - D^\top\alpha - X^\top\beta)$ with $\theta_0 = (\alpha_0^\top, \beta_0^\top)^\top$ and $W = (Y,D,X,Z_1)$.
\end{example}

\begin{example}[Probit model 1]\upshape\label{ex:probit}
	Consider the following probit model:
	\[
		D = \mbf{1}\{X^\top\theta_0 \geq \eps\}, \;\; \eps \sim N(0,1).
	\]
	Assuming that $\eps$ is independent of $X$, the conditional expectation of $D$ given $X$ is $\E [D \mid X] = \Phi(X^\top \theta_0)$, where $\Phi$ is the standard normal distribution function.
	Trivially, this implies that
	\[
		\E[D -  \Phi(X^\top \theta_0) \mid X] = 0.
	\]
	This ``conditional" moment restriction implies the ``unconditional" moment restrictions $\E [X (D -  \Phi(X^\top \theta_0))] = \mbf{0}$ by LIE:
	\begin{align*}
	\E [X (D -  \Phi(X^\top \theta_0))] 
	& = \E \left\{ \E[X (D -  \Phi(X^\top \theta_0)) \mid X] \right\} \\
	& = \E \left\{ X \E[ D -  \Phi(X^\top \theta_0) \mid X] \right\} = \E \left( X \times 0 \right) = \mbf{0}.
	\end{align*}
	Hence, setting $g(W; \theta) = X (D -  \Phi(X^\top \theta))$ with $W = (D,X)$, the probit model also fits in the framework of \eqref{eq:momcond}.
\end{example}

\begin{example}[Probit model 2]\label{ex:prob2}\upshape
	There are other moment conditions that can be used to estimate the probit model.
	First, note that the true parameter $\theta_0$  can be characterized as the maximizer of the population log-likelihood function (see Section \ref{subsec:MLEconsistency}):
	\[
		\theta_0 = \argmax_\theta \E\left[ D \cdot \log \Phi(X^\top \theta) + (1 - D) \cdot \log (1 - \Phi(X^\top \theta ) )  \right].
	\]
	The first order condition for this maximization problem is
	\[
		\E\left[ X  \eta(\theta_0) \right] = \mbf{0},
	\]
	where
	\[
		\eta(\theta_0) \equiv \frac{(D - \Phi(X^\top \theta_0)) \cdot \phi(X ^\top \theta_0)}{\Phi(X^\top \theta_0) \cdot (1 - \Phi(X^\top \theta_0) )},
	\]
	and $\phi$ is the standard normal density function.
	Thus, the score function $X  \eta(\theta)$ serves as $g(W; \theta)$
	
	Here, the term $\eta(\theta_0)$ is referred to as the ``generalized residual" for the probit model; this is the conditional expectation of $-\eps$ given $(D, X)$.
	Namely, using the formula for the expectation of a truncated random variable (see Appendix \ref{sec:truncate}) and the fact  that $-e\phi(e) = \phi'(e)$, we have
	\begin{align*}
		\E[-\eps \mid X, D = 1] = \E[-\eps \mid X, X^\top\theta_0 \geq \eps] 
		& = -\frac{\int_{-\infty}^{X^\top\theta_0} e \phi(e) \mrm{d}e}{\Phi(X^\top\theta_0)} \\
		& =  \frac{ \int_{-\infty}^{X^\top\theta_0} \phi'(e) \mrm{d}e}{\Phi(X^\top\theta_0)} = \underbrace{\frac{ \phi(X^\top \theta_0)}{\Phi(X^\top\theta_0)}}_{= \: \eta(\theta_0)|_{D = 1}}\\
		\E[-\eps \mid X, D = 0] = \E[-\eps \mid X, X^\top\theta_0 < \eps] 
		& = - \frac{\int_{X^\top\theta_0}^\infty e \phi(e) \mrm{d}e}{1 - \Phi(X^\top\theta_0)} \\
		& = \frac{ \int_{X^\top\theta_0}^\infty \phi'(e) \mrm{d}e}{1- \Phi(X^\top\theta_0)} = \underbrace{-\frac{ \phi(X^\top \theta_0)}{1- \Phi(X^\top\theta_0)}}_{= \: \eta(\theta_0)|_{D = 0}}.
	\end{align*}
\end{example}

\section{GMM procedure}
When the data of sample size $n$ are available, the sample analogue of $\E [g(W; \theta)]$ can be obtained by
\[
	\bar{g}_n(\theta) \equiv \frac{1}{n}\sum_{i = 1}^n g(W_i; \theta).
\]
Then, by the law of large numbers, we can expect that $\bar{g}_n(\theta_0) \approx \mbf{0}$ for sufficiently large $n$.
This implies that one can consider estimating $\theta_0$ by solving
\begin{align}\label{eq:mm_estimator}
	\bar{g}_n(\theta) = \mbf{0}
\end{align}
with respect to $\theta$.
This type of estimator is called the \textbf{method of moments\index{method of moments}} estimator.
The method of moments estimator has a unique solution when $J = K$ (i.e., just-identification) under appropriate rank conditions.
However, in general $J > K$, the parameter value that exactly satisfies \eqref{eq:mm_estimator} may not exist.

The idea of the \textbf{generalized method of moments\index{generalized method of moments}} (GMM) is to obtain an estimator of $\theta_0$ by finding a $\theta$ that makes $\bar{g}_n(\theta)$ as close to zero as possible. 
To do this, let $\Omega_n$ be a $J \times J$ positive definite symmetric ``weight" matrix.
Then, the GMM estimator of $\theta_0$ is defined by
\begin{align}\label{eq:gmm}
	\what{\theta}_n^{gmm} \equiv \argmin_{\theta} \bar{g}_n(\theta)^\top \Omega_n \bar{g}_n(\theta).
\end{align}
Since the objective function in \eqref{eq:gmm} has a quadratic form and $\Omega_n$ is positive definite, it holds that $\bar{g}_n(\theta)^\top \Omega_n \bar{g}_n(\theta) \geq 0$ for any $\theta$.
One can simply choose an identity matrix $I_J$ of dimension $J$ as $\Omega_n$.
In this case, $\what{\theta}_n^{gmm}$ is defined as the minimizer of $||\bar{g}_n(\theta) ||^2$, where $||\cdot||$ is the Euclidean norm.
However, as discussed later, the choice of the weight matrix affects the efficiency of the estimator.

\begin{example}[Linear regression (cont.)]\upshape
	Letting $g(W_i; \theta) = X_i( Y_i - X_i^\top\theta)$, we have
	\[
	\bar{g}_n(\theta) = \mbf{X}_n^\top (\mbf{Y}_n - \mbf{X}_n \theta)/n,
	\]
	where $\mbf{X}_n = (X_1, \ldots , X_n)^\top$, and $\mbf{Y}_n = (Y_1, \ldots , Y_n)^\top$.
	Thus, the GMM estimator $\what \theta_n^{gmm}$ is obtained by
	\begin{align*}
		\hat{\theta}_n^{gmm} = \argmin_{\theta} (\mbf{Y}_n - \mbf{X}_n \theta)^\top \mbf{X}_n \Omega_n \mbf{X}_n^\top (\mbf{Y}_n - \mbf{X}_n \theta).
	\end{align*} 
	Solving the first order condition gives
	\begin{align*}
		- 2\mbf{X}_n^\top \mbf{X}_n \Omega_n \mbf{X}_n^\top (\mbf{Y}_n - \mbf{X}_n \what \theta_n^{gmm}) = \mbf{0} 
		& \iff \mbf{X}_n^\top \mbf{X}_n \Omega_n \mbf{X}_n^\top (\mbf{Y}_n - \mbf{X}_n \what \theta_n^{gmm}) = \mbf{0} \\
		& \iff \;\; \what \theta_n^{gmm} = \left[\mbf{X}_n^\top \mbf{X}_n \Omega_n \mbf{X}_n^\top\mbf{X}_n \right]^{-1} \mbf{X}_n^\top \mbf{X}_n \Omega_n \mbf{X}_n^\top\mbf{Y}_n.
	\end{align*}
	This implies that the OLS estimator is a special case of the GMM estimator in which the weight matrix $\Omega_n$ is chosen as $\Omega_n = (\mbf{X}_n^\top \mbf{X}_n)^{-1}$.
\end{example}

\begin{example}[Instrumental variables regression (cont.)]\upshape
	Let $H = (D^\top, X^\top)^\top$ and $g(W_i; \theta) = Z_i (Y_i - H_i^\top\theta)$.
	Then, we have
	\[
	\bar{g}_n(\theta) = \mbf{Z}_n^\top (\mbf{Y}_n - \mbf{H}_n \theta)/n,
	\]
	where $\mbf{Z}_n = (Z_1, \ldots, Z_n)^\top$, and $\mbf{H}_n = (H_1, \ldots , H_n)^\top$.
	Similarly as above, the GMM estimator $\what \theta_n^{gmm}$ can be obtained by
	\begin{align*}
		\hat{\theta}_n^{gmm} = \argmin_{\theta} (\mbf{Y}_n - \mbf{H}_n \theta)^\top \mbf{Z}_n \Omega_n \mbf{Z}_n^\top (\mbf{Y}_n - \mbf{H}_n \theta).
	\end{align*}
	By easy calculations,
	\[
	\hat \theta_n^{gmm} = \left[\mbf{H}_n^\top \mbf{Z}_n \Omega_n \mbf{Z}_n^\top\mbf{H}_n \right]^{-1} \mbf{H}_n^\top \mbf{Z}_n \Omega_n \mbf{Z}_n^\top\mbf{Y}_n.
	\]
	Thus, when setting the weight matrix $\Omega_n$ as $\Omega_n = (\mbf{Z}_n^\top \mbf{Z}_n)^{-1}$, we can see that  the GMM estimator coincides with the 2SLS estimator (see \eqref{eq:2slsMat}).
\end{example}

\section{Asymptotic properties}\label{sec:asympGMM}
Let $\Omega \equiv \plim_{n \to \infty} \Omega_n$.
Suppose that the true parameter $\theta_0$ can be characterized as
\[
	\theta_0 = \argmin_\theta \E[g(W; \theta)]^\top \Omega \E[g(W; \theta)].
\]
Then, under some regularity conditions, one can show that the GMM estimator $\hat \theta_n^{gmm}$ is consistent for $\theta_0$ (see, e.g., \cite{newey1994large}).

The asymptotic distribution of $\what \theta_n^{gmm}$ can be derived as follows.
Hereinafter, we suppress the superscript ``$gmm$'' for notational simplicity.
By the first order condition for the minimization in \eqref{eq:gmm}, it holds that
\[
	\bar{g}'_n(\what \theta_n)^\top \Omega_n \bar{g}_n(\what \theta_n) = \underset{K \times 1}{\mbf{0}},
\] 
where $\bar{g}'_n( \theta) = \underset{J \times K}{\partial \bar{g}_n(\theta) / \partial \theta^\top}$.
Applying the mean-value expansion to $\bar{g}_n(\what \theta_n)$ around $\theta_0$, we have
\[
	\mbf{0} = \bar{g}'_n(\what \theta_n)^\top \Omega_n \left[\bar{g}_n(\theta_0) +  \bar{g}'_n(\bar{\theta}_n) (\what \theta_n - \theta_0) \right],
\]
where $\bar{\theta}_n \in [ \what \theta_n, \theta_0]$.
This implies that
\begin{align*}
	\sqrt{n} (\what \theta_n - \theta_0) 
	& = -\left[ \bar{g}'_n(\what \theta_n)^\top \Omega_n \bar{g}'_n(\bar{\theta}_n) \right]^{-1} \bar{g}'_n(\what \theta_n)^\top \Omega_n (\sqrt{n}\bar{g}_n(\theta_0))  \\
	& = -\left[ \bar{g}'_n(\what \theta_n)^\top \Omega_n \bar{g}'_n(\bar{\theta}_n) \right]^{-1} \bar{g}'_n(\what \theta_n)^\top \Omega_n \left[\frac{1}{\sqrt{n}}\sum_{i=1}^n g(W_i; \theta_0)\right].
\end{align*}
Letting $V \equiv \E[g(W; \theta_0)g(W; \theta_0)^\top]$ (assuming that the data are IID), by CLT, we have
\[
	\frac{1}{\sqrt{n}}\sum_{i=1}^n g(W_i; \theta_0) \overset{d}{\to} \mbf{N}(\mbf{0}, V).
\]
In addition, by WLLN, $\bar{g}'_n(\theta_0) \overset{p}{\to} M \equiv  \E[ \partial g(W; \theta_0) / \partial \theta^\top]$.
Note that if $\what \theta_n$ is a consistent estimator of $\theta_0$, so is $\bar{\theta}_n$ by its definition. 
Thus, by the continuous mapping theorem, we obtain both $\bar{g}'_n(\what \theta_n) \overset{p}{\to} M$ and $\bar{g}'_n(\bar{\theta}_n) \overset{p}{\to} M$.
Finally, by Slutsky's theorem \ref{lem:slutsky}, we have the asymptotic normality of $\sqrt{n} (\what \theta_n - \theta_0)$ as follows:
\begin{align}\label{eq:GMMnormal}
\begin{split}
	\sqrt{n} (\what \theta_n - \theta_0) 
	& \overset{d}{\to} \mbf{N}\left(\mbf{0},  (M^\top \Omega M )^{-1} M^\top \Omega V \Omega M ( M^\top \Omega M )^{-1} \right).
\end{split}
\end{align}

\hrulefill
\begin{exercise}\upshape
	Consider a linear regression model:
	\[
	Y = X^\top \beta_0 + \eps,
	\]
	where the conditional median of $\eps$ given $X$ is zero: $\text{Med}[\eps \mid X] = 0$.
	Describe how to construct a GMM estimator of $\beta_0$. 
\end{exercise}

\begin{exercise}\upshape
	Consider an endogenous probit model:
	\[
		Y = \mathbf{1}\{\beta_0 + D \alpha_0 \ge \eps\}, \;\; \eps \sim N(0,1),
	\]
	where $D$ is potentially dependent on $\eps$.
	Suppose that there is an IV $Z_1$ for $D$ such that it is a determinant of $D$ and is independent of $\eps$. 
	An analyst tries to estimate $\theta_0 = (\beta_0, \alpha_0)^\top$ using GMM with $g(W; \theta) = Z(Y - \Phi(\beta + D \alpha))$ by analogy from Example \ref{ex:probit}, where $Z = (1, Z_1)^\top$, and $W = (Y,D,Z_1)$.
	However, such an estimator is generally inconsistent.
	Explain why.
\end{exercise}

\section{Two-step optimal GMM}
It can be seen from \eqref{eq:GMMnormal} that the variance of the GMM estimator depends on the choice of the weight matrix $\Omega_n$.
An optimal GMM estimator that achieves the smallest possible asymptotic variance can be obtained by setting $\Omega_n = V_n^{-1}$ such that $\plim_{n \to \infty} \Omega_n = V^{-1}$.
With this weight matrix, the asymptotic covariance matrix in \eqref{eq:GMMnormal} can be simplified to
\[
	(M^\top V^{-1} M )^{-1} M^\top V^{-1} V V^{-1} M ( M^\top V^{-1} M )^{-1} =  (M^\top V^{-1} M )^{-1}.
\]
The optimality of this weight matrix can be easily shown as follows.
For any $\Omega$, observe that
\begin{align}\label{eq:matdiff}
\begin{split}
	&  (M^\top \Omega M )^{-1} M^\top \Omega V \Omega M ( M^\top \Omega M )^{-1}   - (M^\top V^{-1} M )^{-1} \\
	& =  (M^\top \Omega M )^{-1} M^\top \Omega V^{1/2}\left\{ I_J -  V^{-1/2}M ( M^\top V^{-1} M )^{-1} M^\top V^{-1/2} \right\} V^{1/2} \Omega M  (M^\top \Omega M )^{-1} \\
	& =   (M^\top \Omega M )^{-1} M^\top \Omega V^{1/2} \left\{ I_J - P (P^\top P)^{-1} P^\top \right\}  V^{1/2} \Omega M  (M^\top \Omega M )^{-1},
\end{split}
\end{align}
where $I_J$ is an identity matrix of dimension $J$, and $P \equiv V^{-1/2}M$.
Note that the matrix $ I_J - P (P^\top P)^{-1} P^\top$ is idempotent, and thus its eigenvalues are either $0$ or $1$.\footnote{
	This can be easily proved.
	Let $A$ be an idempotent matrix such that $AA = A$, and $\lambda$ and $x$ be an eigenvalue of $A$ and its corresponding eigenvector, respectively.
	Then, we have
	\[
	\lambda x = Ax = AAx = A(\lambda x) = \lambda Ax = \lambda^2 x.
	\]
	Since the eigenvector $x$ is a nonzero vector by definition, this is possible only when $\lambda = \lambda^2$, implying that $\lambda \in \{0,1\}$.
	}
Therefore, the left-hand side in \eqref{eq:matdiff} is positive semidefinite, implying the optimality of  $\Omega_n = V_n^{-1}$.

Recall that $V = \Cov[g(W; \theta_0)]$.
Thus, its sample analog $V_n$ can be constructed by
\[
	V_n = \frac{1}{n} \sum_{i = 1}^n g(W_i; \theta_0)g(W_i; \theta_0)^\top.
\]
However, since the true parameter $\theta_0$ is unknown, $V_n$ is also unknown, and the optimal GMM is not feasible.
To estimate $V_n$, we first need to obtain a consistent GMM estimator $\what \theta_n^{gmm}$ of $\theta_0$ with an arbitrary weighting matrix, such as $\Omega_n = I_J$.
This estimator is consistent but not fully efficient, in general.
Then, we can consistently estimate $V_n$ by
\[
	\what V_n  = \frac{1}{n} \sum_{i = 1}^n g(W_i; \what \theta_n^{gmm})g(W_i; \what \theta_n^{gmm})^\top,
\]
and re-estimate $\theta_0$ by
\[
	\what \theta_n^{opt} \equiv \argmin_{\theta} \bar{g}_n(\theta)^\top \what V_n^{-1} \bar{g}_n(\theta).
 \] 
 The estimator $\what \theta_n^{opt} $ is called the \textbf{two-step optimal GMM estimator\index{two-step optimal GMM estimator}}.
The two-step estimator has the same asymptotic distribution as the infeasible optimal GMM:
\[
	\sqrt{n} (\what \theta_n^{opt} - \theta_0)  \overset{d}{\to} \mbf{N}\left(\mbf{0},  (M^\top V^{-1} M )^{-1}  \right).
\]

\begin{example}[Probit model 2 (cont.)]\upshape
	Let $g(W; \theta) = X\eta(\theta)$, where the $\eta(\theta)$ is as defined in Example \ref{ex:prob2}.
	Since $g(W; \theta)$ is the score function of the log-likelihood, the matrix $V$ is equal to the Fisher information matrix $I(\theta)$.
	Furthermore, recalling the definition $M \equiv  \E[ \partial g(W; \theta_0) / \partial \theta^\top]$, we can see that $M$ is the expected Hessian matrix of the log-likelihood: $M = \E[H(W; \theta_0)]$.
	Finally, by the information matrix equality \eqref{eq:infoeq}, we have
	\[
	\sqrt{n} (\what \theta_n^{opt} - \theta_0)  \overset{d}{\to} \mbf{N}\left(\mbf{0}, [I(\theta_0)]^{-1}  \right).
	\]
	Thus, the optimal GMM estimator is asymptotically equivalent to the MLE in this case.
\end{example}

\hrulefill
\begin{exercise}\upshape
	\begin{enumerate}
	\item Consider the multiple regression models in Example \ref{ex:ols}. 
	Prove that the OLS estimator is equivalent to the optimal GMM estimator under homoskedasticity.
	\item Consider the IV regression models in Example \ref{ex:2sls}.
	Prove that the 2SLS estimator is equivalent to the optimal GMM estimator under homoskedasticity.
	\end{enumerate}
\end{exercise}

\begin{exercise}\upshape
	Consider the following model: $Y = \theta_0 + \varepsilon$ with $\E[\eps] = 0$ and, for a $J \times 1$ vector of random variables $X$, $\E[X \varepsilon] = \mathbf{0}$.
	Assuming that IID data $\{(Y_i,X_i): 1 \le i \le n\}$ are available, derive the optimal GMM estimator of $\theta_0$.
\end{exercise}

\section{Over-identification test}

Suppose that we would like to know the validity of our moment conditions by testing the following hypotheses:
\begin{align*}
	& \mbb{H}_0: \E[g(W; \theta_0)] = \mbf{0}\\
	& \mbb{H}_1: \E[g(W; \theta)] \neq \mbf{0} \;\; \text{for all $\theta$}.
\end{align*}
In the case of just-identification $J = K$, it is possible to find a GMM estimator that exactly solves $\bar g_n(\hat \theta_n^{gmm}) = \mbf{0}$.
Note that such $\hat \theta_n^{gmm}$ may exist even when $\mbb{H}_1$ is true.\footnote{
	For example, consider a linear regression model $Y = X^\top \theta_0 + \eps$, where $X$ is endogenous: $\E[X \eps] \neq \mbf{0}$.
	Set $g(W; \theta) = X(Y - X^\top\theta)$ based on the wrong model assumption of $\E[X \eps] = \mbf{0}$.
	Then, we can find a $\hat \theta_n^{gmm}$ that exactly solves $\bar g_n(\hat \theta_n^{gmm}) = \mbf{0}$ (i.e., the OLS estimator), and this estimator is clearly inconsistent.
	}
Therefore, it is generally impossible to statistically test the validity of the moment conditions in a just-identified model.

Suppose now that $J > K$.
Then, by solving $K$ moment equations, we can set them equal to zero.
If all $J$ moment conditions are actually valid, then the remaining $J-K$ moment conditions should also be close to zero, otherwise they are away from zero.
This implies that in an over-identified model, we can test the validity of $J - K$ over-identifying moment conditions.
Under $\mbb{H}_0$, by CLT, we have
\[
	\sqrt{n} V^{-1/2} \bar g_n (\theta_0) \overset{d}{\to} \mbf{N}(\mbf{0}, I_J).
\]
Hence, $n \bar g_n (\theta_0) V^{-1} \bar g_n (\theta_0)$ has a chi-square limit distribution, and we can test $\mbb{H}_0$ based on this fact.
However note that $\theta_0$ and $V$ are unknown.
Thus, replacing them by their consistent estimators, we can show under regularity conditions that
\begin{align*}
	n \bar{g}_n(\what \theta_n^{gmm})^\top \what V_n^{-1} \bar{g}_n(\what \theta_n^{gmm}) \overset{d}{\to} \chi^2(J - K)
\end{align*}
under $\mbb{H}_0$.
Thus, if the value of $n \bar{g}_n(\what \theta_n^{gmm})^\top \what V_n^{-1} \bar{g}_n(\what \theta_n^{gmm})$ is sufficiently large compared with the critical value of $\chi^2(J - K)$, then we can reject $\mbb{H}_0$.
This testing approach is called \textbf{Hansen's over-identification test\index{over-identification test}} (also referred to as Hansen's $J$ test).
Note that the test statistic of the over-identification test is $n$ times the objective function for the optimal GMM.
Thus, the over-identification test can be easily performed as a by-product of the optimal GMM.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Identification}\label{chap:identification}

A minimal requirement for an estimator is consistency, that is, the estimator converges in probability to its true value as the sample size tends to infinity. 
A necessary condition for the existence of consistent estimators for a parameter is that the parameter can be ``identified''.
In econometrics, \textbf{identification\index{identification}} means that the parameter of interest can be ``uniquely'' determined from the observable population data (not from the sample observations).
More intuitively, identification means that if we could obtain infinite observations, the true parameter is knowable as a unique value.
For example, for maximum likelihood estimation of a parameter $\theta_0$, we can say ``$\theta_0$ is identified'' if $\theta_0$ uniquely solves the maximization problem in \eqref{eq:pop-mle}: i.e., $\{\theta : \argmax_\theta \E[\log p(X; \theta)]\}$ is a singleton.
If there is another parameter value, say $\theta_1$, that also maximizes $\E [\log p(X; \theta)]$, we cannot distinguish which of the two is the true value.
In this case, $\theta_0$ is not identified, and it is impossible to construct a consistent estimator for $\theta_0$.

In the following, we discuss the identification problem in econometrics more formally with some examples.
Although there are a number of similar identification concepts in econometrics, all of them eventually ask if the model parameters $\theta_0$ can be recovered from the population observable data.
In this chapter, we mainly follow the definitions and terminology in \cite{hurwicz1950generalization} and \cite{matzkin2013nonparametric}.
\begin{screen}
\begin{center}
	\includegraphics[width=14cm]{identifications.png}

	\footnotesize
	A quote from \cite{lewbel2019identification}
	\normalsize
\end{center}
\end{screen}

\section{A numerical illustration: a heteroskedastic probit model}
Note that the identification problem is indeed crucial in empirical studies.
Even when there seem to be no problems with the data and statistical programming, if identification is not achieved, the resulting estimates are not reliable, or you simply cannot obtain any estimates.

Suppose that a researcher considers estimating the following two heteroskedastic probit models:
\begin{align*}
	\text{Model 1} & \;\; Y = \mathbf{1}\{\beta_0 + X \beta_1 \ge \eps\}, \;\; \text{where} \;\; \eps \sim N(0, \sigma^2_1(X)), \;\; \sigma_1(X) = |X\beta_2| \\
	\text{Model 2} & \;\; Y = \mathbf{1}\{\beta_0 + X \beta_1 \ge \eps\}, \;\; \text{where} \;\; \eps \sim N(0, \sigma^2_2(X)), \;\; \sigma_2(X) = |(1 + \sqrt{X})\beta_2|.
\end{align*}
Here, we assume that $X$ is one-dimensional and non-negative.
At first glance, both models are quite similar and seem to be valid.
However, as shown numerically below, Model 1 is a faulty model while Model 2 is not.

We generate 1000 observations for each model, where the true parameters $(\beta_0,\beta_1,\beta_2)$ are fixed at $(0,1,1)$ for both models:
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
# sample size #
 n <- 1000

# true parameters #
 b0 <- 0
 b1 <- 1
 b2 <- 1

# standard deviation functions #
 sd1 <- function(a) abs(X*a)
 sd2 <- function(a) abs((1 + sqrt(X))*a)

# data generation #
 X <- runif(n,0,2) # X ~ Uniform[0,2]
 e1 <- rnorm(n, mean = 0, sd = sd1(b2))
 e2 <- rnorm(n, mean = 0, sd = sd2(b2))
 Y1 <- ifelse(b0 + X*b1 > e1, 1, 0)
 Y2 <- ifelse(b0 + X*b1 > e2, 1, 0)
\end{lstlisting}
The log-likelihood functions are as follows:
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
 LL1 <- function(p){
	P1 <- pnorm(p[1] + X*p[2], mean = 0, sd = sd1(p[3]))
	P0 <- 1 - P1
	LL <- Y1*log(P1) + (1 - Y1)*log(P0)
	return(-sum(LL))
	}

 LL2 <- function(p){
	P1 <- pnorm(p[1] + X*p[2], mean = 0, sd = sd2(p[3]))
	P0 <- 1 - P1
	LL <- Y2*log(P1) + (1 - Y2)*log(P0)
	return(-sum(LL))
	}
\end{lstlisting}
The obtained MLEs are
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
	MLE1 <- optim(c(0,1,1), LL1, method = "BFGS")
	>  MLE1
	$par
	[1] -0.01150047  2.94351522  2.85940987
	$convergence
	[1] 0 

	MLE2 <- optim(c(0,1,1), LL2, method = "BFGS")
	>  MLE2
	$par
	[1] -0.0326636  0.9913823  0.9722578
	$convergence
	[1] 0
\end{lstlisting}
Here, \texttt{convergence = 0} means that the optimization has been successfully completed.
However, the estimated parameters for Model 1 are clearly inconsistent with the true values, whereas we can correctly estimate the true parameters in Model 2.

Where does this difference come from?
The only difference between Model 1 and Model 2 is the specification of the standrd deviation function.
In fact, the one used in Model 1 has a serious problem that makes the model unidentifiable, whereas the specification in Model 2 does not have such a problem.

In this example, since we know the true parameters, we can detect that there is something wrong with Model 1.
However, in real situations where we have no prior knowledge about the true model, it is generally a difficult task to determine whether the estimation results are legitimate or not.
One thing we can say for sure is that if the model is not identifiable, the resulting parameter estimates are not consistent.

\section{Definition of identification}

Suppose that a dependent variable $Y$ is uniquely determined by a \textbf{structural equation\index{structural equation}}
\[
	h(Y, X, \eps) = 0,
\]
where $X$ is a vector of observable covariates and $\eps$ is an unobserved error.
We can treat the marginal distribution of $X$ as known.
\cite{hurwicz1950generalization} defines a \textbf{structure} as a system of the structural equation and the distribution of $(X, \eps)$.
More specifically, the pair $S \equiv (h, F_{\eps,X})$ forms a structure, where $F_{\eps,X}$ denotes the joint distribution of $(X, \eps)$.
Then, each $S$ determines the distribution of observable variables $F_{Y,X}$.

Let $F^1_{Y,X}$ and $F^2_{Y,X}$ be the distributions of $(Y, X)$ generated by $S^1 \equiv (h^1, F^1_{\eps,X})$ and $S^2 \equiv (h^2, F^2_{\eps,X})$, respectively.

\begin{definition}
	The structures $S^1$ and $S^2$ are said to be \textbf{observationally equivalent\index{observationally equivalent}} if
	\[
	F^1_{Y,X} = F^2_{Y,X}.
	\]
\end{definition}
In words, when $S^1$ and $S^2$ are observationally equivalent, we cannot distinguish them from observable information alone because they generate the same distribution of observable variables.

A \textbf{model} is defined as a set of such structures with some restrictions on $h$ and $F_{\eps,X}$.
Let us denote the true structure as $S^* \equiv (h^*, F^*_{\eps,X})$.
Denote the set of functions $h$ that satisfy the restrictions that $h^*$ is assumed to satisfy by $H$, and denote the set of distributions $F_{\eps,X}$ that satisfy the restrictions that $F^*_{\eps,X}$ is assumed to satisfy is denoted by $\Gamma$.
Then, we can define a set of ``admissible'' structures as $\mcl{S}_{H,\Gamma} \equiv H \times \Gamma$.

\begin{definition}
	Let $F_{Y,X}$ be the distribution generated by a structure $S \in \mcl{S}_{H,\Gamma}$.
	Then, we say that $S^*$ is identified in $\mcl{S}_{H,\Gamma}$ if
	\[
	F_{Y,X} \neq F^*_{Y,X}
	\]
	for all $S \neq S^*$.
\end{definition} 

Next, consider a \textbf{feature} (or simply a parameter) of a structure, $\theta \equiv \theta(S)$.

\begin{definition}[Point-identification] \label{def:identification}
	The feature $\theta^* \equiv \theta(S^*)$ is point-identified (or simply ``identified'') if for any $S \in \mcl{S}_{H,\Gamma}$ that is observationally equivalent to $S^*$,
	\[
	\theta(S) = \theta(S^*).
	\]
\end{definition}
The above definition says that when the observable variables contain sufficiently rich information to uniquely determine the value of a feature $\theta$ under $\mcl{S}_{H,\Gamma}$, it is identifiable.

\begin{example}[Linear regression models]\upshape
	Consider the following linear regression model:
	\[
		Y = X^\top \beta + \eps,
	\]
	where $Y \in \mbb{R}$, $X \in \mbb{R}^k$, and $\eps \in \mbb{R}$.
	The structural equation is given by
	\[
	 	h(Y, X, \eps) = Y - X^\top \beta - \eps.
	\]
	The set of admissible $h(Y, X, \eps)$'s can be formed simply by 
	\[
		H \equiv \{h : h(Y, X, \eps) = Y -X^\top \beta-\eps,  \beta \in \mbb{R}^k\}.	
	\]
	For an admissible set of distributions $F_{\eps,X}$, for example, consider 
	\[
		\Gamma_1 \equiv \{F_{\eps,X} : \E[\eps \mid X] = 0\}.
	\]
	Then, in the class of models defined by $H \times \Gamma_1$, we can have
	\[
	\E [XY ] = \E[XX^\top] \beta
	\]
	by LIE.
	The above relationship is not sufficient to point-identify $\beta$ because the matrix $\E [XX^\top]$ may not be of full rank.
	That is, there are possibly infinite number of different  $\beta$'s that satisfy the above equality.

	Here, we add one more restriction on $\Gamma_1$.
	Namely, let 
	\[
		\Gamma_2 \equiv \{F_{\eps,X} : \E[\eps \mid X] = 0, \E[XX^\top] \text{ is nonsingular.} \}.
	\]
	Then, for any $S_1, S^* \in H \times \Gamma_2$ that are observationally equivalent, we have
	\begin{align*}
	\beta(S_1) & = \E_1[XX^\top]^{-1} \E_1[XY ] \\
	\beta(S^*) & = \E^*[XX^\top]^{-1} \E^*[XY ],
	\end{align*}
	where $\E_1$ denotes the expectation under $F^1_{Y,X}$ generated by $S_1$, and $\E^*$ denotes the expectation under $F^*_{Y,X}$ generated by $S^*$.
	Since $S_1$ and $S^*$ are observationally equivalent, we have $F^1_{Y,X} = F^*_{Y,X}$, implying that $\E_1[XX^\top]^{-1} = \E^*[XX^\top]^{-1}$ and $\E_1[XY ] = \E^*[XY ]$ hold.
	Thus,  $\beta(S_1) =  \beta(S^*)$ for any $S_1$ that is observationally equivalent to $S^*$, i.e.,  $\beta^* \equiv \beta(S^*)$ is identified.
\end{example}

As shown in the above example, in general, a parameter of interest $\theta^*$ can be identified if we can find a mapping, say $\psi$, such that $\theta^* = \psi(\text{the moments of observed random variables})$ holds.
In this case, we say that the identification is ``constructive''.
Since any observationally equivalent distributions have the same moments of all order, the requirement in Definition \ref{def:identification} is met automatically.
However, it is usually difficult to find such closed-form expressions for $\theta^*$ (of course, the lack of closed-form solution does not imply the failure of identification). 
There are many different, but closely related, notions of identification in the econometrics literature.
For a recent comprehensive survey, see \cite{lewbel2019identification}.

\begin{example}[Binary response models]\upshape
Consider the following binary response model:
\[
	Y = \mbf{1}\{ X^\top \beta \ge \eps\},
\]
where $X \in \mbb{R}^k$, and $\eps \in \mbb{R}$.
The structural equation is
\[
	h(Y, X, \eps) = Y - \mbf{1}\{ X^\top \beta \ge \eps \}.
\]
For simplicity, the following assumption is maintained throughout this part: $\Pr(X^\top \beta = X^\top \beta^*) = 1$
if and only if  $\beta =  \beta^*$ (which is the full-rankness condition in this context).
Further, let
\begin{align*}
	H_1 &
	\equiv \{h : h(Y, X, \eps) = Y - \mbf{1}\{ X^\top \beta \ge \eps\},  \beta \in \mbb{R}^k \}\\
	\Gamma_1 
	& \equiv \{F_{\eps,X} : \eps \text{ is independent of } X, \: \eps \sim N(0, 1)\} \\
	\Gamma_2
	& \equiv \{F_{\eps,X} : \eps \text{ is independent of } X, \: \eps \sim N(0, \sigma^2)\}.
\end{align*}
Note that clearly $\Gamma_1 \subset \Gamma_2$ holds by their definitions.
As it turns out,  $\beta$ can be identified under $H_1 \times \Gamma_1$ but not identified under $H_1 \times \Gamma_2$.
To see this, first focus on the models in $H_1 \times \Gamma_2$.
Then, for any observationally equivalent $S_1, S^* \in H_1 \times \Gamma_2$, we have
\begin{align*}
	\E_1[Y \mid X] 
	& = \Phi \left(\frac{X^\top \beta_1}{\sigma_1} \right) \\
	\E^*[Y \mid X]
	& = \Phi \left(\frac{X^\top \beta^*}{\sigma^*} \right).
\end{align*}
Since $S$ and $S^*$ are observationally equivalent ($F^1_{Y,X} = F^*_{Y,X}$), we have $\E_1[Y \mid X] = \E^*[Y \mid X]$ almost surely (a.s.), which implies
\begin{align}\label{eq:prob_nonident}
	\Phi \left( \frac{X^\top \beta_1}{\sigma_1} \right) = \Phi \left( \frac{X^\top \beta^*}{\sigma^*} \right ) \;\; \text{a.s.}
	\Longrightarrow \Pr\left(\frac{X^\top \beta_1}{\sigma_1} = \frac{X^\top \beta^*}{\sigma^*}\right) = 1
	\Longrightarrow \frac{\beta_1}{\sigma_1} = \frac{\beta^*}{\sigma^*},
\end{align}
where the first $\Longrightarrow$ is by the fact that $\Phi(\cdot)$ is strictly increasing and continuous.
This tells us that the exact value of $\beta^*$ is not identified in $H_1 \times \Gamma_2$; in order to achieve point identification of $\beta^*$, we need to impose an additional restriction on the model which ensures $\sigma_1 = \sigma^*$.
What we can identify within $H_1 \times \Gamma_2$ is only the relative scale of the elements of $\beta^*$.
For example, letting $\beta_{k}^*$ be the $k$-th element of $\beta^*$, it holds by \eqref{eq:prob_nonident} that
\[
	(\beta_{1k}/\sigma_1)/(\beta_{1k'}/\sigma_1) = (\beta_{k}^*/\sigma^*)/(\beta_{k'}^*/\sigma^*) \iff \beta_{1k}/\beta_{1k'} = \beta_{k}^*/\beta_{k'}^*
\]
for all observationally equivalent $S_1$.
In this situation, we say that  $\beta^*$ is identified ``up to scale''.
The above result is intuitively understandable from the fact that for any constant $c > 0$ the following two models
\[
	Y = \mbf{1}\{ X^\top \beta \ge \eps \} \;\; \text{and} \;\; Y = \mbf{1}\{ X^\top(c \beta) \ge c\eps\}
\]
can generate exactly the same distributions for the observable data.
Thus, for identification of  $\beta^*$, we need to introduce some scale normalization restriction.
A convenient way to do so is to assume that $\eps$ is distributed as the standard normal (i.e., probit model).
Then, for models in $H_1 \times \Gamma_1$, since both $\sigma_1$ and $\sigma^*$ are fixed at one, we have $\beta_1 =  \beta^*$ for any $S_1$ observationally equivalent to $S^*$ from \eqref{eq:prob_nonident}, implying that  $\beta^*$ is identifiable.

Other possible identification restrictions are, for example, as follows:
\begin{align*}
	H_2 
	& \equiv \{h : h(Y, X, \eps) = Y - \mbf{1}\{ X^\top \beta \ge \eps\} ,  \beta_1 = 1,  \beta_{-1} \in \mbb{R}^{k-1}\}, \; \text{where }  \beta = ( \beta_1,  \beta^\top_{-1})^\top,\\
	& \footnotesize{\text{(assuming that the first element of  $\beta$ is known to be positive)}} \\
	H_3
	& \equiv \{h : h(Y, X, \eps) = Y - \mbf{1}\{ X^\top \beta \ge \eps \} , \beta \in \mbb{R}^k, || \beta|| = 1\}.
\end{align*}
Then, we can show that $( \beta_{-1}, \sigma^2)$ and $(\beta, \sigma^2)$ are identifiable in $H_2 \times \Gamma_2$ and $H_3 \times \Gamma_2$, respectively.

It would be worth noting that the normality assumption on $\eps$ is in fact not necessary for identification.
With some scale normalization,  $\beta^*$ can be identified even when the distribution function of $\eps$ is completely unknown (see, e.g., \cite{manski1975maximum}).
\end{example}

\section{Partial identification}
In the above discussion, we have considered under what conditions a parameter of interest $\theta^* \equiv \theta(S^*)$ can be identified as a unique value.
However, in practice, it is often the case that such identification conditions are quite restrictive and not testable.
Without these conditions, although the exact value of $\theta^*$ may not be identified, if we can still identify $\theta^*_L$ and $\theta^*_U$ such that
\[
	\theta^*_L \le \theta^* \le \theta^*_U,
\]
we can infer the value of $\theta^*$ (for example, if $\theta^*_L$ is positive, we can say at least that $\theta^*$ is positive).
In this situation, we say that $\theta^*$ is ``partially'' identified (or set-identified), and $[\theta^*_L, \theta^*_U]$ is called the identified interval, or more generally, the identified set.
More formal definition is as follows:
\begin{definition}[Partial-identification]
	 The feature $\theta^* \equiv \theta(S^*)$ is \textbf{partially identified\index{partial identification}} if for any $S \in \mcl{S}_{H,\Gamma}$ that is observationally equivalent to $S^*$,
	\[
	\theta(S), \theta(S^*) \in \Theta^*.
	\]
	The set $\Theta^*$ is called the \textbf{identified set\index{identified set}}.
\end{definition}
The identified set $\Theta^*$ is said to be ``informative'' if $\Theta^*$ is a bounded set, and is ``uninformative'' if not.
Note that the identified set is not unique; for an extreme example, $\mbb{R}^{\text{dim}(\theta)}$ is always an (uninformative) identified set for $\theta^*$.
In the literature on partial identification, we are usually interested in finding the smallest (i.e., the most informative) identified set.

\begin{example}[Missing data]\upshape
	Let $Y \in \{0, 1\}$ be a dummy response variable of interest.
	Suppose that $Y$ is not observable for some individuals for some reason, and let $D \in \{0, 1\}$ be a dummy variable indicating the observability of $Y$:
	\begin{align*}
	\left\{ 
	\begin{array}{ll}
	\text{$Y$ is observed} & \text{if $D = 1$}\\
	\text{$Y$ is unobserved} & \text{if $D = 0$}.
	\end{array}\right.
	\end{align*}
	This situation is quite common when the data are collected from a questionnaire survey.
	Suppose that we would like to know the population ratio of $Y = 1$, $\E [Y]$.
	Then, a common practice is to estimate it by estimating instead $\E [Y \mid D = 1]$ using only observable data subset.
	However, in order for the equality $\E [Y] = \E[Y \mid D = 1]$ to hold, we need a strong condition, such as the independence 	between $Y$ and $D$, the so-called \textbf{missing at random\index{missing at random}} assumption.

	Even when $Y$ and $D$ are dependent, in general, we can construct an informative identified interval for $\E [Y]$.
	First, observe that
	\[
		\E[Y] = \E[Y \mid D = 1] \Pr(D = 1) + \E[Y \mid D = 0] \Pr(D = 0).
	\]
	For the terms on the right-hand side, only $\E [Y \mid D = 0]$ is an unknown component.
	Note that we must have $0 \le \E[Y \mid D = 0] \le 1$ since $Y$ is a dummy variable.
	Thus, the following inequalities must hold:
	\[
		\E[Y \mid D = 1] \Pr(D = 1) \le \E[Y] \le \E[Y \mid D = 1] \Pr(D = 1) + \Pr(D = 0).
	\]
	Hence, the identified interval for $\E [Y ]$ is given by
	\[
		\Theta'_{\E[Y]} \equiv \Big[ \E[Y \mid D = 1] \Pr(D = 1), \: \E[Y \mid D = 1] \Pr(D = 1) + \Pr(D = 0) \Big] .
	\]

	\begin{center}
		\includegraphics[width=15cm]{partialident.png}
	\end{center}

	In order to obtain a more informative identified interval than $\Theta'_{\E[Y]}$, we need to add more assumptions on $\E [Y \mid D = 0]$.
	For example, suppose that we know that $\E [Y \mid D = 1] \le \E[Y \mid D = 0]$.\footnote{
		For example, let $Y$ be the yes/no answer to a question about the experience of illegal drug use.
		Then, it would be somewhat likely that the respondents who did not answer this question have more experience of drug use ($Y = \text{yes}$) than those who did answer the question.
		Thus, it should be reasonable to assume that $\Pr(Y = \text{yes} \mid D = 1) \le \Pr(Y = \text{yes} \mid D = 0)$ in this example.
	}
	Then, the resulting identified interval for $\E [Y ]$ is
	\[
	\Theta^{\prime\prime}_{\E[Y ]} \equiv \Big[ \E[Y \mid D = 1], \: \E[ Y \mid D = 1] \Pr(D = 1) + \Pr(D = 0) \Big].
	\]
	Clearly, $\Theta^{\prime\prime}_{\E[ Y ]} \subseteq \Theta'_{\E[ Y ]}$.
\end{example}

\begin{example}[Regression with interval-valued dependent variables]\upshape
	It is fairly common that the exact value of variables such as wage or household wealth is not directly asked by surveys; instead, they usually ask the same question in the form of intervals.
	For example, let $Y$ be a wage variable, whose exact level is unknown, but we know that
	\[
	Y \in [Y_L, Y_L + \Delta],
	\]
	where both the value of $Y_L$ and that of $\Delta$ are known, $-\infty < Y_L < \infty$, and $0 < \Delta < \infty$.
	Suppose that we would like to estimate the impact of an explanatory variable $X$ on $Y$ using the following simple 	regression model:
	\[
	Y =  \beta_0 +  \beta_1 X + \eps, \;\; \E[\eps \mid X] = 0.
	\]
	For simplicity, we assume that $\E [X] = 0$.
	Then, if $Y$ were observed, we can identify $\beta_1$ by
	\[
		 \beta_1 = \frac{\E[ XY ]}{\E[X^2]},
	\]
	assuming that $\E [X^2] > 0$.
	When $Y$ is unknown, researchers often try to estimate $\beta_1$ by the sample analog of $\E [X(Y_L + \Delta/2)]/\E[X^2]$.
	However, this approach does not have a theoretical justification.
	\bigskip

	A more conservative and safer approach is to use partial identification. 
	Note that there exists a latent random variable $t \in [0, 1]$ such that
	\[
		Y = Y_L + t \Delta.
	\]
	Thus, we can write
	\[
		\beta_1 = \frac{\E[XY_L]}{\E[X^2]} + \frac{\E[X \cdot t \Delta]}{\E[X^2]}.
	\]
	Observe that
	\begin{align*}
		\E[X \cdot  t\Delta] 
		& =  \E[X \cdot  t \Delta\mbf{1}\{ X > 0 \}] + \E[X \cdot  t \Delta\mbf{1}\{ X < 0\}] \\
		& \le \E[X \cdot  1 \Delta\mbf{1}\{ X > 0 \}] + \E[X \cdot  0 \Delta\mbf{1}\{ X < 0\}] \\
		& =  \E[X \cdot  \Delta\mbf{1}\{ X > 0\}].
	\end{align*}
	This implies that the upper bound of  $\beta_1$ is obtained by
	\[
	 	\beta_1 \le \frac{\E[XY_L]}{\E[X^2]} + \frac{\E[X \cdot \Delta\mbf{1}\{ X > 0\}]}{\E[X^2]}	.
	 \]
	Similarly, we can show that $\E [X \cdot t\Delta] \ge \E[X \cdot \Delta\mbf{1}\{ X < 0\}]$, and thus the lower bound of  $\beta_1$ is
	\[
		\beta_1 \ge \frac{\E[XY_L]}{\E[X^2]} + \frac{\E[X \cdot \Delta\mbf{1}\{ X < 0\}]}{\E[X^2]}.
	\]
	Consequently, we can obtain the identified interval for  $\beta_1$ as
	\[
	\Theta_{\beta_1} \equiv \Bigg[
	\frac{\E[XY_L]}{\E[X^2]} + \frac{\E[X \cdot \Delta \mbf{1}\{ X < 0 \}]}{\E[X^2]},
	\;\;
	\frac{\E[XY_L]}{\E[X^2]} + \frac{\E[X \cdot \Delta\mbf{1}\{ X > 0 \}]}{\E[X^2]}
	\Bigg].
	\]
	From this, we can observe that the length of the identified interval is proportional to $\Delta$.
	For more general discussion on this topic, see \cite{bontemps2012set}
\end{example}

\hrulefill
\begin{exercise}\upshape
	Consider a simple regression model:
	\[
	Y = \beta_0 + \beta_1 X + \eps, \;\; \E \eps = 0,
	\]
	where the regressor $X$ is suspected to be endogenous: $\Cov(X, \eps) \neq 0$.
	Suppose that we have an ``imperfect'' instrument $Z \in \mbb{R}$ for $X$ such that $\Cov(X, Z) > 0$ and $\underbar c \le \Cov(Z,\eps) \le \bar c$, where $\underbar c$ and $\bar c$ are known constants.
	Derive an informative identified interval for $\beta_1$.
\end{exercise}

\begin{exercise}\upshape
Imagine you are a seller at an online store like Amazon.
You have sold out your products to 300 people at the site.
Among these 300 buyers, 100 of them posted the product's review with a star rating.
The distribution of the star-ratings is as follows.
Suppose that you would like to know the value of $\mu \equiv \E[\text{the number of $\star$'s}|\text{buyers}]$.

\begin{table}[h]
	\caption{Distribution of star-ratings}
	\begin{center}
	\begin{tabular}{c|c}
	\hline
	$\star \star \star \star \star $ & 20 \\
	$\star \star \star \star $ & 32 \\
	$\star \star \star $ & 24 \\
	$\star \star $ & 9 \\
	$\star $ & 15 \\
	\hline
	Total & 100\\
	\hline
	\end{tabular}
	\end{center}
\end{table}

\begin{enumerate}
	\item If you assume that the missing-at-random (MAR) holds, what is the estimate of $\mu$?
	\item Do you think the MAR assumption is credible in this context? Why?
	\item When imposing no assumptions on the non-respondents, we can only partially identify $\mu$.
	(a) Derive the identified set, and (b) report the computed identified set.
	\item Suppose you would like to obtain a smaller identified set than the one obtained in 3.
	Explain specifically (a) what kind of additional assumption(s) you would introduce and (b) to what extent the resulting identified set is smaller than the one in 3. 
\end{enumerate}
\end{exercise}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Structural Estimation}

One of the most important tasks in econometrics is to empirically simulate the impact of new policies.
However, there is a huge difficulty to quantify the policy impact if the policy is purely hypothetical and there are no similar policies implemented in the past.
In such situations, pure causal inference methods are infeasible simply because we cannot observe data ``after'' the policy.

One possible approach to circumvent this problem is to use a \textbf{structural estimation\index{structural estimation}} technique.
Structural estimation is a method to construct econometric models explicitly based on economic theory and  estimate the parameters of functions (such as utility function, supply function, etc.) that govern the behavior of individuals and firms.
Once these functions are recovered, even when we do not have any data for the policy, we can predict how individuals and firms behave in the hypothetical situation using some economic theory as a guideline.\footnote{
	The idea of structural estimation is closely related to the well-known \textbf{Lucas critique\index{Lucas critique}} of macroeconomic forecasting:
	\begin{quote}\it
		Given that the structure of an econometric model consists of optimal decision-rules of economic agents, and that optimal decision-rules vary systematically with changes in the structure of series relevant to the decision maker, it follows that any change in policy will systematically alter the structure of econometric models. \upshape a quote from \cite{LUCAS197619}.
	\end{quote}
	His statement can be interpreted as that (atheoretical) ``reduced-form'' econometric models should not be used for the purpose of predicting the effect of a new policy.
	Rather, we should model the ``deep'' (i.e., policy invariant) parameters that characterize individual behavior based on microeconomic foundations.
	}

Note that the structural estimation does not refer to a particular estimation method or a type of econometric models.
Rather, it is a way of thinking about econometric models.
As shown below, even a simple OLS regression can be viewed as a structural estimation approach under certain microeconomic assumptions.
Then, the validity of a structural econometric model and its prediction is reduced to the validity of the economic assumptions on which the model is based.

\section{OLS estimation of production functions}
Consider a problem of estimating the agricultural production function of farmers.
For simplicity, suppose that each  farmer has only two inputs, labor $L$ (the number of workers) and capital $K$ (the size of farm land). 
The amount of agricultural output is denoted by $P$.
Then, simply running a linear regression of $P$ on $(L,K)$ is not recognized as a ``structural'' estimation, unless there is a particular reason to believe that the production function is a linear function.
Thus, the obtained regression coefficients cannot be viewed as the estimates of the deep parameters.

A more reasonable (i.e., more justifiable in terms of economic theory) functional form of the production function would be a Cobb--Douglas production function:
\[
	P = A L^\alpha K^\beta,
\] 
where $A$  denotes an idiosyncratic productivity disturbance.
The parameters $\alpha$ and $\beta$ are the output elasticities of labor and capital, respectively.
Taking the log of both sides yields the following linear regression model:
\[
	\log(P) = c + \log(L) \alpha + \log(K) \beta + \eps,
\]
where $c = \E[\log(A)]$, and $\eps = \log(A) - c$.
Hence, by simply running an OLS regression of $\log(P)$ on $(\log(L), \log(K))$, we can estimate the elasticity parameters.

It should be important to note that, although atheoretical (non-structural) estimation approaches cannot be used to simulate counterfactual policy changes, they can be used for the purpose of predicting the other farmers' output level $P$ under the same market condition.
If one's research interest is of the latter type, it is not necessary to consider a structural estimation; rather, it has been shown that using modern \textbf{machine learning} techniques is superior to the classical econometric methods for this purpose (see, e.g., \cite{bajari2015machine}).

\begin{center}
	\includegraphics[width = 10.5cm]{prodfunc.png}
\end{center}

\section{Discrete choice models: the random utility framework}

Discrete choice models, such as the logit model, can be viewed as structural econometric models based on the revealed preference approach.
Suppose that there are two goods, say Good 1 and Good 2.
Let $D_i$ be a dummy outcome variable that takes one if consumer $i$ chooses Good 1 and zero if Good 2 is chosen.
Following the revealed preference approach, we can assume that $D_i = 1$ if and only if $U_{i1} > U_{i2}$, where $U_{i1}$ and $U_{i2}$ are the utilities obtained from Good 1 and Good 2, respectively.

For each individual $i$, let $X_i$ be a vector of ``observable'' individual characteristics, and $\eps_{ji}$ be an ``unobservable'' random variable that may be specific to Good $j$.\footnote{
	Note that $\eps_{ji}$ is unobservable only to researchers, but of course $i$ knows the value of her own $\epsilon$.
	}
Then, without loss of generality, we can write $i$'s utility of choosing Good $j$ as $U_{ij} = U_{ij}(X_i, \eps_{ji})$.

Here, unlike the traditional economic model of consumer demand, it is allowed that we cannot fully observe the variables that determine the utility.
Namely, the utility can depend on an unobservable random factor $\eps$.
This framework is called \textbf{random utility model\index{random utility model}} (RUM).\footnote{
	Based on the random utility framework, Daniel McFadden developed a set of econometric methods for analyzing discrete choice behavior. 
	He was awarded the Nobel Prize in economics for this work.
	}
To facilitate the discussion, suppose that the utility function takes the following form:
\[
	U_{ij}(X_i, \eps_{ji}) \equiv V_j(X_i) + \eps_{ji}, \;\; \text{for $j = 1,2$}.
\]
Then, we have
\begin{align*}
	D_i = 1
	& \iff U_{i1}(X_i, \eps_{1i}) > U_{i2}(X_i, \eps_{2i}) \\
	& \iff  V_1(X_i) + \eps_{1i} >  V_2(X_i) + \eps_{2i} \\
	& \iff  V_1(X_i) - V_2(X_i) > \eps_{2i} - \eps_{1i}.
\end{align*}

Now, we assume that the error terms $\eps_{ji}$'s are independent of $X_i$ and are IID as \textbf{Type-1 Extreme Value\index{Type-1 Extreme Value}} (also known as \textbf{Gumbel} distribution).
It is known that the difference of two independent Type-1 Extreme Value variables has the standard logistic distribution.
Therefore, we obtain
\[
	\Pr(D_i = 1 \mid X_i) = \frac{\exp[V_1(X_i) - V_2(X_i)]}{1 + \exp[V_1(X_i) - V_2(X_i)]}.
\]
In applications, we often assume a linear functional form for $V_1(X_i) - V_2(X_i)$ such that
\[
	V_1(X_i) - V_2(X_i) = X_i^\top \beta_0.
\]
for some $\beta_0$.
From this perspective, we can see that the regression coefficient $\beta_0$ in the binary response models corresponds to the difference of the marginal utilities; that is, 
\begin{align*}
	\beta_0 
	& = \frac{\partial V_1(X_i)}{\partial X} - \frac{\partial V_2(X_i)}{\partial X}\\
	& = \frac{\partial U_{i1}(X_i, \eps_{1i})}{\partial X} - \frac{\partial U_{i2}(X_i, \eps_{2i})}{\partial X}
\end{align*}
under the assumptions made here.

\section{Estimation of entry games}\label{sec:games}

In the last few decades, there has been a growing interest in the econometric analysis of game theoretic models (see, e.g., \cite{de2013econometric} for a survey on this topic).
An important early example is the estimation of \textbf{entry games\index{entry games}} (e.g., \cite{bresnahan1990entry,berry1992estimation}).
Suppose that there are two players competing in an entry game across $n$ different markets.
The players are labeled by $j = 1, 2$.
In applications, these labels refer to the actual name of the firms (e.g., JAL and ANA; Walmart and Kmart; IKEA and Nitori). 
A player's entry decision depends on its profit, which in turn depends on whether the rival also enters the market or not.

Let $D_{ij}$ denote whether player $j$ enters the market $i$ ($D_{ij} = 1$) or not ($D_{ij} = 0$), where $i = 1, \ldots, n$.
For each player $j$, we assume that the payoff from entering the market $i$ is given by
\[
	u_{ij}(d_{-j}) \equiv \rho d_{-j} + X_{ij}^\top \beta - \eps_{ij},
\]
where $d_{-j}\in\{0,1\}$ denotes the action chosen by $j$'s opponent, $X_{ij}$ and $\eps_{ij}$ denote observable and unobservable (to econometricians) payoff covariates, and $\rho$ and $\beta$ are the parameters of interest.
We assume that $(\eps_{i1}, \eps_{i2})$ are independent of $(X_{i1}, X_{i2})$ for all $i$.
The parameter $\rho$ captures the \textbf{strategic interaction effect\index{strategic interaction effect}}, in which if it is negative (resp. positive) the model exhibits strategic substitutes (resp. complements).
Note that the parameters $(\rho, \beta)$ can be heterogeneous among the players in general, but we assume that they are homogeneous to simplify the discussion.
Then, the payoff matrix of the game in market $i$ is given by
\begin{center}
\begin{tabular}{c|c|c}
	\hline
	             & $D_{i2} = 0$ & $D_{i2} = 1$ \\ \hline
	$D_{i1} = 0$ & $(0, 0)$     & $(0, X_{i2}^\top \beta - \eps_{i2})$ \\ \hline 
	$D_{i1} = 1$ & $(X_{i1}^\top \beta - \eps_{i1}, 0)$ & $(\rho + X_{i1}^\top \beta - \eps_{i1}, \rho + X_{i2}^\top \beta - \eps_{i2})$\\ \hline
\end{tabular}
\end{center}

Hereinafter, we omit the market index $i$ for simplicity when there is no confusion.
Now, assume that the realizations of $(X_1, X_2)$ and $(\eps_1, \eps_2)$ are known to both players in each market, i.e., we assume a \textbf{complete information game\index{complete information game}}.
Further, we adopt the pure strategy Nash equilibrium as the solution concept of this game.
In addition, suppose that the game is a simultaneous-move game.
Then, we can see that the players' entry decisions follow the following simultaneous binary choice model:
\begin{align}\label{eq:entry}
\begin{split}
D_1 & = \mathbf{1}\{\rho D_2 + X_1^\top \beta \geq \eps_1 \} \\
D_2 & = \mathbf{1}\{\rho D_1 + X_2^\top \beta \geq \eps_2 \}.
\end{split}
\end{align}
Then, we have the following relationship between the realized outcomes $(D_1, D_2)$ and the error terms $(\eps_1, \eps_2)$:
\begin{equation} \label{eq:equilibrium}
\begin{array}{lcl}
(D_1, D_2) = (1, 1) & \Longrightarrow & \eps_1 \le \rho + X_1^\top \beta, \; \eps_2 \le \rho + X_2^\top\beta, \\
(D_1, D_2) = (1, 0) & \Longrightarrow & \eps_1 \le X_1^\top \beta, \;\;\;\;\;\;\;  \eps_2 > \rho + X_2^\top\beta, \\
(D_1, D_2) = (0, 1) & \Longrightarrow & \eps_1 > \rho + X_1^\top \beta, \; \eps_2 \le X_2^\top\beta, \\
(D_1, D_2) = (0, 0) & \Longrightarrow & \eps_1 > X_1^\top \beta, \;\;\;\;\;\;\; \eps_2 > X_2^\top\beta.
\end{array}
\end{equation}
As conventionally assumed in the literature on entry games, we assume strategic substitutes, i.e., $\rho < 0$.
Then, the relationship in \eqref{eq:equilibrium} can be visually summarized in Figure \ref{fig:nash}.
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width = 11cm]{fig_nash.png}
		\caption{Nash equilibrium}
		\label{fig:nash}
	\end{center}
\end{figure}
As shown in the figure, the space of $(\eps_1, \eps_2)$ cannot be partitioned into non-overlapping regions associated with the four alternative realizations of $(D_1, D_2)$.
Both $(D_1,D_2) = (0, 1)$ and $(D_1, D_2) = (1, 0)$ can occur when $(\eps_1, \eps_2)$ fall into the shaded area:
\[
	\begin{array}{l}
		\rho + X_1^\top\beta < \eps_1 \le X_1^\top\beta \\
		\rho + X_2^\top\beta < \eps_2 \le X_2^\top\beta
	\end{array} 
	\begin{array}{ll}
		\Longrightarrow & \text{$(D_1, D_2) = (0,1)$ or $(1,0)$}
	\end{array}
\]
That is, \textbf{multiple Nash equilibria\index{multiple equilibria}} exist in this region.

This non-uniqueness of model-consistent decisions is called \textbf{incompleteness\index{incompleteness}} and has been extensively studied in the literature on simultaneous equations models for discrete outcomes (see, e.g., \cite{tamer2003incomplete}).
When the model is incomplete, the standard maximum likelihood estimation is infeasible because the likelihood function is not well-defined.\footnote{
	If we simply construct the likelihood function based on the relationship \eqref{eq:equilibrium}, then it sums up to a value larger than one.
	}
In what follows, we discuss two approaches to overcome this problem.

\subsection{Rewriting the model in terms of the number of entrants}

Notice that even though there are multiple equilibria in the entry decisions, ``the number of entrants'' can be uniquely determined in our model under $\rho < 0$.
Namely, let $\mbf{N} \in \{0,1,2\}$ be the number of players choosing 1 and $F_j(\cdot)$ and $F(\cdot, \cdot)$ be the marginal and joint distribution function of $\eps_j$'s, respectively.
Then, we have the conditional likelihood function for each action as follows
\begin{align*}
	\text{No entrants} \;\; \Pr(\mbf{N} = 0 \mid X_1, X_2) 
	& = 1 - F_1(X_1^\top\beta) - F_2(X_2^\top\beta) + F(X_1^\top\beta, X_2^\top\beta) \\
	\text{Duopoly} \;\; \Pr(\mbf{N} = 2 \mid X_1, X_2) 
	& = F(\rho + X_1^\top\beta, \rho + X_2^\top\beta) \\
	\text{Monopoly} \;\; \Pr(\mbf{N} = 1 \mid X_1, X_2)
	& = 1 - \Pr(\mbf{N} = 0 \mid X_1, X_2) - \Pr(\mbf{N} = 2 \mid X_1, X_2).  
\end{align*}
Thus, the model can be estimated by solving the following maximum likelihood problem:
\[
	\max_{(\rho, \beta, \ldots)} \sum_{i = 1}^n \sum_{k \in \{0,1,2\}} \mbf{1}\{\mbf{N}_i = k\} \log \Pr(\mbf{N}_i = k | X_{i1}, X_{i2}).
\]
Here, the set of estimation parameters may include not only $\rho$ and $\beta$ but also some additional parameters, such as a correlation parameter between $\eps_1$ and $\eps_2$. 
This approach is pursued in \cite{bresnahan1990entry} and \cite{berry1992estimation}, which highlights that the presence of multiple equilibria does not necessarily imply the lack of point identification.

\subsection{Stochastic equilibrium selection rule}\label{subsec:stochastic}

An alternative approach is to explicitly introduce a stochastic (or possibly deterministic) equilibrium selection mechanism. 
Unlike the above approach, this approach allows us to identify the entry behavior in the region of multiplicity.
The simplest form of this approach is to assume a pure random equilibrium selection.
More specifically, suppose that the players would choose $(D_1, D_2) = (1,0)$ with probability $\lambda$ when in the multiple equilibria region.
Then, the likelihood function for each realization can be well-defined as follows:
\begin{align*}
	\Pr(0, 0 \mid X_1, X_2) 
	& = 1 - F_1(X_1^\top\beta) - F_2(X_2^\top\beta) + F(X_1^\top\beta, X_2^\top\beta)  \\
	\Pr(1, 1 \mid X_1, X_2) 
	& = F(\rho + X_1^\top\beta, \rho + X_2^\top\beta) \\
	\Pr(0, 1 \mid X_1, X_2) 
	& = F_2( X_2^\top\beta) - F(\rho + X_1^\top\beta, X_2^\top\beta) - \lambda P_{mul}(X_1, X_2) \\
	\Pr(1, 0 \mid X_1, X_2)
	& = 1 - \sum_{(d_1, d_2) \in \{(0,0), (1,1), (0,1)\}}\Pr( d_1, d_2 \mid X_1, X_2), 
\end{align*}
where
\[
	P_{mul}(X_1, X_2) \equiv F(X_1^\top\beta, X_2^\top\beta)  - F(\rho + X_1^\top\beta, X_2^\top\beta) - F(X_1^\top\beta, \rho + X_2^\top\beta) +   F(\rho + X_1^\top\beta, \rho + X_2^\top\beta) ,
\]
which is the probability that $(\eps_1, \eps_2)$ reside in the multiple equilibria region for given $(X_1, X_2)$.
Then, we can estimate the model by solving the following maximum likelihood problem:
\[
	\max_{(\rho, \beta, \lambda, \ldots)} \sum_{i = 1}^n \sum_{(d_1, d_2) \in \{0,1\}^2} \mbf{1}\{(D_{i1}, D_{i2}) = (d_1, d_2) \} \log \Pr( d_1, d_2 | X_{i1}, X_{i2}).
\]

\subsection{When the sign of $\rho$ is unknown}

So far, we have assumed that the sign of the strategic interaction effect is known a priori.
Although such assumption may be justified by economic theory in particular situations, in general, we do not have prior knowledge about $\rho$.
When the the sign of $\rho$ is unknown, it is difficult to obtain point identification of the model.

To tackle this problem, \cite{ciliberto2009market} proposed a partial identification approach.
For each realization $(D_1, D_2) = (d_1, d_2)$, we can generally write
\begin{align*}
	\Pr(d_1, d_2 \mid X_1, X_2) 
	& = \int \Pr(d_1, d_2 \mid X_1, X_2, \eps_1, \eps_2) \text{d}F \\
	& = \int_{\mcl{R}_{uni}^{(d_1, d_2)}(X_1, X_2)} \Pr(d_1, d_2 \mid X_1, X_2, \eps_1, \eps_2) \text{d}F + \int_{\mcl{R}_{mul}^{(d_1, d_2)}(X_1, X_2)} \Pr(d_1, d_2 \mid X_1, X_2, \eps_1, \eps_2) \text{d}F \\
	& = \int_{\mcl{R}_{uni}^{(d_1,d_2)}(X_1,X_2)} \text{d}F + \int_{\mcl{R}_{mul}^{(d_1, d_2)}(X_1,X_2)} \Pr(d_1, d_2 \mid X_1, X_2, \eps_1, \eps_2) \text{d}F.
\end{align*}
Here, $\mcl{R}_{uni}^{(d_1,d_2)}(X_1, X_2)$ denotes the region of $(\eps_1, \eps_2)$ where $(D_1, D_2) = (d_1, d_2)$ uniquely occurs for given $(X_1, X_2)$, and $\mcl{R}_{mul}^{(d_1,d_2)}(X_1, X_2)$ is the multiple equilibria region.
The probability function $\Pr(d_1, d_2 \mid X_1, X_2, \eps_1, \eps_2)$ plays the role of equilibrium selection function for $(d_1, d_2)$ in the multiplicity region.\footnote{
	If we can impose some parametric functional form assumption on $\Pr(d_1, d_2 \mid X_1, X_2, \eps_1, \eps_2)$ for $(\eps_1, \eps_2) \in \mcl{R}_{mul}^{(d_1, d_2)}(X_1,X_2)$, then we may point identify the model.
	Such an approach was taken in \cite{bajari2010identification}.
}
Whatever the functional form of $\Pr(d_1, d_2 \mid X_1, X_2, \eps_1, \eps_2)$ is, since $0 \le \Pr(d_1, d_2 \mid X_1, X_2, \eps_1, \eps_2) \le 1$ must hold, we have
\begin{align}\label{eq:gameinequality}
	\int_{\mcl{R}_{uni}^{(d_1,d_2)}(X_1,X_2)} \text{d}F
	\le \Pr(d_1, d_2 \mid X_1, X_2) 
	\le \int_{\mcl{R}_{uni}^{(d_1,d_2)}(X_1,X_2)} \text{d}F + \int_{\mcl{R}_{mul}^{(d_1, d_2)}(X_1,X_2)} \text{d}F. 
\end{align}
Thus, the identified set of the model parameters can be defined by the set of parameters that satisfy \eqref{eq:gameinequality}.
For practical implementation of this approach, see \cite{ciliberto2009market}.

\section{Estimation of first-price auction models}

To be added in a future update.

\section{BLP Demand Estimation}

To be added in a future update.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Bootstrap}

The bootstrap is a method to estimate the distribution of an estimator or that of a test statistic by repeatedly resampling the original data.
The bootstrap was introduced by \cite{efron1979bootstrap} and has continued to be one of the central interests of statistical research in all areas.
The bootstrap has several advantages over the classical inference methods that are based on the asymptotic distributions.
One is its algorithmic simplicity.
It is often the case that in complicated econometric models, the statistic of interest has a very complicated form of asymptotic variance, which makes its estimation cumbersome.
If we use a bootstrap method instead, we may circumvent such complicated computation.
A more important  advantage of using bootstrap is that under some regularity conditions, the approximated distribution obtained by the bootstrap is at least as accurate as and often more accurate than the approximation obtained in the first-order asymptotic theory.
That is, the bootstrap is not just a convenient alternative to the conventional asymptotic method, but it sometimes exhibits theoretically desirable properties compared to the conventional method.

The contents of this chapter are mostly based on \cite{HOROWITZ20013159} and \cite{horowitz2019bootstrap}.
Detailed mathematical proofs will be omitted.\footnote{For those who are interested in the theory of bootstrap, I suggest take a look at \cite{hall1992bootstrap} first, among many others.}

\section{The basic idea of the bootstrap method}

Suppose that we have an IID sample $\{X_1, \ldots, X_n\}$ of size $n$ obtained from an unknown distribution $F_0$.
Let $T_n \equiv T_n(X_1, \ldots, X_n)$ be the statistic of which we would like to know the distribution.
A conventional approach is to derive its asymptotic distribution as $n \to \infty$.
For example, let $T_n$ be the $t$-statistic for testing the null hypothesis $\mathbb{H}_0: \E X = 0$; namely,
\begin{align*}
	T_n = \frac{\bar X_n}{s_n},
\end{align*}
where $\bar X_n$ amd $s_n$ are the sample mean and sample standard deviation, respectively.
Then, by CLT, we know that $T_n$ is distributed as the standard normal $N(0,1)$ as $n \to \infty$.
This approximation is actually very accurate if the sample size is not small.
However, for more involved test statistics in complicated models, the asymptotic approximation can be inaccurate for moderate sample sizes.

Now, consider a general statistic $T_n$ and let $G_n(\cdot \mid F)$ be the distribution function of $T_n$ when the data are drawn from the distribution function $F$:
\begin{align*}
	G_n(t \mid F) 
	& \equiv P_F(T_n(X_1, \ldots, X_n) \le t) \\
	& = \int \mathbf{1}\{T_n(x_1, \ldots, x_n) \le t\} \mathrm{d} F^n.
\end{align*}
The true distribution of $T_n$ under $F_0$ can be written as $G_n(\cdot \mid F_0)$.
Thus, if we happen to know the exact form of $G_n(\cdot \mid F_0)$, we can conduct any statistical inference on $T_n$; however, this is a rare case since $F_0$ is usually unknown.

For data $\{X_1, \ldots, X_n\}$ that are drawn from $F_0$, we have the empirical distribution function $F_n$ defined as
\begin{align*}
	F_n(t) \equiv \frac{1}{n}\sum_{i = 1}^n \mathbf{1}\{X_i \le t\}.
\end{align*} 
Using this empirical distribution, we can compute $G_n(\cdot \mid F_n)$, instead of $G_n(\cdot \mid F_0)$, in the following manner:
\begin{description}
	\item[Step 1.] Generate a bootstrap sample $\{X_1^*, \dots, X_n^*\}$ from $F_n$. 
	\item[Step 2.] Compute the test statistic $T_n^* = T_n(X_1^*, \dots, X_n^*)$.
	\item[Step 3.] Repeat Steps 1 and 2 many times, and compute the empirical distribution of $T_n^*$. 
\end{description}
Step 1 can be simply performed by a simple random sampling from $\{X_1, \ldots, X_n\}$ with replacement.
By increasing the number of repetitions in Step 3, we can approximate $G_n(\cdot \mid F_n)$ arbitrarily precisely.\footnote{
	Note that, theoretically, the exact form of $G_n(t \mid F_n)$ can be obtained by calculating the proportion of all possible bootstrap samples that have $\{T_n^* \le t\}$.
	However, this would require $n^n$ calculations, which is not possible in practice except when $n$ is very small.
	Thus, in Step 3, we basically generate ``as many bootstrap samples as possible''.
}
The above procedure is called the \textbf{nonparametric bootstrap\index{nonparametric bootstrap}}.

By a uniform LLN, we have $|F_n(z) - F_0(z)| \to 0$ over all $z$ with probability one.
(Formally, this result is known as the \textbf{Glivenko-Cantelli theorem}.)
From this result,  for sufficiently large $n$, we can anticipate that $G_n(\cdot \mid F_n) \approx G_n(\cdot \mid F_0)$ in some sense if $G_n$ is continuous in $F$.
This is the consistency property of bootstrap.
A more formal definition is as follows.

Taking the limit $n \to \infty$ of $G_n(\cdot \mid F_0)$, we define the asymptotic true distribution of $T_n$ as $G_\infty(\cdot \mid F_0)$.
\begin{definition}[Bootstrap consistency]
	The bootstrap distribution estimator $G_n(\cdot \mid F_n)$ is said to be \textbf{consistent\index{Bootstrap consistency}} if, for any $\kappa > 0$,
	\begin{align*}
		\Pr \left(\; |G_n(t \mid F_n) - G_\infty( t \mid F_0)| > \kappa \;\; \text{for all } t \in \mathbb{R} \right) \to 0.
	\end{align*}
\end{definition}
Note that in the above definition, the bootstrap distribution $G_n(t \mid F_n) = P_{F_n}(T_n(X_1^*, \ldots, X_n^*) \le t)$ is a random CDF because $F_n$ depends on the original sampling $\{X_1, \ldots, X_n\}$.
Thus, the probability is taken over this randomness.
$G_\infty(\cdot \mid F_0)$ is not a stochastic function.
Once a consistent bootstrap distribution $G_n(\cdot \mid F_n)$ is obtained, we can perform statistical inference based on the bootstrap critical values and the confidence intervals directly obtained from $G_n(\cdot \mid F_n)$.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width = 13cm]{bootstrap.png}
		\caption{The idea of bootstrap}
	\end{center}
\end{figure}

It is difficult to provide general and easy-to-check conditions for a bootstrap estimator to be consistent.
In many practical situations, it is often the case that the test statistic of interest can be expressed as $T_n = (\bar g_n - t_n)/s_n$ for some sequences $t_n$ and $s_n$, where $\bar g_n = n^{-1}\sum_{i = 1}^n g(X_i)$ with an IID sample $\{X_1, \ldots, X_n\}$.
For this special case, \cite{mammen1992bootstrap} shows that the nonparametric bootstrap distribution of $T_n^* = (\bar g_n^* - \bar g_n)/s_n$, where $\bar g_n^* = n^{-1}\sum_{i = 1}^n g(X^*_i)$, is consistent for the distribution of $T_n$ if and only if $T_n \overset{d}{\to} N(0,1)$.
Many of commonly used estimators and test statistics, such as least squares estimators, ML estimators, $t$-statistic, etc, have this form and are asymptotically normal.
Thus, the nonparametric bootstrap is consistent for most applications.
Nonetheless, it is important to know when the bootstrap may fail.

One such example is the estimation of the distribution of the maximum or minimum value of a sample.
Let $\{X_1, \ldots, X_n\}$ be an IID sample drawn from $\mathrm{Uniform}[0, \alpha_0]$.
As we have seen in Exercise \ref{ex:maxuni}, the ML estimator of $\alpha_0$ is given by 
\begin{align*}
	\hat \alpha_n^{mle} = \max\{X_1, \ldots, X_n\}. 
\end{align*}
Define $T_n \equiv n(\alpha_0 - \hat \alpha_n^{mle})/\alpha_0$.
One can show that the limiting distribution of $T_n$ is the standard exponential.
In particular, we have $\Pr(T_n = 0) = 0$ for all $n$.

Now, let $\{X_1^*, \ldots, X_n^*\}$ be a bootstrap sample that is obtained by sampling the original data $\{X_1, \ldots, X_n\}$ randomly with replacement.
Then, the bootstrap analog of $T_n$ is $T_n^* \equiv n(\hat \alpha_n^{mle} - \hat \alpha_n^*)/\hat \alpha_n^{mle}$, where $\hat \alpha_n^* = \max\{X_1^*, \ldots, X_n^*\}$ (note that the ``population'' value for the upper bound of the bootstrap samples is $\hat \alpha_n^{mle}$).
Hence,
\begin{align*}
	P_{F_n}(T_n^* = 0)
	& = \int \mathbf{1}\{\hat \alpha_n^* = \hat \alpha_n^{mle}\} \mathrm{d} F_n^n\\
	& = \int (1 - \mathbf{1}\{\text{$\hat \alpha_n^{mle} \not\in $ bootstrap sample}\}) \mathrm{d} F_n^n\\
	& = 1 - (1 - (1/n))^n \to 1 - e^{-1}.
\end{align*}
Thus, the bootstrap distribution of $T_n^*$ is not consistent for the distribution of $T_n$.
This result would be intuitively understandable noting the fact that the bootstrap maximum never exceeds the maximum of the original sample.

As in the above example, when the target parameter is on a boundary, simple nonparametric bootstrap methods often fail.
For other examples, see \cite{HOROWITZ20013159} and \cite{horowitz2019bootstrap}

\section{Asymptotic refinements}

Let $\{\mbf{X}_1, \ldots, \mbf{X}_n\}$ be an IID sample from the distribution of the random vector $\mbf{X} \sim F_0$.
Further, let $\mbf{Z}(\cdot)$ be a vector-valued function of $\mbf{X}$, and define $\zeta \equiv \E [\mbf{Z}(\mbf{X})]$, $\mbf{Z}_i \equiv \mbf{Z}(\mbf{X}_i)$, and $\bar{\mbf{Z}} \equiv n^{-1} \sum_{i = 1}^n \mbf{Z}_i$.
The parameter of interest is $H(\zeta)$ and we estimate it by $H(\bar{\mbf{Z}})$.

For example, for a simple linear regression with a standardized regressor, we have the slope parameter $\beta = H(\zeta)$ and its estimate $\hat \beta = H(\bar{\mbf{Z}})$, where $\mbf{X} = (X, Y)$, $\mbf{Z}(\mbf{X}) = (X Y , X^2 )$, 
\begin{align*}
	\zeta = \biggl(\underbrace{\E[XY]}_{\zeta_1}, \underbrace{\E[X^2]}_{\zeta_2}\biggr), 
	\;\; \bar{\mbf{Z}} = \biggl(\underbrace{\frac{1}{n} \sum_{i = 1}^n X_i Y_i}_{\bar{\mbf{Z}}_1}, \underbrace{\frac{1}{n} \sum_{i = 1}^n X_i^2}_{\bar{\mbf{Z}}_2} \biggr),
	\;\;
	H(\zeta) = \frac{\zeta_1}{\zeta_2}, 
	\;\;  H(\bar{\mbf{Z}}) = \frac{\bar{\mbf{Z}}_1}{\bar{\mbf{Z}}_2}.
\end{align*}

In the following, we focus on estimating the distribution of the statistic
\begin{align*}
	T_n = \frac{\sqrt{n}(H(\bar{\mbf{Z}}) - H(\zeta))}{s_n},
\end{align*}
where $s_n^2$ is a consistent estimator of the variance of $\sqrt{n}(H(\bar{\mbf{Z}}) - H(\zeta))$.
We assume that $H$ is sufficiently smooth such that it has sufficiently many derivatives.
In addition, we assume the following condition:

\paragraph{Cram\'{e}r Condition} 
Let $\tau$ be a vector of constants with the same dimension as $\mbf{Z}$.
Let $i \equiv \sqrt{-1}$.
$\mbf{Z}$ satisfies the Cram\'{e}r condition:
\begin{align*}
	\limsup_{||\tau || \to \infty} |\E \exp(i \tau' \mbf{Z})| < 1. 
\end{align*} 
That is,  the Cram\'er condition requires that the absolute value of the characteristic function $\phi_\mbf{Z}(\tau) \equiv \E \exp(i \tau' \mbf{Z})$ is strictly bounded by one uniformly in $\tau$.
This condition is satisfied if $\mbf{Z}$ is continuous and has a proper density function.\footnote{
	Note that $|\phi_\mbf{Z}(\tau)| \le 1$ always holds for all $\tau$:
	\begin{align*}
		|\phi_\mbf{Z}(\tau)| 
		& \le \E | \exp(i \tau' \mbf{Z})| \\
		& = \E |\cos(\tau' \mbf{Z}) + i \sin (\tau' \mbf{Z})| \;\; (\text{Euler's formula}) \\
		& = \E\left[  (\cos^2(\tau' \mbf{Z}) +  \sin^2(\tau' \mbf{Z}))^{1/2} \right] = 1.
	\end{align*}
	For a formal derivation of the Cram\'er condition, see, e.g., Section 2.4 of \cite{hall1992bootstrap}.
}

Let $G_n(\cdot \mid F_0)$ and $G_\infty(\cdot \mid F_0)$ denote the distribution and the asymptotic distribution of $T_n$, respectively.
Similarly, $G_n(\cdot \mid F_n)$ and $G_\infty(\cdot \mid F_n)$ are defined as the bootstrap counterparts of $G_n(\cdot \mid F_0)$ and $G_\infty(\cdot \mid F_0)$, respectively.
Then, under the Cram\'er condition, we have the following result:
\begin{align}\label{eq:edgeworth}
	\begin{split}
		G_n(t \mid F_0) & = G_\infty(t \mid F_0) + \frac{1}{n^{1/2}} g_1(t, F_0) + \frac{1}{n} g_2(t, F_0) + \frac{1}{n^{3/2}} g_3(t, F_0) + O(n^{-2}) \\
		G_n(t \mid F_n) & = G_\infty(t \mid F_n) + \frac{1}{n^{1/2}} g_1(t, F_n) + \frac{1}{n} g_2(t, F_n) + \frac{1}{n^{3/2}} g_3(t, F_n) + O(n^{-2}) 
	\end{split}
\end{align}
uniformly over $t$.\footnote{
	For those who are unfamiliar with the Big-O notation (a.k.a. Landau notation): $f(n) = O(g(n))$ is equivalent to that there exists a constant $C$ such that $|f(n)| \le C |g(n)|$ for all sufficiently large $n$.
	Hence, for example, $O(n^{-2})$ means that it is a term converging to zero at rate $n^{-2}$ or faster.
}
Here, $g_1$ and $g_3$ are even functions and $g_2$ is an odd function with respect to the first argument.
The series expansion of a distribution function as given in \eqref{eq:edgeworth} is called an \textbf{Edgeworth expansion\index{Edgeworth expansion}}.
From this result, we can see that the conventional asymptotic method that approximates $G_n(t \mid F_0)$ by its asymptotic analog $G_\infty(t \mid F_0)$ has an approximation error of order $n^{-1/2}$.

Note that in our setup, both $T_n \overset{d}{\to} N(0,1)$ and $T_n^* \overset{d}{\to} N(0,1)$ hold.
That is, the first terms on the right-hand side of \eqref{eq:edgeworth} are both $\Phi(t)$, the standard normal CDF.
When the asymptotic distribution of a statistic does not depend on the DGP, such as $G_\infty(t \mid F) = N(0,1)$ for any $F$ as in the present case, we say that the statistic is \textbf{asymptotically pivotal\index{asymptotically pivotal}}.
Most test statistics are asymptotically pivotal, but most estimators are not.

Then, it follows from \eqref{eq:edgeworth} that the gap between the true distribution and the bootstrap distribution is 
\begin{align*}
	G_n(t \mid F_0) - G_n(t \mid F_n) 
	& = \frac{1}{n^{1/2}}[g_1(t, F_0) - g_1(t, F_n)] + \frac{1}{n}[g_2(t, F_0) - g_2(t, F_n)] + O(n^{-3/2}).
\end{align*}
The leading term on the right-hand side is $n^{-1/2}[g_1(t, F_0) - g_1(t, F_n)]$.
Since $F_n$ converges to $F_0$, this term converges to zero faster than $n^{-1/2}$.
Indeed, one can show that $[g_1(t, F_0) - g_1(t, F_n)] = O(n^{-1/2})$, and therefore the approximation error by the bootstrap is of order $n^{-1}$, i.e., an \textbf{asymptotic refinement\index{asymptotic refinement}}.
Thus, the bootstrap is more accurate than the conventional asymptotic approximation.
\bigskip

The bootstrap is even more accurate when approximating the symmetrical distribution $\Pr(|T_n| \le t)$.
Noting that $g_1$ and $g_3$ are even functions and $g_2$ is an odd function (i.e., $g_1(t, F) = g_1(-t, F)$, $g_2(t, F) = - g_2(-t, F)$, and $g_3(t, F) = g_3(-t, F)$) and that $\Phi(-t) = 1 - \Phi(t)$,
\begin{align*}
	P_{F_0}(|T_n| \le t) 
	& = P_{F_0}(T_n \le t) -  P_{F_0}(T_n \le - t) \\
	& = G_n(t \mid F_0) - G_n( - t \mid F_0) \\
	& = \Phi(t) - \Phi(-t) + \frac{1}{n^{1/2}}[g_1(t, F_0) - g_1(-t, F_0)] + \frac{1}{n}[g_2(t, F_0) - g_2(-t, F_0)] \\
	& \quad + \frac{1}{n^{3/2}} [g_3(t, F_0) - g_3(-t, F_0)] + O(n^{-2}) \\
	& = 2 \Phi(t) - 1 + \frac{2}{n}g_2(t, F_0) + O(n^{-2}).
\end{align*}
Similarly,
\begin{align*}
	P_{F_n}(|T_n^*| \le t) 
	& = 2 \Phi(t) - 1 + \frac{2}{n}g_2(t, F_n) + O(n^{-2}).
\end{align*}
Further, similarly as above, one can show that $[g_2(t, F_0) - g_2(t, F_n)] = O(n^{-1/2})$.
Thus, the order of the error in the bootstrap approximation to the symmetrical distribution is $n^{-3/2}$.

Note that if the statistic $T_n$ is not asymptotically pivotal (for example, not standardized $T_n = \sqrt{n}(H(\bar{\mbf{Z}}) - H(\zeta))$), the leading term of the difference $G_n(t \mid F_0) - G_n(t \mid F_n)$ will be $G_\infty(t \mid F_0) - G_\infty(t \mid F_n)$, which is typically of order $n^{-1/2}$.
Then, the error of the bootstrap approximation of the distribution
of a statistic that is not asymptotically pivotal converges to zero at the same rate as that of the conventional asymptotic approximation (i.e., no asymptotic refinements); there is no merit of using bootstrap other than  computational simplicity.

\begin{table}[h]
	\begin{tabular}{l|cccc}
		\hline
	& Asymptotic distribution     & \multicolumn{3}{c}{Bootstrap}    \\
    &                             & Pivotal                   & Pivotal + Symmetricity         & Not pivotal                 \\
		\hline
	The speed of approximation & $n^{-1/2}$ & $n^{-1}$ & $n^{-3/2}$ & $n^{-1/2}$ \\
		\hline
	\end{tabular}
	\caption{Approximation errors for estimating $G_n(\cdot \mid F_0)$}
\end{table}

\section{Other bootstrap resampling schemes}
\subsection{Parametric bootstrap}

The consistency property of the nonparametric bootstrap method is essentially a consequence of the consistency of $F_n$ for $F_0$.
Moreover, the size of the error in the bootstrap approximation is determined by the size of $|F_n - F_0|$.
Thus, to improve the performance of bootstrap, $F_n$ should be obtained by the most efficient available estimator.
Clearly, if we know the form of $F_0$ perfectly, we should use $F_0$ to generate bootstrap samples, rather than $F_n$.
When it is known that $F_0$ belongs to a parametric family with some unknown parameters $\theta$ such that $F_0(\cdot) = F(\cdot ; \theta_0)$, we can use $F(\cdot ; \theta_n)$ as a bootstrap sample generator with $\theta_n$ being a consistent estimator of $\theta_0$ such as an ML estimator.
This approach is called the \textbf{parametric bootstrap\index{parametric bootstrap}}.

\subsection{Residual bootstrap}

Suppose that we are interested in estimating the regression model
\begin{align*}
	Y_i = \mbf{X}_i^\top \beta_0 + \eps_i, \;\; \E(\eps_i \mid \mbf{X}_i) = 0.
\end{align*}
We assume that the data $\{(\mbf{X}_i, Y_i) : 1 \le i \le n\}$ are IID such that $\E(\eps_i^2) = \sigma^2$ (i.e., homoskedasticity).
$\mbf{X}_i$ includes a constant term.
For this setup, we can generate bootstrap samples in the following manner:
\begin{description}
	\item[Step 1.] Obtain the OLS estimate $\hat \beta_n$ of $\beta_0$.
	\item[Step 2.] Compute the residuals $\hat \eps_i = Y_i - \mbf{X}_i^\top \hat \beta_n$ for all $i=1, \ldots, n$.
	\item[Step 3.] Create a bootstrap sample $\{(Y_i^*, \mbf{X}_i): 1 \le i \le n\}$, where $Y_i^* = \mbf{X}_i^\top \hat \beta_n + \hat \eps_i^*$, and $\eps^*_i$'s are obtained by a random sampling from $\{\hat \eps_1, \ldots, \hat \eps_n\}$ with replacement.
\end{description}
In the above procedure, only the residuals are resampled repeatedly to create bootstrap samples, and the regressors $\mbf{X}_i$'s are the same as the original sample and are not resampled.
This approach is called the \textbf{residual bootstrap\index{residual bootstrap}}.
Note that in Step 3, we implicitly assume that the errors are identically distributed.

If we know the exact form of the distribution function of the error terms, as in the parametric bootstrap, we can draw new error terms from it, rather than sampling from the residuals.
Such a situation is common in parametric limited dependent variable models such as probit and logit.

\subsection{Wild bootstrap}

We continue to consider the same regression model as above.
However, we now assume that the error terms are heteroskedastic: $\E(\eps_i^2) = \sigma_i^2$.
In this case, using the residual bootstrap is not appropriate.

Letting $\hat \beta_n$ denote the OLS estimate of $\beta_0$ and $\hat \eps_i = Y_i - \mbf{X}_i^\top \hat \beta_n$, the \textbf{wild bootstrap\index{wild bootstrap}} generates new $Y_i^*$'s as $Y_i^* = \mbf{X}_i^\top \hat \beta_n + u_i^*$, where
\begin{align*}
	u_i^* \equiv W_i \hat \eps_i, \;\; i = 1, \ldots, n
\end{align*}
and $\{W_i: 1 \le i \le n\}$ is a randomly generated IID sequence that is independent of the whole data and satisfies $\E W_i = 0$ and $\E W_i^2 = 1$.
Common choices of distributions for $W_i$ include the standard normal and the two-point distribution that takes $(1 - \sqrt{5})/2$ with probability $(\sqrt{5} + 1)/(2 \sqrt{5})$ and $(\sqrt{5} + 1)/2$ with probability $(\sqrt{5} - 1)/(2 \sqrt{5})$.
It can be easily confirmed that
\begin{align*}
	\E [u_i^* \mid \{(\mbf{X}_i, Y_i)\}] 
	& = \E [W_i \mid \{(\mbf{X}_i, Y_i)\}]  \hat \eps_i = 0 \\
	\E [(u_i^*)^2 \mid \{(\mbf{X}_i, Y_i)\}] 
	& = \E [W_i^2  \mid \{(\mbf{X}_i, Y_i)\}]  \hat \eps_i^2 =  \hat \eps_i^2.
\end{align*}
Hence, the heteroskedasticity in the original sample is retained in the bootstrap sample.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Nonparametric Regression}

Consider a general regression model: $Y = g(X) + \eps$.
As shown in \eqref{eq:msemin}, in terms of minimization of MSE, the best regression function is the conditional expectation function: $g(X) = \E[Y \mid X]$.
A linear regression model $Y = X^\top\beta + \eps$ is a special case when we impose a funcitonal form  assumption $g(X) = X^\top \beta$, and thus its optimality hinges on whether the assumption $\E[Y \mid X] = X^\top\beta$ is satisfied or not.
In reality, however, it would be extremely rare that the data follow such a linear relationship exactly, and if the functional form assumption does not hold, what the linear regression produces is only a ``linear approximation" of $\E[Y \mid X]$.
As shown in the figure below, linear approximation is not always informative and sometimes even misleading.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width = 8cm]{reg.png}
		\caption{Linear approximation of a quadratic function}
	\end{center}
\end{figure}


Thus, ideally, we would like to estimate $\E [Y \mid X]$ without imposing any a priori functional form assumptions.
Such a regression approach is called \textbf{nonparametric\index{nonparametric}}.
In contrast, if one assumes a specific functional form $g(X, \theta)$ with a finite dimensional parameter vector $\theta$, this approach is \textbf{parametric\index{parametric}}.
The most typical parametric regression models are linear regression models.
For another example, if the range of a dependent variable is restricted to $(0,1)$, a logistic regression model $g(X, \theta) = \exp(X^\top \theta)/[1 + \exp(X^\top \theta)]$ is popularly used. 
The intermediate case between nonparametric and parametric models is called \textbf{semiparamtric\index{semiparametric}}.
As explained below, estimating a fully nonparametric regression model is almost infeasible in practice due to the curse of dimensionality unless the dimension of $X$ is less than three or at most four.
On the other hand, assuming a full parametric specification has a risk of model misspecification biases.
Then, the main idea of semiparametric regression models is to introduce mild functional form assumptions to facilitate the estimation while retaining the flexibility of nonparametric models for certain variables.
For example, if one assumes that the impact of a subset $X_1$ of $X$ on $Y$, say $\varphi(\cdot)$, is nonlinear and its functional form is left unspecified, and that the remaining subset $X_2$ is linearly related to $Y$, then the resulting regression model would be 
\begin{align*}
	Y = \varphi(X_1) + X_2^\top\beta + \eps.
\end{align*}
This type of semiparametric model is called a partially linear regression model, which will be discussed in detail later.

Note that if $X$ is a discrete random variable, the estimation of $\E[Y \mid X]$ is straightforward; split the data into subsamples according to the realized value of $X$, and compute the average of $Y$ for each subsample.
Thus, hereinafter, we assume that $X$ is a continuous random variable.

\section{Nonparametric regression}

Suppose we would like to estimate the following regression model:
\begin{align}\label{eq:nonpara}
	Y = g_0(X) + \eps,
\end{align}
where $\E[\eps \mid X] = 0$ (i.e., $g_0(X) = \E[Y \mid X]$).
If we additionally assume a parametric regression model $g(X, \theta_0) = g_0(X)$, once a consistent estimator $\hat \theta_n$ for $\theta_0$ is obtained, we can estimate $g(x, \hat \theta_n) \approx g_0(x)$ for any $x$.
However, the correct parametric specification is usually unknown to us.
Then, we consider estimating $g_0(\cdot)$ with its functional form fully unrestricted, except for some smoothness constraints.

Let $\mathcal{X} \subseteq \mathbb{R}^d$ be the support of $X$.
For any given $x \in \mathcal{X}$, we would like to develop a consistent estimator of $g_0(x)$.
There are two major approaches used in nonparametric regression: local regression and global regression.
The local approach is to estimate $g_0(x)$ by fitting a simple model using only the observations in the neighborhood of $x$ (the evaluation point $x \in \mathcal{X}$ does not have to be an observaed data point).
Since any local estimator only gives an estimate of $g_0(\cdot)$ at a specific point $x$, if one wants to recover the whole shape of $g_0(\cdot)$, it is necessary to repeat the estimation at different points many times.
In contrast, the global approach considers a basis expansion of the function $g_0(x)\approx p(x)^\top \theta_0$ and estimates the whole functional form of $g_0(\cdot)$ at once.
Here, $p(\cdot)$ is a vector of known basis functions of increasing dimension, and $\theta_0$ is the corresponding coefficient vector.
This type of nonparametric regression is called \textbf{series regression\index{series regression}} estimator or \textbf{sieve method\index{sieve method}}.

\subsection{$k$-nearest-neighbor regression and kernel regression}

We first describe the local approach.
For simplicity, we temporarily assume that $X$ is one-dimensional.
One simplest local regression method is the \textbf{$k$-nearest-neighbor ($k$NN) regression\index{$k$-nearest-neighbor regression}}.
The $k$NN regression estimator simply computes the average of $Y$ over the $k$-closest observations to $x$ on $\mathcal{X}$.
Suppose we have $n$ IID observations $\{(Y_i, X_i): 1 \le i \le n\}$.
Re-ordering the observations in the increasing order of $|X_i - x|$, so that $|X^{(1)} - x| \le |X^{(2)} - x| \le \cdots \le |X^{(n)} - x|$, we estimate $g_0(x)$ by
\begin{align*}
	\hat g^{knn}_n(x) \equiv \frac{1}{k}\sum_{i = 1}^k Y^{(i)}.
\end{align*}
Clearly, to establish the consistency of the $k$NN estimator, $|X^{(k)} - x|$ must get closer to zero as $n$ increases.
On the other hand, $k$ should increase to infinity to apply the law of large numbers to $\hat g^{knn}_n(x)$. 
That is, there is a bias-variance trade-off in terms of the choice of $k$.
Therefore, to meet these conditions simultaneously, we need to deliberately choose $k$ so that it grows to infinity but not too fast.

\bigskip

Another popularly used local regression method is the \textbf{kernel regression\index{kernel regression}}.
The idea of kernel regression is similar to that of $k$NN regression, but differs in that it introduces a \textbf{bandwidth\index{bandwidth}} parameter $h$ such that only the observations whose $X$ value falling into the interval $[x-h, \; x+h]$ are used to compute the estimator.
For example, a \textbf{local constant regression\index{local constant regression}} estimator is given by
\begin{align*}
	\hat g^{const}_n(x) \equiv \frac{\sum_{i = 1}^n \mathbf{1}\{|X_i - x | \le h\} Y_i}{\sum_{i = 1}^n\mathbf{1}\{|X_i - x | \le h\}}.
\end{align*}
The above estimator is slightly inefficient in practice because, once the criterion $|X_i - x | \le h$ is met, the observations adjacent to $x$ and those not that close to $x$ have the same contributions to the estimator.
Thus, it is possible to improve the estimator by assigning larger weights for the observations closer to the evaluation point $x \in \mathcal{X}$.

To this end, we introduce a general weighting function $K(\cdot)$, which is referred to as the \textbf{kernel function\index{kernel function}}.
The kernel function is a continuous density function (i.e., $\int K(u)\text{d}u = 1$), and is assumed to be symmetric around zero.
Some typical choices for the kernel function are: uniform kernel $K(u) = \frac{1}{2}\mathbf{1}\{|u| \le 1\}$, Epanechnikov kernel $K(u) = \frac{3}{4}(1-u^2)\mathbf{1}\{|u| \le 1\}$, and Gaussian kernel $K(u) = \frac{1}{\sqrt{2\pi}}\exp(-u^2/2)$; see Figure \ref{fig:kernels}.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width = 18cm]{kernels.png}
		\caption{Kernel density functions}\label{fig:kernels}
	\end{center}
\end{figure}

One can easily see that the local constant estimator given above is numerically equivalent to
\begin{align*}
	\hat g^{const}_n(x) = \frac{(nh)^{-1}\sum_{i = 1}^n K\left( \frac{X_i - x}{h}\right) Y_i}{(nh)^{-1}\sum_{i = 1}^n K\left( \frac{X_i - x}{h}\right)}
\end{align*}
with $K$ being the uniform kernel.
Dividing both the numerator and denominator by $nh$ is for technical convenience.
From a straightforward calculation, one can find that this $\hat g^{const}_n(x)$ is equivalent to the solution of the following weighted least squares problem:
\begin{align*}
	\hat g^{const}_n(x) = \argmin_a \sum_{i = 1}^n (Y_i - a)^2K\left( \frac{X_i - x}{h}\right).
\end{align*}
Hence, the local constant estimator can be viewed as fitting a constant-only model to the (weighted) local observations in the neighborhood of $x$ (this is why the estimator is called local ``constant'').

As an extension of the local constant approach, we can consider fitting a linear regression model to the local observations instead of simply taking the average or fitting a constant-only model.
Such an approach is called the \textbf{local linear regression\index{local linear regression}}.
That is, the local linear kernel regression estimator is defined as $\hat g^{ll}_n(x) \equiv \hat a_n$, where
\begin{align*}
	(\hat a_n, \hat b_n) = \argmin_{(a, b)} \sum_{i = 1}^n (Y_i - a - b (X_i -x))^2K\left( \frac{X_i - x}{h}\right).
\end{align*}
It is known that by including an additional regressor $(X_i - x)$, the local linear regression estimator achieves a nice bias-correction property that the local constant estimator does not have (for more details, see, e.g., \cite{li2007nonparametric}).
As a further generalization, one may consider including higher order polynomials $(X_i - x)^2$, $(X_i - x)^3$, \ldots as additional regressors.
These estimators are called the local polynomial estimators.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width = 18cm]{localreg.png}
		\caption{Loacal nonparametric regression}
	\end{center}
\end{figure}

Here, let us provide a sketch for the proof of the consistency of $\hat g^{const}_n(x)$.
Let $f_X$ be the density function of $X$.
We first show that the numerator term of $\hat g^{const}_n(x)$ converges to $\E[Y \mid X = x]f_X(x)$ in probability.
Observe that
\begin{align*}
	\E\left[\frac{1}{nh} \sum_{i = 1}^n K\left( \frac{X_i - x}{h}\right) Y_i \right] 
	& = \frac{1}{h}\E\left[ K\left( \frac{X - x}{h}\right) Y \right] \;\; \text{(IID assumption)}\\
	& = \frac{1}{h}\int K\left( \frac{w - x}{h}\right) \E[ Y \mid X = w] f_X (w)\text{d}w \;\; \text{(LIE)} \\
	& = \int K(u) \E[ Y \mid X = x + uh] f_X (x + uh)\text{d}u \;\; \text{(change of variables: $u = (w-x)/h$)}.
\end{align*}
Recalling that $\int K(u)\text{d}u = 1$ and assuming that $\E[ Y \mid X = \cdot]$ and $f_X (\cdot)$ are bounded continuous functions, we have 
\begin{align*}
	\lim_{h \to 0} \E\left[\frac{1}{nh} \sum_{i = 1}^n K\left( \frac{X_i - x}{h}\right) Y_i \right] = \E[ Y \mid X = x] f_X (x)
\end{align*}
by the bounded convergence theorem.
By similar but slightly more tedious calculations, we can show that the variance of $(nh)^{-1} \sum_{i = 1}^n K\left( \frac{X_i - x}{h}\right) Y_i$ is of order $O((nh)^{-1})$.
Thus, if $h \to 0$ and $nh \to \infty$ as $n \to \infty$, by Chebyshev's inequality, we get $(nh)^{-1} \sum_{i = 1}^n K\left( \frac{X_i - x}{h}\right) Y_i \overset{p}{\to} \E[ Y \mid X = x] f_X (x)$.

By the same argument as above, we can prove that the denominator of $\hat g^{const}_n(x)$, $(nh)^{-1} \sum_{i = 1}^n K\left( \frac{X_i - x}{h}\right)$, converges to $f_X(x)$ in probability (this is the so-called \textbf{kernel density estimator\index{kernel density estimator}}) if $h \to 0$ and $nh \to \infty$ as $n \to \infty$.
Combining these results together, the consistency of $\hat g^{const}_n(x)$ is proved.

Note that, similar to the $k$NN regression estimator, there is a bias-variance trade-off for the choice of the bandwidth $h$.
As we have discussed, as $n$ increases, $h$ must converge to zero sufficiently slowly so that $nh$ can increase to infinity.
Intuitively, the effective sample size for a kernel regression is of order $O(nh)$ for each evaluation point, and thus to apply the law of large numbers, $nh$ must grow to infinity.

\bigskip

So far, we have focused on the case where the dimension of $X$ is just one.
When $X = (X_1, \ldots, X_d)$ is a general $d$-dimensional variable, the aforementioned estimators require some modifications.
Specifically, we introduce a ``multivariate'' kernel weighting function, say $\mathbf{K}(u)$, where $u = (u_1, \ldots, u_d) \in \mathbb{R}^d$, and we can estimate $g_0(x)$ in a similar manner using $\mathbf{K}(u)$ in the place of $K(u)$.
One simple way to construct $\mathbf{K}(u)$ is to take the product of one-dimensional kernel functions: $\mathbf{K}(u) = \prod_{j=1}^d K(u_j)$.
For example, if one employs the same bandwith $h$ for all $X_j$'s, the product uniform kernel function is
\begin{align*}
	\mathbf{K}\left(\frac{X_i - x}{h}\right) = \frac{1}{2^d}\mathbf{1}\{|X_{i,1} - x_1| \le h\}\mathbf{1}\{|X_{i,2} - x_2| \le h\} \times \cdots \times \mathbf{1}\{|X_{i,d} - x_d| \le h\}.
\end{align*}
This means that only the observations contained in the $d$-dimensional cube centered at $x$ with each side length of $2h$ can be used for estimating $g_0(x)$.
If the data are uniformly distributed on $\mathcal{X}$, the size of the subsample in each cube is of order $O(nh^d)$.
Therefore, since $h$ is assumed to tend to zero, the size of the effective sample decreases ``exponentially'' (not proportionally) with respect to $d$.
Thus, nonparametric estimation of $g_0(x)$ becomes less and less feasible quickly and inefficient as $d$ grows.
This issue is known as the \textbf{curse of dimensionality\index{curse of dimensionality}}.
As far as I can see in the empirical literature, full nonparametric regression has rarely been considered when $d$ is larger than three, except when the sample size is very large (e.g., $10^5$ or more).

\subsection{Series regression}

Again, we first focus on the estimation of the model in \eqref{eq:nonpara} with a one-dimensional $X$, and extend the discussion to more general cases later.
The idea of approximating a general continuous function by a linear combination of basis functions has a long history.
Perhaps one of the most familiar to readers would be the Maclaurin expansion (i.e., the Taylor expansion at 0):
assuming that $g_0(\cdot)$ is sufficiently many times differentiable, at each $x \in \mathcal{X}$
\begin{align*}
	g_0(x) = g_0(0) + g_0^{(1)}(0)x + \frac{g_0^{(2)}(0)}{2!}x^2 + \cdots + \frac{g_0^{(k)}(0)}{k!}x^k + \cdots,
\end{align*}
where $g_0^{(k)}$ denotes the $k$-th order derivative of $g_0$.
This suggests that if $g_0(\cdot)$ is sufficiently smooth, we can approximate $g_0(x)$ by $g_0(x) \approx p(x)^\top \theta_0$ with $p(x) = (1, x, x^2, \ldots, x^k)^\top$, the \textbf{power series} of order $k$, and $\theta_0 = (g_0(0), g_0^{(1)}(0), \ldots, g_0^{(k)}(0)/k!)^\top$.

A similar well-known result is the Weierstrass approximation theorem.
The theorem states that for any continuous function on a closed interval $\mathcal{X}$, the same power-series approximation holds in the sense that the approximation error $g_0(x) - p(x)^\top \theta_0$ can be made arbitrarily small for any $x \in \mathcal{X}$ by choosing a sufficiently large polynomial order and an appropriate coefficient vector $\theta_0$. 
Remarkably, the differentiability is not required here.

Historically, the most important series expansion result for humankind would be the \textbf{Fourier series\index{Fourier series}} expansion.
The French mathematician Joseph Fourier (1768-1830) claimed that any periodic function can be decomposed into a linear combination of sine and cosine curves.
Although its formal proof was not given by Fourier, it was later shown that his claim was after all true for a wide class of functions.
Then, named after his discovery, the following series expansion is called the Fourier series expansion: for a continuous function $g_0(\cdot)$ on a closed interval $\mathcal{X}$,
\begin{align*}
	g_0(x) = a_0 + a_1\sin(\omega x) + b_1\cos(\omega x) + a_2\sin(\omega 2 x) + b_2\cos(\omega 2 x) + \cdots + a_k\sin(\omega k x) + b_k\cos(\omega k x) + \cdots,
\end{align*}
where $\omega$ is a constant determined by the size of $\mathcal{X}$.

In Figure \ref{fig:seriesreg}, we provide numerical results for the power series approximation and the Fourier series approximation.
In the figure, the true functional form of $g_0(\cdot)$ is presented as the dotted curve.
We can clearly observe that in both series expansions, more and more precise approximation can be achieved by increasing the order of the basis functions.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width = 17cm]{seriesreg.png}
		\caption{Series approximation}\label{fig:seriesreg}
	\end{center}
\end{figure}

As such, with suitable conitnuity conditions, a fairly large class of functions can be approximated by some series expansion.
In addition to those introduced above, basis functions that are often adopted in applications include splines, wavelets, artificial neural networks, and so forth (see \cite{chen2007large} for their definitions and other examples).
Now, let $\{p_1(x), p_2(x), \ldots \}$ be a sequence of $\mathbb{R}$-valued basis functions, and $p(x) = (p_1(x), \ldots , p_k(x))^\top$.
We assume that for a sufficiently large $k$, there exists a $k \times 1$ vector $\theta_0$ such that the target function $g_0(\cdot)$ can be well-approximated by $p(\cdot)^\top\theta_0$ uniformly over $\mathcal{X}$ in the following sense:
\begin{align*}
	|g_0(x) - p(x)^\top\theta_0 | \le O(k^{-\alpha}) \;\; \text{for any $x \in \mathcal{X}$},
\end{align*}
where $\alpha > 0$ is a constant that depends on the choice of the basis function, the smoothness of $g_0$, and the dimension of $X$ (for details, see \cite{chen2007large}).\footnote{
	Among the basis functions listed above, while the power series is quite popular, it is known that the power series expansion has relatively a ``slower'' convergence rate than the others.
}
Using this series approximation, we can rewrite \eqref{eq:nonpara} as follows:
\begin{align*}
	Y
	& = g_0(x) + \eps\\
	& = p(X)^\top\theta_0 + \eta
\end{align*}
where $\eta \equiv g_0(x) - p(X)^\top\theta_0 + \eps$.
Thus, the nonparametric regression model can be transformed into a ``linear regression'' model with increasing dimension.
Indeed, we can estimate $\theta_0$ simply by running an OLS regression of $Y$ on $p(x)$:\footnote{
	Here, we implicitly assume for simplicity that $\frac{1}{n}\sum_{i = 1}^n p(X_i)p(X_i)^\top$ is a nonsingular matrix.
	However, noting that the dimension of this matrix grows to infinity along with $n$, it is generally possible that the matrix is not invertible for finite $n$.
	In that case, the inverse should be replaced by a generalized inverse.
	}
\begin{align*}
	\hat \theta_n 
	& \equiv \argmin_\theta \frac{1}{n}\sum_{i = 1}^n (Y_i -  p(X_i)^\top\theta)^2 \\
	& = \left(\frac{1}{n}\sum_{i = 1}^n p(X_i)p(X_i)^\top\right)^{-1}\frac{1}{n}\sum_{i = 1}^n p(X_i)Y_i
\end{align*}
Once we obtain $\hat \theta_n$, for any given $x \in \mathcal{X}$, we can estimate $g_0(x)$ by $\hat g_n(x)  \equiv p(x)^\top \hat \theta_n$.
Thus, compared to the kernel regression approach, series regression is much easier to implement.

To prove the consistency of $\hat g_n(x)$, by the triangle inequality, observe that
\begin{align*}
	|\hat g_n(x) - g_0(x)|
	& \le |p(x)^\top (\hat \theta_n - \theta_0)| + |g_0(x) - p(x)^\top \theta_0| \\
	& \le |p(x)^\top (\hat \theta_n - \theta_0)| + O(k^{-\alpha})
\end{align*}
Hence, it suffices to show that $|p(x)^\top (\hat \theta_n - \theta_0)| = o_P(1)$.
Here, assuming that the basis functions are bounded, we have $||p(x)|| = (p_1^2(x) + \cdots + p_k^2(x))^{1/2} = O(\sqrt{k})$.
Further, under some regularity conditions, we can show that
\begin{align}\label{eq:seriesrate}
		||\hat \theta_n - \theta_0|| = O_P\left( \sqrt{\frac{k}{n}} \right).
\end{align}
A sketch of the proof of this result can be found in the appendix of this chapter.
Then, combining these results with Cauchy-Schwarz inequality gives that $|p(x)^\top (\hat \theta_n - \theta_0)| \le ||p(x)|| \cdot ||\hat \theta_n - \theta_0 || = O_P(k/\sqrt{n})$.
Thus, if $k$ grows to infinity sufficiently slowly such that $k/\sqrt{n} \to 0$ is maintained, we obtain $\hat g_n(x) - g_0(x) \overset{p}{\to} 0$ for any $x \in \mathcal{X}$, as desired.

\begin{comment}
To prove the asymptotic normality, we require much stronger conditions.
Here, we only state the result under the homoskedasticity assumption $\E[\eps^2 \mid X] = \sigma^2$: for a given $x \in \mathcal{X}$,
\begin{align*}
	\frac{\sqrt{n}(\hat m_n(x) - g_0(x))}{\sigma \sqrt{p(x)^\top \E[p(X)p(X)^\top]^{-1}p(x)}} \overset{d}{\to} N(0,1).
\end{align*}
Then, based on this result, we can compute the confidence interval for each $g_0(x)$.
\end{comment}

\bigskip

Now we turn to the cases where $X = (X_1, \ldots, X_d)$ is a general $d$-dimensional variable.
As can be inferred from a multivariate Taylor expansion, if $X$ is $d$-dimensional, the number of basis functions required to approximate the target function increases in the order $O(k^d)$.
In general, we can construct a multivariate basis functions by taking the Kronecker product of univariate basis functions:
\begin{align*}
	p(x) = p(x_1) \otimes p(x_2) \otimes \cdots \otimes p(x_d).
\end{align*}
Thus, if $d$ is not small, the number of coefficient parameters to be estimated easily reaches several hundreds, resulting in a serious loss of efficiency.
This is the series-regression version of the curse of dimensionality.

\subsection{An empirical illustration: estimation of Engel curve}

As an empirical illustration, we nonparametrically estimate the Engel curve for food consumption using the local constant kernel regression and the series regression based on B-splines; a B-spline function is a piecewise polynomial defined by a set of ``cut points'', called \textbf{knots}.

Install \textbf{np} and \textbf{splines} packages, and load them.
The data used here is \texttt{Engel95}, a random sample taken from the U.K. Family Expenditure Survey 1995, which can be imported from the \textbf{np} package.

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
 library(np) # run nonparametric kernel regression
 library(splines) # compute b-spline functions

 data("Engel95") # sample size n = 1655
 food <- Engel95$food # share of food consumption
 exp  <- Engel95$logexp # log of total expenditure 
\end{lstlisting}

We first run the kernel regression using the \textbf{npreg} function.
This function automatically computes an optimal bandwidth parameter $h$, so that we do not need to specify it.

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
# Kernel regression #
 ghat_kernel <- npreg(food ~ exp)
\end{lstlisting}

Next, we perform the series regression.
The B-spline basis function can be computed using the \textbf{bs} function.
Here, we set the 25\%, 50\%, and 75\% empirical quantiles of \texttt{exp} as the knot values and create B-splines with $k = 6$.
After the B-splines are created, we simply run an OLS regression.

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
# Spline series regression #
 knots <- quantile(exp, (1:3)/4)
 p <- bs(exp, knots = knots) # compute the B-splines with k = 6
 ghat_spline <- predict(lm(food ~ p))
\end{lstlisting}

To summarize the estimation results of nonparametric regression, we present them visually.
Running the code below, we can observe that both the kernel and series regression perform very similarly.

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize, frame=single]
 plot(exp, food, xlim = range(exp), ylim = range(food), 
	main = "Nonparametric regression", xlab = "exp", ylab = "food")
 par(new = T)
 plot(exp[order(exp)], ghat_kernel$mean[order(exp)], xlab = "", ylab = "",
	xlim = range(exp), ylim = range(food), type = "l", lwd = 2, col = 2)
 par(new = T)
 plot(exp[order(exp)], ghat_spline[order(exp)], xlab = "", ylab = "",
	xlim = range(exp), ylim = range(food), type = "l", lwd = 2, col = 3)
 legend("topright", c("Kernel regression", "Spline series regression"), 
	lty = c(1,1), lwd = c(2,2), col = c(2,3))
\end{lstlisting}

\begin{center}
	\includegraphics[width = 12cm]{engel.png}
\end{center}

\section{Semiparamtric regression}

\subsection{Partially linear models}

\subsection{Generalized additive models}

\subsection{Functional coefficient models}

\section*{Appendix: Proof of \eqref{eq:seriesrate}}

\end{document}